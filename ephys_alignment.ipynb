{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy.signal import resample\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "from datetime import datetime, date\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating making binned_FR files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/AdamHarris/Desktop/drive-download-20250122T174732Z-001/bp01-2024-03-15-144724.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>info</td>\n",
       "      <td>experiment_name</td>\n",
       "      <td>Adam\\ABCD_mEC_open_field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>info</td>\n",
       "      <td>task_name</td>\n",
       "      <td>Peter\\7x7open_field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>info</td>\n",
       "      <td>task_file_hash</td>\n",
       "      <td>2633371478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>info</td>\n",
       "      <td>setup_id</td>\n",
       "      <td>Big maze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>info</td>\n",
       "      <td>framework_version</td>\n",
       "      <td>2.0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1788.100</td>\n",
       "      <td>event</td>\n",
       "      <td>sync</td>\n",
       "      <td>rsync</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1791.942</td>\n",
       "      <td>event</td>\n",
       "      <td>sync</td>\n",
       "      <td>rsync</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>1800.000</td>\n",
       "      <td>event</td>\n",
       "      <td>timer</td>\n",
       "      <td>session_timer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>1800.000</td>\n",
       "      <td>variable</td>\n",
       "      <td>run_end</td>\n",
       "      <td>{\"session_duration\": 30}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1800.000</td>\n",
       "      <td>info</td>\n",
       "      <td>end_time</td>\n",
       "      <td>2024-03-15T15:17:24.312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>355 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time      type            subtype                   content\n",
       "0       0.000      info    experiment_name  Adam\\ABCD_mEC_open_field\n",
       "1       0.000      info          task_name       Peter\\7x7open_field\n",
       "2       0.000      info     task_file_hash                2633371478\n",
       "3       0.000      info           setup_id                  Big maze\n",
       "4       0.000      info  framework_version                     2.0.1\n",
       "..        ...       ...                ...                       ...\n",
       "350  1788.100     event               sync                     rsync\n",
       "351  1791.942     event               sync                     rsync\n",
       "352  1800.000     event              timer             session_timer\n",
       "353  1800.000  variable            run_end  {\"session_duration\": 30}\n",
       "354  1800.000      info           end_time   2024-03-15T15:17:24.312\n",
       "\n",
       "[355 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv):\n",
    "    \"\"\"\n",
    "    Load cluster labels from cluster_group.tsv and cluster_KSlabel.tsv,\n",
    "    returning a dictionary: { cluster_id: label }.\n",
    "    \n",
    "    The priority is:\n",
    "      - cluster_group.tsv if available\n",
    "      - fallback to cluster_KSlabel.tsv if the cluster is missing or 'unsorted' in the first\n",
    "    \"\"\"\n",
    "    # Read cluster_group.tsv if it exists\n",
    "    if os.path.exists(cluster_group_tsv):\n",
    "        df_group = pd.read_csv(cluster_group_tsv, sep='\\t')\n",
    "        df_group.columns = ['cluster_id', 'group']  # Usually: cluster_id, group\n",
    "    else:\n",
    "        df_group = pd.DataFrame(columns=['cluster_id', 'group'])\n",
    "    \n",
    "    # Read cluster_KSlabel.tsv if it exists\n",
    "    if os.path.exists(cluster_kslabel_tsv):\n",
    "        df_ks = pd.read_csv(cluster_kslabel_tsv, sep='\\t')\n",
    "        df_ks.columns = ['cluster_id', 'KSlabel']  # Usually: cluster_id, KSlabel\n",
    "    else:\n",
    "        df_ks = pd.DataFrame(columns=['cluster_id', 'KSlabel'])\n",
    "    \n",
    "    # Convert cluster_id to int in both for easier merges\n",
    "    df_group['cluster_id'] = df_group['cluster_id'].astype(int, errors='ignore')\n",
    "    df_ks['cluster_id']    = df_ks['cluster_id'].astype(int, errors='ignore')\n",
    "    \n",
    "    # Merge them into a single DataFrame, outer join so we keep all\n",
    "    df_merge = pd.merge(df_group, df_ks, on='cluster_id', how='outer')\n",
    "    \n",
    "    # Create a dictionary mapping cluster_id -> final label\n",
    "    # We consider 'group' as the primary label if present, else use 'KSlabel'\n",
    "    cluster_label_dict = {}\n",
    "    for idx, row in df_merge.iterrows():\n",
    "        cluster_id = int(row['cluster_id'])\n",
    "        \n",
    "        group_label = row['group'] if 'group' in row and pd.notnull(row['group']) else None\n",
    "        ks_label    = row['KSlabel'] if 'KSlabel' in row and pd.notnull(row['KSlabel']) else None\n",
    "        \n",
    "        # Priority: if group_label is available (e.g. \"good\", \"mua\", etc.), use it, else use ks_label\n",
    "        if group_label is not None:\n",
    "            cluster_label_dict[cluster_id] = group_label\n",
    "        else:\n",
    "            cluster_label_dict[cluster_id] = ks_label\n",
    "    \n",
    "    return cluster_label_dict\n",
    "\n",
    "def get_good_clusters(cluster_label_dict):\n",
    "    \"\"\"\n",
    "    Return a sorted list of cluster IDs considered 'good'.\n",
    "    This function can be adapted depending on how you define 'good' or 'SU'.\n",
    "    \"\"\"\n",
    "    good = []\n",
    "    for clust_id, label in cluster_label_dict.items():\n",
    "        # Some Kilosort pipelines use 'good' or 'single' or 'SU'. Adjust as appropriate.\n",
    "        if label in ['good', 'Good', 'su', 'SU']:\n",
    "            good.append(clust_id)\n",
    "    return sorted(good)\n",
    "\n",
    "def bin_spikes(spike_times, spike_clusters, good_clusters, bin_size_ms=25, session_offset=0, session_duration_ms=None):\n",
    "    \"\"\"\n",
    "    For a set of spikes that fall within [session_offset, session_offset+session_duration_ms),\n",
    "    shift them to start at 0 and bin them at bin_size_ms. Return a 2D array: (n_clusters x n_time_bins).\n",
    "    \n",
    "    - spike_times: 1D array of spike times (ms).\n",
    "    - spike_clusters: 1D array of cluster IDs for each spike_time.\n",
    "    - good_clusters: list of cluster IDs that are 'good'.\n",
    "    - bin_size_ms: size of each bin in ms.\n",
    "    - session_offset: start time of session in ms within the concatenated data.\n",
    "    - session_duration_ms: total duration of this session in ms.\n",
    "    \"\"\"\n",
    "    # Identify the spikes for this session\n",
    "    t_start = session_offset\n",
    "    t_end   = session_offset + session_duration_ms\n",
    "    \n",
    "    # Boolean mask for spikes in this session\n",
    "    in_session_mask = (spike_times >= t_start) & (spike_times < t_end)\n",
    "    \n",
    "    # Subset spike times & clusters\n",
    "    sess_spike_times    = spike_times[in_session_mask] - t_start  # shift to 0-based\n",
    "    sess_spike_clusters = spike_clusters[in_session_mask]\n",
    "    \n",
    "    # Figure out how many bins we need\n",
    "    n_bins = int(np.ceil(session_duration_ms / bin_size_ms))\n",
    "    \n",
    "    # We'll create a 2D array: shape = (len(good_clusters), n_bins)\n",
    "    spike_matrix = np.zeros((len(good_clusters), n_bins), dtype=np.int32)\n",
    "    \n",
    "    # Make a quick index from cluster_id -> row index in spike_matrix\n",
    "    cluster_index_map = {clust_id: i for i, clust_id in enumerate(good_clusters)}\n",
    "    \n",
    "    # Digitize times to bin indices\n",
    "    bin_indices = (sess_spike_times // bin_size_ms).astype(int)\n",
    "    \n",
    "    # Accumulate counts\n",
    "    for t_bin, clust in zip(bin_indices, sess_spike_clusters):\n",
    "        if clust in cluster_index_map:\n",
    "            # increment the corresponding cluster row, bin column\n",
    "            spike_matrix[cluster_index_map[clust], t_bin] += 1\n",
    "    \n",
    "    return spike_matrix\n",
    "\n",
    "def main():\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Load the JSON file containing session file sizes in bytes\n",
    "    # -------------------------------------------------------------------------\n",
    "    json_path = '/path/to/session_sizes.json'\n",
    "    with open(json_path, 'r') as f:\n",
    "        session_dict = json.load(f)\n",
    "    # session_dict: { file_path: file_size_in_bytes, ... }\n",
    "\n",
    "    # Sort the sessions by file_path or, if you prefer, by recording time\n",
    "    # (In your example, the keys appear in chronological order, but let's force a sorted list anyway.)\n",
    "    session_items = sorted(session_dict.items(), key=lambda x: x[0])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Compute session boundaries in ms\n",
    "    # -------------------------------------------------------------------------\n",
    "    bytes_per_sample = 768\n",
    "    sampling_rate_hz = 30000\n",
    "    # each second has 30000 samples, each sample is 768 bytes\n",
    "    #  => 30000 samples per sec => 30000 * 768 bytes per second\n",
    "    #  => 1 sample = 768 bytes => 1 ms = 30 samples\n",
    "    session_boundaries = []\n",
    "    cumulative_ms = 0.0\n",
    "    \n",
    "    session_info = []  # will hold (file_path, start_ms, end_ms)\n",
    "    \n",
    "    for (file_path, file_size) in session_items:\n",
    "        # number of raw samples in this session\n",
    "        n_samples = file_size / bytes_per_sample\n",
    "        \n",
    "        # convert to ms (1 second = 1000 ms -> 30000 samples => 1 ms = 30 samples)\n",
    "        duration_ms = n_samples / 30.0  # because 30 samples per ms at 30 kHz\n",
    "        start_ms = cumulative_ms\n",
    "        end_ms   = cumulative_ms + duration_ms\n",
    "        \n",
    "        session_info.append((file_path, start_ms, end_ms))\n",
    "        cumulative_ms = end_ms\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Load the Kilosort spike_times, spike_clusters\n",
    "    # -------------------------------------------------------------------------\n",
    "    kilosort_folder = '/path/to/kilosort_output'  # where spike_times.npy etc. reside\n",
    "    \n",
    "    spike_times = np.load(os.path.join(kilosort_folder, 'spike_times.npy'))  # confirm if truly in ms or samples\n",
    "    spike_clusters = np.load(os.path.join(kilosort_folder, 'spike_clusters.npy'))\n",
    "    \n",
    "    # If your spike_times are actually in samples at 30 kHz, do:\n",
    "    # spike_times = spike_times / 30.0  # to convert to ms\n",
    "    # Adjust if needed to keep consistent with how your pipeline is set.\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Load cluster labels (cluster_group.tsv, cluster_KSlabel.tsv) & get \"good\" clusters\n",
    "    # -------------------------------------------------------------------------\n",
    "    cluster_group_tsv   = os.path.join(kilosort_folder, 'cluster_group.tsv')\n",
    "    cluster_kslabel_tsv = os.path.join(kilosort_folder, 'cluster_KSlabel.tsv')\n",
    "    cluster_labels = load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv)\n",
    "    \n",
    "    good_clusters = get_good_clusters(cluster_labels)\n",
    "    print(f\"Found {len(good_clusters)} 'good' clusters.\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) For each session, partition the spikes and bin into 25 ms bins\n",
    "    # -------------------------------------------------------------------------\n",
    "    bin_size_ms = 25\n",
    "    session_spike_counts = []  # will hold one array for each session\n",
    "    \n",
    "    for i, (file_path, start_ms, end_ms) in enumerate(session_info):\n",
    "        duration_ms = end_ms - start_ms\n",
    "        print(f\"Session {i}: {file_path}\")\n",
    "        print(f\"   start_ms = {start_ms:.2f}, end_ms = {end_ms:.2f}, duration_ms = {duration_ms:.2f}\")\n",
    "        \n",
    "        # bin the spikes for this session\n",
    "        spike_count_mat = bin_spikes(\n",
    "            spike_times=spike_times,\n",
    "            spike_clusters=spike_clusters,\n",
    "            good_clusters=good_clusters,\n",
    "            bin_size_ms=bin_size_ms,\n",
    "            session_offset=start_ms,\n",
    "            session_duration_ms=duration_ms\n",
    "        )\n",
    "        \n",
    "        session_spike_counts.append(spike_count_mat)\n",
    "    \n",
    "    # session_spike_counts is now a list of arrays of shape [n_good_clusters, n_time_bins_for_that_session].\n",
    "    # You can save them or keep them in memory for further analysis.\n",
    "    \n",
    "    # Example: store in a dictionary, then maybe save to npy or pickle\n",
    "    results_dict = {}\n",
    "    for i, (file_path, start_ms, end_ms) in enumerate(session_info):\n",
    "        results_dict[file_path] = session_spike_counts[i]\n",
    "    \n",
    "    # If you wish to save them individually:\n",
    "    output_dir = '/path/to/output_matrices'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for i, (file_path, start_ms, end_ms) in enumerate(session_info):\n",
    "        # A nice name for the output file\n",
    "        session_filename = os.path.basename(file_path)  # or parse the date/time from the path\n",
    "        out_path = os.path.join(output_dir, f\"binnedSpikes_session{i}.npy\")\n",
    "        np.save(out_path, session_spike_counts[i])\n",
    "        print(f\"Saved binned spikes for session {i} to {out_path}\")\n",
    "    \n",
    "    print(\"All sessions processed and binned spike arrays saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 'good' clusters.\n",
      "Session 0: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_12-59-35/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "   start_ms = 0.00, end_ms = 1219909.20, duration_ms = 1219909.20\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 220\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll sessions processed and binned spike arrays saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 187\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   start_ms = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_ms\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, end_ms = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_ms\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, duration_ms = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration_ms\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# bin the spikes for this session\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     spike_count_mat \u001b[38;5;241m=\u001b[39m \u001b[43mbin_spikes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspike_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspike_times_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspike_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspike_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgood_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgood_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbin_size_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbin_size_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession_duration_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration_ms\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     session_spike_counts\u001b[38;5;241m.\u001b[39mappend(spike_count_mat)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# session_spike_counts is now a list of arrays of shape [n_good_clusters, n_time_bins_for_that_session].\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# You can save them or keep them in memory for further analysis.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Example: store in a dictionary, then maybe save to npy or pickle\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 94\u001b[0m, in \u001b[0;36mbin_spikes\u001b[0;34m(spike_times, spike_clusters, good_clusters, bin_size_ms, session_offset, session_duration_ms)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Subset spike times & clusters\u001b[39;00m\n\u001b[1;32m     93\u001b[0m sess_spike_times    \u001b[38;5;241m=\u001b[39m spike_times[in_session_mask] \u001b[38;5;241m-\u001b[39m t_start  \u001b[38;5;66;03m# shift to 0-based\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m sess_spike_clusters \u001b[38;5;241m=\u001b[39m \u001b[43mspike_clusters\u001b[49m\u001b[43m[\u001b[49m\u001b[43min_session_mask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Figure out how many bins we need\u001b[39;00m\n\u001b[1;32m     97\u001b[0m n_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(session_duration_ms \u001b[38;5;241m/\u001b[39m bin_size_ms))\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv):\n",
    "    \"\"\"\n",
    "    Load cluster labels from cluster_group.tsv and cluster_KSlabel.tsv,\n",
    "    returning a dictionary: { cluster_id: label }.\n",
    "    \"\"\"\n",
    "\n",
    "    # Try to read with a header\n",
    "    if os.path.exists(cluster_group_tsv):\n",
    "        df_group = pd.read_csv(cluster_group_tsv, sep='\\t')\n",
    "        # If the file indeed has columns named something else, rename:\n",
    "        # e.g. many labs have these columns: 'cluster_id', 'group'\n",
    "        # If your file is correct, you can skip or adapt the rename logic below\n",
    "        if list(df_group.columns) != ['cluster_id', 'group']:\n",
    "            # Attempt a rename; adjust logic if your columns differ\n",
    "            df_group.columns = ['cluster_id', 'group']\n",
    "    else:\n",
    "        df_group = pd.DataFrame(columns=['cluster_id', 'group'])\n",
    "    \n",
    "    if os.path.exists(cluster_kslabel_tsv):\n",
    "        df_ks = pd.read_csv(cluster_kslabel_tsv, sep='\\t')\n",
    "        # e.g. columns might be 'cluster_id', 'KSlabel'\n",
    "        if list(df_ks.columns) != ['cluster_id', 'KSlabel']:\n",
    "            df_ks.columns = ['cluster_id', 'KSlabel']\n",
    "    else:\n",
    "        df_ks = pd.DataFrame(columns=['cluster_id', 'KSlabel'])\n",
    "    \n",
    "    # Ensure correct types\n",
    "    df_group['cluster_id'] = df_group['cluster_id'].astype(int, errors='ignore')\n",
    "    df_ks['cluster_id']    = df_ks['cluster_id'].astype(int, errors='ignore')\n",
    "    \n",
    "    # Merge\n",
    "    df_merge = pd.merge(df_group, df_ks, on='cluster_id', how='outer')\n",
    "    \n",
    "    # Build the dictionary\n",
    "    cluster_label_dict = {}\n",
    "    for idx, row in df_merge.iterrows():\n",
    "        clust_id = int(row['cluster_id'])\n",
    "        \n",
    "        # \"group\" could be \"good\", \"mua\", etc.\n",
    "        group_label = row['group']   if 'group'   in row and pd.notnull(row['group'])   else None\n",
    "        ks_label    = row['KSlabel'] if 'KSlabel' in row and pd.notnull(row['KSlabel']) else None\n",
    "        \n",
    "        # Priority: if group_label is available, use it, otherwise fallback to ks_label\n",
    "        if group_label is not None:\n",
    "            cluster_label_dict[clust_id] = group_label\n",
    "        else:\n",
    "            cluster_label_dict[clust_id] = ks_label\n",
    "    \n",
    "    return cluster_label_dict\n",
    "\n",
    "    \n",
    "\n",
    "def get_good_clusters(cluster_label_dict):\n",
    "    \"\"\"\n",
    "    Return a sorted list of cluster IDs considered 'good'.\n",
    "    This function can be adapted depending on how you define 'good' or 'SU'.\n",
    "    \"\"\"\n",
    "    good = []\n",
    "    for clust_id, label in cluster_label_dict.items():\n",
    "        # Some Kilosort pipelines use 'good' or 'SU'; adjust as needed\n",
    "        if label in ['good', 'Good', 'su', 'SU']:\n",
    "            good.append(clust_id)\n",
    "    return sorted(good)\n",
    "\n",
    "def bin_spikes(spike_times, spike_clusters, good_clusters,\n",
    "               bin_size_ms=25,\n",
    "               session_offset=0,\n",
    "               session_duration_ms=None):\n",
    "    \"\"\"\n",
    "    For a set of spikes that fall within [session_offset, session_offset+session_duration_ms),\n",
    "    shift them to start at 0 and bin them at bin_size_ms. Return a 2D array: (n_clusters x n_time_bins).\n",
    "    \n",
    "    - spike_times: 1D array of spike times (ms).\n",
    "    - spike_clusters: 1D array of cluster IDs for each spike_time.\n",
    "    - good_clusters: list of cluster IDs that are 'good'.\n",
    "    - bin_size_ms: size of each bin in ms.\n",
    "    - session_offset: start time of session in ms within the concatenated data.\n",
    "    - session_duration_ms: total duration of this session in ms.\n",
    "    \"\"\"\n",
    "    # Identify the spikes for this session\n",
    "    t_start = session_offset\n",
    "    t_end   = session_offset + session_duration_ms\n",
    "    \n",
    "    # Boolean mask for spikes in this session\n",
    "    in_session_mask = (spike_times >= t_start) & (spike_times < t_end)\n",
    "    \n",
    "    # Subset spike times & clusters\n",
    "    sess_spike_times    = spike_times[in_session_mask] - t_start  # shift to 0-based\n",
    "    sess_spike_clusters = spike_clusters[in_session_mask]\n",
    "    \n",
    "    # Figure out how many bins we need\n",
    "    n_bins = int(np.ceil(session_duration_ms / bin_size_ms))\n",
    "    \n",
    "    # We'll create a 2D array: shape = (len(good_clusters), n_bins)\n",
    "    spike_matrix = np.zeros((len(good_clusters), n_bins), dtype=np.int32)\n",
    "    \n",
    "    # Make a quick index from cluster_id -> row index in spike_matrix\n",
    "    cluster_index_map = {clust_id: i for i, clust_id in enumerate(good_clusters)}\n",
    "    \n",
    "    # Digitize times to bin indices\n",
    "    bin_indices = (sess_spike_times // bin_size_ms).astype(int)\n",
    "    \n",
    "    # Accumulate counts\n",
    "    for t_bin, clust in zip(bin_indices, sess_spike_clusters):\n",
    "        if clust in cluster_index_map:\n",
    "            spike_matrix[cluster_index_map[clust], t_bin] += 1\n",
    "    \n",
    "    return spike_matrix\n",
    "\n",
    "def main():\n",
    "\n",
    "\n",
    "    kilosort_folder = \"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/28032024_31032024_combined_all\"\n",
    "    json__path =os.path.join(kilosort_folder, [i for i in os.listdir(kilosort_folder) if i.endswith('.json')][0])\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Load the JSON file containing session file sizes in bytes\n",
    "    # -------------------------------------------------------------------------\n",
    "    # json_path = '/path/to/session_sizes.json'\n",
    "    with open(json_path, 'r') as f:\n",
    "        session_dict = json.load(f)\n",
    "    # session_dict: { file_path: file_size_in_bytes, ... }\n",
    "\n",
    "    # Sort the sessions by file_path or, if you prefer, by recording time\n",
    "    session_items = sorted(session_dict.items(), key=lambda x: x[0])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Compute session boundaries in ms\n",
    "    # -------------------------------------------------------------------------\n",
    "    bytes_per_sample = 768\n",
    "    sampling_rate_hz = 30000\n",
    "    # each second has 30000 samples, each sample is 768 bytes\n",
    "    # => 1 sample = 768 bytes => 1 second = 30000 samples => 30000 * 768 bytes\n",
    "    \n",
    "    session_info = []\n",
    "    cumulative_ms = 0.0\n",
    "    \n",
    "    for (file_path, file_size) in session_items:\n",
    "        # number of raw samples in this session\n",
    "        n_samples = file_size / bytes_per_sample\n",
    "        \n",
    "        # convert to ms (1 second = 1000 ms -> 30000 samples => 1 ms = 30 samples)\n",
    "        duration_ms = n_samples / 30.0  # 30 samples per ms\n",
    "        start_ms = cumulative_ms\n",
    "        end_ms   = cumulative_ms + duration_ms\n",
    "        \n",
    "        session_info.append((file_path, start_ms, end_ms))\n",
    "        cumulative_ms = end_ms\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Load the Kilosort spike_times, spike_clusters\n",
    "    #    Convert spike_times from samples to ms\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    spike_times_samples = np.load(os.path.join(kilosort_folder, 'spike_times.npy'))\n",
    "    spike_clusters = np.load(os.path.join(kilosort_folder, 'spike_clusters.npy'))\n",
    "    \n",
    "    # Convert from samples to ms, since each sample = 1/30000 sec = 0.0333... ms\n",
    "    spike_times_ms = spike_times_samples / 30.0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Load cluster labels (cluster_group.tsv, cluster_KSlabel.tsv) & get \"good\" clusters\n",
    "    # -------------------------------------------------------------------------\n",
    "    cluster_group_tsv   = os.path.join(kilosort_folder, 'cluster_group.tsv')\n",
    "    cluster_kslabel_tsv = os.path.join(kilosort_folder, 'cluster_KSlabel.tsv')\n",
    "    cluster_labels = load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv)\n",
    "    \n",
    "    good_clusters = get_good_clusters(cluster_labels)\n",
    "    print(f\"Found {len(good_clusters)} 'good' clusters.\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) For each session, partition the spikes and bin into 25 ms bins\n",
    "    # -------------------------------------------------------------------------\n",
    "    bin_size_ms = 25\n",
    "    session_spike_counts = []  # will hold one array for each session\n",
    "    \n",
    "    for i, (file_path, start_ms, end_ms) in enumerate(session_info):\n",
    "        duration_ms = end_ms - start_ms\n",
    "        print(f\"Session {i}: {file_path}\")\n",
    "        print(f\"   start_ms = {start_ms:.2f}, end_ms = {end_ms:.2f}, duration_ms = {duration_ms:.2f}\")\n",
    "        \n",
    "        # bin the spikes for this session\n",
    "        spike_count_mat = bin_spikes(\n",
    "            spike_times=spike_times_ms,\n",
    "            spike_clusters=spike_clusters,\n",
    "            good_clusters=good_clusters,\n",
    "            bin_size_ms=bin_size_ms,\n",
    "            session_offset=start_ms,\n",
    "            session_duration_ms=duration_ms\n",
    "        )\n",
    "        \n",
    "        session_spike_counts.append(spike_count_mat)\n",
    "    \n",
    "    # session_spike_counts is now a list of arrays of shape [n_good_clusters, n_time_bins_for_that_session].\n",
    "    # You can save them or keep them in memory for further analysis.\n",
    "    \n",
    "    # Example: store in a dictionary, then maybe save to npy or pickle\n",
    "    results_dict = {}\n",
    "    for i, (file_path, start_ms, end_ms) in enumerate(session_info):\n",
    "        results_dict[file_path] = session_spike_counts[i]\n",
    "    \n",
    "    # If you wish to save them individually:\n",
    "    output_dir = '/Users/AdamHarris/Desktop/test_output_mats'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, (file_path, start_ms, end_ms) in enumerate(session_info):\n",
    "        # A nice name for the output file\n",
    "        session_filename = os.path.basename(file_path)  # or parse the date/time from the path\n",
    "        out_path = os.path.join(output_dir, f\"binnedSpikes_session{i}.npy\")\n",
    "        np.save(out_path, session_spike_counts[i])\n",
    "        print(f\"Saved binned spikes for session {i} to {out_path}\")\n",
    "    \n",
    "    print(\"All sessions processed and binned spike arrays saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(cohort, mouse, data_folder):\n",
    "    data_directory = f\"{data_folder}/cohort{cohort}/\"       Data_folderx+'/cohort'+str(cohort)+'/'#+mouse[:4]+'/'\n",
    "    MetaDatafile_path= f\"{data_directory}/MetaData.xlsx - {mouse}.csv\"\n",
    "    \n",
    "    with open(MetaDatafile_path, 'r') as f:\n",
    "        MetaData = np.genfromtxt(f, delimiter=',',dtype=str, usecols=np.arange(0,18))\n",
    "    MetaData_structure=MetaData[0]\n",
    "    Include_mask=MetaData[1:,np.where(MetaData[0]=='Include')[0][0]]=='1' ##added 06/07/2021\n",
    "    for indx, variable in enumerate(MetaData_structure):\n",
    "        if variable != 'Weight_pre':\n",
    "            Variable_dic[mouse][variable]=np.asarray(remove_empty(MetaData[1:,indx][Include_mask]))\n",
    "        else:\n",
    "            Variable_dic[mouse][variable]=MetaData[1:,indx][Include_mask]\n",
    "\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/28032024_31032024_combined_all/bp01_28032024_31032024_02042024_20241213162222_ALL_20241213162222.json\n",
      "{'/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_12-59-35/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28106707968, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_13-23-31/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28535519232, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_13-48-00/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28141166592, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_14-12-33/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27900291072, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_15-50-23/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28036426752, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_16-14-38/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28030823424, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_16-39-20/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28268375040, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_17-03-45/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28307146752, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_18-15-20/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 41690207232, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_11-08-32/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 44299321344, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_11-56-18/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28109703168, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-19-36/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27991913472, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-44-10/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 7314868224, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-50-05/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27885699072, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_13-13-48/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27876805632, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_14-38-18/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 42007790592, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_15-12-26/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27983803392, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_15-37-15/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28526736384, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_16-00-28/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28819178496}\n",
      "Found 27 'good' clusters.\n",
      "/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_12-59-35/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/sample_numbers.npy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 254\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll sessions processed and binned spike arrays saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 254\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 203\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m sync_file   \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(session_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_numbers.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mprint\u001b[39m(sync_file)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43msync_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: sync_file not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Skipping first-sync truncation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# We'll treat the entire session from session_abs_start to session_abs_end\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# i.e. no extra offset.\u001b[39;00m\n",
      "File \u001b[0;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv):\n",
    "    \"\"\"\n",
    "    Load cluster labels from cluster_group.tsv and cluster_KSlabel.tsv,\n",
    "    returning a dictionary: { cluster_id: label }.\n",
    "    \n",
    "    The priority is:\n",
    "      - cluster_group.tsv (if present)\n",
    "      - fallback to cluster_KSlabel.tsv (if the cluster is missing a label in the first)\n",
    "    \"\"\"\n",
    "    # cluster_group file\n",
    "    if os.path.exists(cluster_group_tsv):\n",
    "        df_group = pd.read_csv(cluster_group_tsv, sep='\\t')\n",
    "        # If needed, rename columns if they don't match exactly:\n",
    "        if 'cluster_id' not in df_group.columns or 'group' not in df_group.columns:\n",
    "            df_group.columns = ['cluster_id', 'group']\n",
    "    else:\n",
    "        df_group = pd.DataFrame(columns=['cluster_id', 'group'])\n",
    "    \n",
    "    # cluster_KSlabel file\n",
    "    if os.path.exists(cluster_kslabel_tsv):\n",
    "        df_ks = pd.read_csv(cluster_kslabel_tsv, sep='\\t')\n",
    "        if 'cluster_id' not in df_ks.columns or 'KSlabel' not in df_ks.columns:\n",
    "            df_ks.columns = ['cluster_id', 'KSlabel']\n",
    "    else:\n",
    "        df_ks = pd.DataFrame(columns=['cluster_id', 'KSlabel'])\n",
    "    \n",
    "    # Convert cluster_id to int in both for easier merges\n",
    "    df_group['cluster_id'] = df_group['cluster_id'].astype(int, errors='ignore')\n",
    "    df_ks['cluster_id']    = df_ks['cluster_id'].astype(int, errors='ignore')\n",
    "    \n",
    "    # Merge them into a single DataFrame, outer join so we keep all\n",
    "    df_merge = pd.merge(df_group, df_ks, on='cluster_id', how='outer')\n",
    "    \n",
    "    # Create a dictionary mapping cluster_id -> final label\n",
    "    cluster_label_dict = {}\n",
    "    for idx, row in df_merge.iterrows():\n",
    "        clust_id = int(row['cluster_id'])\n",
    "        \n",
    "        group_label = None\n",
    "        ks_label    = None\n",
    "        \n",
    "        if 'group' in row and pd.notnull(row['group']):\n",
    "            group_label = row['group']\n",
    "        if 'KSlabel' in row and pd.notnull(row['KSlabel']):\n",
    "            ks_label = row['KSlabel']\n",
    "        \n",
    "        # Priority: if group_label is available, use it; otherwise fallback to ks_label\n",
    "        final_label = group_label if group_label is not None else ks_label\n",
    "        cluster_label_dict[clust_id] = final_label\n",
    "    \n",
    "    return cluster_label_dict\n",
    "\n",
    "def get_good_clusters(cluster_label_dict):\n",
    "    \"\"\"\n",
    "    Return a sorted list of cluster IDs considered 'good'.\n",
    "    Adjust the condition below for your labeling convention:\n",
    "    e.g. 'good', 'Good', 'su', 'SU'\n",
    "    \"\"\"\n",
    "    good = []\n",
    "    for clust_id, label in cluster_label_dict.items():\n",
    "        if label in ['good', 'Good', 'su', 'SU']:\n",
    "            good.append(clust_id)\n",
    "    return sorted(good)\n",
    "\n",
    "def bin_spikes(spike_times_ms, spike_clusters, good_clusters,\n",
    "               bin_size_ms=25,\n",
    "               session_offset=0,\n",
    "               session_duration_ms=None):\n",
    "    \"\"\"\n",
    "    Bin 'good' clusters' spikes into 25 ms bins for one session.\n",
    "\n",
    "    - spike_times_ms: 1D array of spike times (ms, in concatenated timeline).\n",
    "    - spike_clusters: 1D array of cluster IDs for each spike_time.\n",
    "    - good_clusters: list of cluster IDs that are 'good'.\n",
    "    - bin_size_ms: size of each bin in ms (default = 25 ms).\n",
    "    - session_offset: the starting time (ms) of this session in the *concatenated* timeline.\n",
    "    - session_duration_ms: total duration of this session from that offset.\n",
    "    \n",
    "    Returns a 2D array of shape (n_good_clusters, n_time_bins).\n",
    "    \"\"\"\n",
    "    if session_duration_ms <= 0:\n",
    "        # If there's no valid time range, return an empty matrix\n",
    "        return np.zeros((len(good_clusters), 0), dtype=np.int32)\n",
    "    \n",
    "    t_start = session_offset\n",
    "    t_end   = session_offset + session_duration_ms\n",
    "    \n",
    "    # Boolean mask for spikes in [t_start, t_end)\n",
    "    in_session_mask = (spike_times_ms >= t_start) & (spike_times_ms < t_end)\n",
    "    \n",
    "    # Subset spike times & clusters\n",
    "    # (Make sure these arrays are 1D with .squeeze())\n",
    "    sess_spike_times    = spike_times_ms[in_session_mask].squeeze() - t_start\n",
    "    sess_spike_clusters = spike_clusters[in_session_mask].squeeze()\n",
    "    \n",
    "    # Figure out how many bins we need\n",
    "    n_bins = int(np.ceil(session_duration_ms / bin_size_ms))\n",
    "    \n",
    "    # We'll create a 2D array: shape = (len(good_clusters), n_bins)\n",
    "    spike_matrix = np.zeros((len(good_clusters), n_bins), dtype=np.int32)\n",
    "    \n",
    "    # Make a quick index from cluster_id -> row index in spike_matrix\n",
    "    cluster_index_map = {clust_id: i for i, clust_id in enumerate(good_clusters)}\n",
    "    \n",
    "    # Digitize times to bin indices\n",
    "    bin_indices = (sess_spike_times // bin_size_ms).astype(int)\n",
    "    \n",
    "    # Accumulate counts in each bin\n",
    "    for t_bin, clust in zip(bin_indices, sess_spike_clusters):\n",
    "        if 0 <= t_bin < n_bins:  # just a safety check\n",
    "            if clust in cluster_index_map:\n",
    "                spike_matrix[cluster_index_map[clust], t_bin] += 1\n",
    "    \n",
    "    return spike_matrix\n",
    "\n",
    "def main():\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Load the JSON file containing session file sizes in bytes\n",
    "    # -------------------------------------------------------------------------\n",
    "    kilosort_folder = \"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/28032024_31032024_combined_all\"\n",
    "    json_path =os.path.join(kilosort_folder, [i for i in os.listdir(kilosort_folder) if i.endswith('.json')][0])\n",
    "    print(json_path)\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        session_dict = json.load(f)\n",
    "\n",
    "    print(session_dict)\n",
    "    # session_dict: { file_path: file_size_in_bytes, ... }\n",
    "    # e.g. \"/path/to/2024-03-21_12-59-35/continuous.dat\": 28106707968\n",
    "    \n",
    "    # Sort the sessions by file_path or by date/time—adjust to your preference\n",
    "    session_items = sorted(session_dict.items(), key=lambda x: x[0])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Compute absolute session boundaries in the concatenated timeline (ms)\n",
    "    # -------------------------------------------------------------------------\n",
    "    bytes_per_sample = 768\n",
    "    sampling_rate_hz = 30000  # 30 kHz\n",
    "    # => each sample = 768 bytes\n",
    "    # => 1 ms corresponds to 30 samples at 30 kHz\n",
    "    \n",
    "    session_info = []  # will hold (file_path, start_ms, end_ms) in the *concatenated* timeline\n",
    "    cumulative_ms = 0.0\n",
    "    \n",
    "    for (file_path, file_size) in session_items:\n",
    "        # number of raw samples in this session\n",
    "        n_samples = file_size / bytes_per_sample\n",
    "        \n",
    "        # convert to ms:  n_samples / 30  (since 30 samples = 1 ms)\n",
    "        duration_ms = n_samples / 30.0\n",
    "        start_ms = cumulative_ms\n",
    "        end_ms   = cumulative_ms + duration_ms\n",
    "        \n",
    "        session_info.append((file_path, start_ms, end_ms))\n",
    "        cumulative_ms = end_ms\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Load spike_times and spike_clusters from Kilosort output\n",
    "    #    Convert from sample indices (30 kHz) to ms\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    spike_times_samples = np.load(os.path.join(kilosort_folder, 'spike_times.npy'))\n",
    "    spike_clusters      = np.load(os.path.join(kilosort_folder, 'spike_clusters.npy'))\n",
    "    \n",
    "    # Make sure they are both squeezed to 1D\n",
    "    spike_times_samples = spike_times_samples.squeeze()\n",
    "    spike_clusters      = spike_clusters.squeeze()\n",
    "    \n",
    "    # Convert from samples to ms\n",
    "    spike_times_ms = spike_times_samples / 30.0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Load cluster labels from TSVs & choose 'good' clusters\n",
    "    # -------------------------------------------------------------------------\n",
    "    cluster_group_tsv   = os.path.join(kilosort_folder, 'cluster_group.tsv')\n",
    "    cluster_kslabel_tsv = os.path.join(kilosort_folder, 'cluster_KSlabel.tsv')\n",
    "    cluster_labels = load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv)\n",
    "    \n",
    "    good_clusters = get_good_clusters(cluster_labels)\n",
    "    print(f\"Found {len(good_clusters)} 'good' clusters.\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) For each session, load sample_numbers.npy to find the first sync pulse,\n",
    "    #    then truncate everything prior to that pulse for binning.\n",
    "    # -------------------------------------------------------------------------\n",
    "    bin_size_ms = 25\n",
    "    session_spike_counts = []  # will hold one array per session\n",
    "    \n",
    "    for i, (file_path, session_abs_start, session_abs_end) in enumerate(session_info):\n",
    "        # The absolute session boundaries in the concatenated timeline are\n",
    "        #  [session_abs_start, session_abs_end).\n",
    "        file_path_volumes = file_path.replace('/ceph/', '/Volumes/')\n",
    "        session_dir = os.path.dirname(file_path_volumes)\n",
    "        sync_file   = os.path.join(session_dir, 'sample_numbers.npy')\n",
    "        print(sync_file)\n",
    "        \n",
    "        if not os.path.exists(sync_file):\n",
    "            print(f\"Warning: sync_file not found for {file_path}. Skipping first-sync truncation.\")\n",
    "            # We'll treat the entire session from session_abs_start to session_abs_end\n",
    "            # i.e. no extra offset.\n",
    "            first_sync_ms = 0.0\n",
    "        else:\n",
    "            # Load the sample_numbers array\n",
    "            sample_numbers = np.load(sync_file).squeeze()\n",
    "            # Typically it's 1D. We'll assume the first sync pulse is sample_numbers[0].\n",
    "            first_sync_sample = sample_numbers[0]\n",
    "            # Convert that to ms\n",
    "            first_sync_ms = first_sync_sample / 30.0\n",
    "        \n",
    "        # Now we want to shift the session's start forward by first_sync_ms\n",
    "        # So that time=0 in our local bins corresponds to the first sync pulse\n",
    "        new_session_start = session_abs_start + first_sync_ms\n",
    "        new_session_duration = session_abs_end - new_session_start\n",
    "        \n",
    "        # Bin the spikes for this truncated session\n",
    "        spike_count_mat = bin_spikes(\n",
    "            spike_times_ms=spike_times_ms,\n",
    "            spike_clusters=spike_clusters,\n",
    "            good_clusters=good_clusters,\n",
    "            bin_size_ms=bin_size_ms,\n",
    "            session_offset=new_session_start,\n",
    "            session_duration_ms=new_session_duration\n",
    "        )\n",
    "        \n",
    "        session_spike_counts.append(spike_count_mat)\n",
    "        \n",
    "        print(f\"Session {i}: {file_path}\")\n",
    "        print(f\"   Original: start_ms={session_abs_start:.2f}, end_ms={session_abs_end:.2f}\")\n",
    "        print(f\"   First sync pulse at {first_sync_sample} samples => {first_sync_ms:.2f} ms\")\n",
    "        print(f\"   Final binning range: [{new_session_start:.2f}, {new_session_start + new_session_duration:.2f}) ms\")\n",
    "        print(f\"   Output shape: {spike_count_mat.shape}\\n\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) Save each session’s 2D spike count matrix\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_dir = '/Users/AdamHarris/Desktop/test_output_mats'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, (file_path, _, _) in enumerate(session_info):\n",
    "        out_name = f\"binnedSpikes_session{i}.npy\"\n",
    "        out_path = os.path.join(output_dir, out_name)\n",
    "        np.save(out_path, session_spike_counts[i])\n",
    "        print(f\"Saved binned spikes for session {i} -> {out_path}\")\n",
    "    \n",
    "    print(\"All sessions processed and binned spike arrays saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = r'/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/MetaData.xlsx - bp01_sleep.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cohort_and_mouse_id(filepath):\n",
    "    \"\"\"\n",
    "    Extracts the cohort number and mouse ID from a given file path.\n",
    "    Args:\n",
    "        filepath (str): The file path from which to extract the cohort number and mouse ID.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the cohort number (str) and mouse ID (str) if the pattern\n",
    "               is found, otherwise (None, None).\n",
    "    \"\"\"\n",
    "    match = re.search(r\"cohort(\\d+)/.*?/([^/]+)/\", filepath)\n",
    "    if match:\n",
    "        cohort = match.group(1)  # Extract the cohort number\n",
    "        mouse_id = match.group(2)  # Extract the mouse ID\n",
    "        return cohort, mouse_id\n",
    "    else:\n",
    "        print('cannot retrieve mouse and cohort from filepath')\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def load_json(json_path):\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        session_dict = json.load(f)\n",
    "    \n",
    "    session_items = session_dict.items()\n",
    "\n",
    "    return session_items\n",
    "    \n",
    "\n",
    "def load_metadata(cohort, mouse, data_folder):\n",
    "\n",
    "    \"\"\"\n",
    "    Load metadata for a given mouse in a given cohort.\n",
    "    Returns two DataFrames: one for awake, one for sleep.\n",
    "    \"\"\"\n",
    "    data_directory = f\"{data_folder}/cohort{cohort}/\"   \n",
    "    metadata_path= f\"{data_directory}/MetaData.xlsx - {mouse}.csv\"\n",
    "\n",
    "    metadata_awake = pd.read_csv(metadata_path, delimiter=',',dtype=str)\n",
    "    filtered_metadata_awake=metadata_awake[metadata_awake['Include']==1]\n",
    "    \n",
    "    try:\n",
    "        metadata_path_sleep= f\"{data_directory}/MetaData.xlsx - {mouse}_sleep.csv\"\n",
    "        metadata_sleep = pd.read_csv(metadata_path_sleep, delimiter=',',dtype=str)\n",
    "        filtered_metadata_sleep=metadata_sleep[metadata_sleep['Include']==1]\n",
    "        return filtered_metadata_awake, filtered_metadata_sleep\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print('No sleep metadata found for this mouse')\n",
    "        return filtered_metadata_awake, None\n",
    "    \n",
    "\n",
    "def extract_ephys_date_time(filepath):\n",
    "    \"\"\"\n",
    "    Extracts the date and time from a given file path using a regular expression.\n",
    "\n",
    "    The function searches for a date-time pattern in the format 'YYYY-MM-DD_HH-MM-SS'\n",
    "    within the provided file path. If a match is found, it returns the date and time\n",
    "    as separate strings. If no match is found, it returns (None, None).\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The file path containing the date-time information.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the date and time as strings (date, time).\n",
    "               If no match is found, returns (None, None).\n",
    "    \"\"\"\n",
    "    # Regular expression to capture date-time format\n",
    "    match = re.search(r\"(\\d{4}-\\d{2}-\\d{2})_(\\d{2}-\\d{2}-\\d{2})\", filepath)\n",
    "    if match:\n",
    "        date = match.group(1)\n",
    "        time = match.group(2)\n",
    "        return date, time\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def wake_or_sleep(ephys_filepath, metadata_awake, metadata_sleep):\n",
    "    \"\"\"\n",
    "    Determines whether an electrophysiology (ephys) session is an awake or sleep session based on metadata.\n",
    "    Args:\n",
    "        ephys_filepath (str): The file path to the ephys data file.\n",
    "        metadata_awake (pd.DataFrame): A DataFrame containing metadata for awake sessions.\n",
    "        metadata_sleep (pd.DataFrame): A DataFrame containing metadata for sleep sessions.\n",
    "    Returns:\n",
    "        str: 'awake' if the session is an awake session, 'sleep' if the session is a sleep session, or None if the session is not found in either metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    sleep_status = None\n",
    "\n",
    "    date, time = extract_ephys_date_time(ephys_filepath)\n",
    "    \n",
    "    # first try awake metadata\n",
    "\n",
    "    filtered_awake =  metadata_awake[(metadata_awake[\"Date\"] == date) & (metadata_awake[\"Ephys\"] == time)]\n",
    "    \n",
    "    if len(filtered_awake>1):\n",
    "        print('More than one awake entry found for this date and time')\n",
    "    if len(filtered_awake==0):\n",
    "        print('Not an awake session')\n",
    "    if len(filtered_awake==1):\n",
    "        print('awake session')\n",
    "        sleep_status = 'awake'\n",
    "\n",
    "    if sleep_status =='awake':\n",
    "        return sleep_status\n",
    "    \n",
    "    else:\n",
    "        # then try sleep metadata\n",
    "        filtered_sleep = metadata_sleep[(metadata_sleep[\"Date\"] == date) & (metadata_sleep[\"Ephys\"] == time)]\n",
    "\n",
    "        if len(filtered_sleep>1):\n",
    "            print('More than one sleep entry found for this date and time')\n",
    "        if len(filtered_sleep==0):\n",
    "            print('Not an sleep session')\n",
    "        if len(filtered_sleep==1):\n",
    "            print('sleep session')\n",
    "            sleep_status = 'sleep'\n",
    "\n",
    "        return sleep_status\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_behaviour_txt(data_path, mouse, cohort, date, Behaviour_timestamp, Structure_abstract, int_subject_IDs=True):\n",
    "    \"\"\"\n",
    "    Retrieves the pycontrol output file for awake sessions and produces a dictionary \n",
    "    containing the raw pycontrol data.\n",
    "    Parameters:\n",
    "    data_path (str): The base directory path where the data is stored.\n",
    "    mouse (str): The identifier for the mouse.\n",
    "    cohort (str): The cohort number or identifier.\n",
    "    date (str): The date of the session in 'YYYYMMDD' format.\n",
    "    Behaviour_timestamp (str): The timestamp of the behaviour session.\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are event names and values are numpy arrays of event times.\n",
    "    The function performs the following steps:\n",
    "    1. Constructs the file path for the behaviour data file.\n",
    "    2. Reads the file and extracts relevant information.\n",
    "    3. Parses session information including experiment name, task name, subject ID, and start date.\n",
    "    4. Extracts state and event IDs, and session data.\n",
    "    5. Converts subject ID to integer if `int_subject_IDs` is True.\n",
    "    6. Returns a dictionary with event names as keys and numpy arrays of event times as values.\n",
    "    Note:\n",
    "    - The function assumes that the file format and structure are consistent with the expected format.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    Behaviourfile_path = f\"{data_path}/cohort{cohort}/{mouse}/behaviour/{mouse}-{date}-{Behaviour_timestamp}.txt\"\n",
    "\n",
    "    print('Importing data file: '+os.path.split(Behaviourfile_path)[1])\n",
    "\n",
    "    with open(Behaviourfile_path, 'r') as f:\n",
    "        all_lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "\n",
    "    # Extract and store session information.\n",
    "    file_name = os.path.split(Behaviourfile_path)[1]\n",
    "    Event = namedtuple('Event', ['time','name'])\n",
    "\n",
    "    info_lines = [line[2:] for line in all_lines if line[0]=='I']\n",
    "\n",
    "    experiment_name = next(line for line in info_lines if 'Experiment name' in line).split(' : ')[1]\n",
    "    task_name       = next(line for line in info_lines if 'Task name'       in line).split(' : ')[1]\n",
    "    subject_ID_string    = next(line for line in info_lines if 'Subject ID'      in line).split(' : ')[1]\n",
    "    datetime_string      = next(line for line in info_lines if 'Start date'      in line).split(' : ')[1]\n",
    "\n",
    "\n",
    "    if int_subject_IDs: # Convert subject ID string to integer.\n",
    "        subject_ID = int(''.join([i for i in subject_ID_string if i.isdigit()]))\n",
    "    else:\n",
    "        subject_ID = subject_ID_string\n",
    "\n",
    "    datetime = datetime.strptime(datetime_string, '%Y/%m/%d %H:%M:%S')\n",
    "    datetime_string = datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Extract and store session data.\n",
    "\n",
    "    state_IDs = eval(next(line for line in all_lines if line[0]=='S')[2:])\n",
    "    event_IDs = eval(next(line for line in all_lines if line[0]=='E')[2:])\n",
    "    variable_lines = [line[2:] for line in all_lines if line[0]=='V']\n",
    "\n",
    "    if Structure_abstract not in ['ABCD','AB','ABCDA2','ABCDE','ABCAD']:\n",
    "        pass\n",
    "    else:\n",
    "        structurexx = next(line for line in variable_lines if 'active_poke' in line).split(' active_poke ')[1]\n",
    "        if 'ot' in structurexx:\n",
    "            structurex=structurexx[:8]+']'\n",
    "        else:\n",
    "            structurex=structurexx\n",
    "\n",
    "        if Structure_abstract in ['ABCD','AB','ABCDE']:\n",
    "            structure=np.asarray((structurex[1:-1]).split(',')).astype(int)\n",
    "        else:\n",
    "            structure=structurex\n",
    "\n",
    "        ID2name = {v: k for k, v in {**state_IDs, **event_IDs}.items()}\n",
    "        data_lines = [line[2:].split(' ') for line in all_lines if line[0]=='D']\n",
    "        events = [Event(int(dl[0]), ID2name[int(dl[1])]) for dl in data_lines]\n",
    "        times = {event_name: np.array([ev.time for ev in events if ev.name == event_name])  \n",
    "                    for event_name in ID2name.values()}\n",
    "  \n",
    "    return times\n",
    "\n",
    "\n",
    "\n",
    "def make_trial_times_array(times_dic, target_binning=25, Structure_abstract='ABCD'):\n",
    "    \"\"\"\n",
    "    Generate an array of trial onset times from a dictionary of event timestamps.\n",
    "    Parameters:\n",
    "    times_dic (dict): Dictionary containing pycontrol trial events and their timestamps. \n",
    "                        The keys are event names, and the values are lists of timestamps.\n",
    "    target_binning (int, optional): The bin size to convert timestamps into. Default is 25 ms.\n",
    "    Structure_abstract (str, optional): The structure of the session. Common types are 'ABCD', 'ABCDE', 'AB', 'ABCAD'. \n",
    "                                        Default is 'ABCD'.\n",
    "    Returns:\n",
    "    np.ndarray: A 2D array where each row corresponds to a trial and each column corresponds to a state onset time.\n",
    "                The times are aligned such that the first state onset (e.g., 'A_on') is set to 0, and all times are \n",
    "                converted to the specified bin size.\n",
    "    Raises:\n",
    "    ValueError: If an unknown Structure_abstract is provided or if a required state is not found in times_dic.\n",
    "    Notes:\n",
    "    - The function aligns the trial times so that the first state onset (e.g., 'A_on') is set to 0.\n",
    "    - The timestamps are converted from milliseconds to the specified bin size.\n",
    "    - For 'ot' sessions, the first 'A_on' event is registered as 'A_on_first', and subsequent 'A_on' events are standard.\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    trial_times = []\n",
    "\n",
    "    if Structure_abstract == 'ABCD':\n",
    "        states = ['A_on', 'B_on', 'C_on', 'D_on']\n",
    "    elif Structure_abstract == 'ABCDE':\n",
    "        states = ['A_on', 'B_on', 'C_on', 'D_on', 'E_on']\n",
    "    elif Structure_abstract == 'AB':\n",
    "        states = ['A_on', 'B_on']\n",
    "    elif Structure_abstract == 'ABCAD':\n",
    "        states = ['A_on', 'B_on', 'C_on', 'A2_on', 'D_on']\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown Structure_abstract: {Structure_abstract}\")\n",
    "\n",
    "    for state in states:\n",
    "        if state not in times_dic:\n",
    "            raise ValueError(f\"State {state} not found in times_dic\")\n",
    "\n",
    "    first_A_on = times_dic['A_on'][0]\n",
    "\n",
    "    for i in range(len(times_dic[states[0]])):\n",
    "        trial = []\n",
    "        for state in states:\n",
    "            if state == 'A_on' and 'A_on_first' in times_dic:\n",
    "                trial.append(times_dic['A_on_first'][i] - first_A_on)\n",
    "            else:\n",
    "                trial.append(times_dic[state][i] - first_A_on)\n",
    "        trial_times.append(trial)\n",
    "\n",
    "    trial_times = np.array(trial_times)\n",
    "    trial_times = (trial_times / target_binning).astype(int)\n",
    "\n",
    "    return trial_times\n",
    "\n",
    "def make_pokes_dic(times_dic):\n",
    "    \n",
    "    return pokes_dic\n",
    "\n",
    "\n",
    "def get_behaviour_tsv(data_path, mouse, cohort, Behaviour_timestamp):\n",
    "\n",
    "    \"\"\"\n",
    "    retrieves the pycontrol output file for bigmaze tsv sessions and produces a dictionary \n",
    "    containing the events in file. \n",
    "    for our bigmaze files this will just be RSYNC as we are just doing openfield or objectvector \n",
    "    sessions\n",
    "    IMPORTANT: the later version of pycontrol has timestamps in seconds instead of milliseconds\n",
    "    so we will here be multiplying by 1000 to keep units the same\n",
    "    \"\"\"\n",
    "    Behaviourfile_path =   f\"{data_path}/cohort{cohort}/{mouse}/behaviour/{mouse}-{date}-{Behaviour_timestamp}.tsv\"\n",
    "    behaviour_df = pd.read_csv(Behaviourfile_path, sep='\\t')\n",
    "    behaviour_df[\"time\"] = behaviour_df[\"time\"]*1000\n",
    "    df_rsync = of_behaviour.query(\"type == 'event' & subtype == 'sync' & content == 'rsync'\")\n",
    "\n",
    "\n",
    "    return behaviour_df, df_rsync[\"time\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_tracking(data_path, mouse, cohort, date, Tracking_timestamp):\n",
    "    \"\"\"\n",
    "    Retrieves tracking data for a given mouse from a specified cohort and date.\n",
    "    Parameters:\n",
    "    data_path (str): The base directory path where the data is stored.\n",
    "    mouse (str): The identifier for the mouse.\n",
    "    cohort (int): The cohort number.\n",
    "    date (str): The date of the tracking data in 'YYYY-MM-DD' format.\n",
    "    Tracking_timestamp (str): The timestamp associated with the tracking data.\n",
    "    Returns:\n",
    "    tuple: A tuple containing the following elements:\n",
    "        - xy (DataFrame or None): DataFrame containing the xy coordinates from the sleap file, or None if not found.\n",
    "        - pinstate (DataFrame or None): DataFrame containing the pinstate data, or None if not found.\n",
    "        - ROIs (DataFrame or None): DataFrame containing the ROIs data, or None if not found.\n",
    "        - hd (DataFrame or None): DataFrame containing the head direction data, or None if not found.\n",
    "    Raises:\n",
    "    FileNotFoundError: If any of the required files are not found, a message is printed and the corresponding return value is set to None.\n",
    "    \"\"\"\n",
    "\n",
    "    if cohort == 7:\n",
    "        sleap_tag = 'cleaned_coordinates'\n",
    "        head_direction_tag = 'head_direction'\n",
    "        ROIs_tag = 'ROIs'\n",
    "\n",
    "    behaviour_folder = f\"{data_path}/cohort{cohort}/{mouse}/behaviour\"\n",
    "    files_in_behaviour = os.listdir(behaviour_folder)\n",
    "\n",
    "    # pinstate\n",
    "    try:\n",
    "        print(f\"finding pinstate file for {mouse}_pinstate_{date}-{Tracking_timestamp}\")\n",
    "        pinstate_path = f\"{behaviour_folder}/{mouse}_pinstate_{date}-{Tracking_timestamp}.csv\"\n",
    "        pinstate = pd.read_csv(pinstate_path)\n",
    "    except FileNotFoundError:\n",
    "        pinstate = None\n",
    "        print('Cannot find pinstate file')\n",
    "\n",
    "    # xy coords\n",
    "    try:\n",
    "        coords_search = [i for i in behaviour_folder if date in i and Tracking_timestamp in i and sleap_tag in i]\n",
    "        if len(coords_search==1):\n",
    "            xy = pd.read_csv(os.path.join(behaviour_folder, coords_search[0]))\n",
    "        else:\n",
    "            print(f'check for duplicates or missing file for {date}_{Tracking_timestamp}')\n",
    "            xy=None\n",
    "    except FileNotFoundError:\n",
    "        print('Cannot find sleap file')\n",
    "        xy = None\n",
    "\n",
    "    # ROIs:\n",
    "    try:\n",
    "        ROIs_search = [i for i in behaviour_folder if date in i and Tracking_timestamp in i and ROIs_tag in i]\n",
    "        if len(ROIs_search==1):\n",
    "            ROIs = pd.read_csv(os.path.join(behaviour_folder, ROIs_search[0]))\n",
    "        else:\n",
    "            print(f'check for duplicates or missing file for {date}_{Tracking_timestamp} ROIs')\n",
    "            ROIs=None\n",
    "    except FileNotFoundError:\n",
    "        print('Cannot find ROIs file')\n",
    "        ROIs=None\n",
    "    \n",
    "    # head_direction \n",
    "\n",
    "    try:\n",
    "        hd_search = [i for i in behaviour_folder if date in i and Tracking_timestamp in i and head_direction_tag in i]\n",
    "        if len(ROIs_search==1):\n",
    "            hd = pd.read_csv(os.path.join(behaviour_folder, hd_search[0]))\n",
    "        else:\n",
    "            print(f'check for duplicates or missing file for {date}_{Tracking_timestamp} head directions')\n",
    "            hd=None\n",
    "    except FileNotFoundError:\n",
    "        print('Cannot find head direction file')\n",
    "        hd=None\n",
    " \n",
    "\n",
    "    return  xy, pinstate, ROIs, hd\n",
    "\n",
    "\n",
    "def resample_tracking(sleap_df, pinstate, camera_fps=60, target_fps=40, body_part = 'head_back'):\n",
    "\n",
    "    \"\"\"\n",
    "    takes in sleap dataframe and pinstate and returna\n",
    "    a resampled and first sync pulse truncated\n",
    "    list of xy coordinates from a select stable bodypart\n",
    "    \"\"\"\n",
    "\n",
    "    sync_indices = np.where(pinstate > np.median(pinstate))[0]\n",
    "\n",
    "    first_sync_idx = sync_indices[0]\n",
    "    print(f\"First sync pulse detected at frame index: {first_sync_idx}\")\n",
    "\n",
    "    # Trim the SLEAP data to remove rows before the first sync pulse\n",
    "    print(\"Trimming data to start after the first sync pulse...\")\n",
    "    sleap_trimmed = sleap_df[first_sync_idx:]\n",
    "\n",
    "    print(f\"Resampling data from {camera_fps} FPS to {target_fps} FPS...\")\n",
    "    resample_factor = target_fps / camera_fps\n",
    "\n",
    "    coords = []\n",
    "\n",
    "    for column in [f'{body_part}.x', f'{body_part}.y']:\n",
    "        resampled_column = resample(sleap_trimmed[column].values, resampled_length)\n",
    "        coords.append(resampled_column)\n",
    "    \n",
    "    return coords\n",
    "\n",
    "\n",
    "def resample_ROIs(ROIs_df, pinstate, camera_fps=60, target_fps=40, body_part='head_back'):\n",
    "    \"\"\"\n",
    "    Resamples the ROIs dataframe to match the target FPS.\n",
    "    Args:\n",
    "        ROIs_df (pd.DataFrame): DataFrame containing the ROIs data.\n",
    "        pinstate (np.array): Array containing the pinstate data.\n",
    "        camera_fps (int): Original frames per second of the camera.\n",
    "        target_fps (int): Target frames per second for resampling.\n",
    "        body_part (str): The column name in the ROIs_df to be resampled.\n",
    "    Returns:\n",
    "        pd.Series: Resampled series of the specified body part.\n",
    "    \"\"\"\n",
    "    sync_indices = np.where(pinstate > np.median(pinstate))[0]\n",
    "    first_sync_idx = sync_indices[0]\n",
    "    print(f\"First sync pulse detected at frame index: {first_sync_idx}\")\n",
    "\n",
    "    # Trim the ROIs data to remove rows before the first sync pulse\n",
    "    print(\"Trimming data to start after the first sync pulse...\")\n",
    "    ROIs_trimmed = ROIs_df.iloc[first_sync_idx:]\n",
    "\n",
    "    # Calculate the resampling factor\n",
    "    resample_factor = camera_fps / target_fps\n",
    "\n",
    "    # Resample the specified body part column\n",
    "    body_part_column = ROIs_trimmed[body_part]\n",
    "    resampled_length = int(len(body_part_column) / resample_factor)\n",
    "    resampled_body_part = body_part_column.iloc[::int(resample_factor)].reset_index(drop=True)\n",
    "\n",
    "    return resampled_body_part\n",
    "\n",
    "def resample_headDirections(HD_df, pinstate, camera_fps=60, target_fps=40, body_part = 'head_back'):\n",
    "    \n",
    "    \"\"\"\n",
    "    takes in HD dataframe and pinstate and returns\n",
    "    a resampled and first sync pulse truncated\n",
    "    list of xy coordinates from a select stable bodypart\n",
    "    \"\"\"\n",
    "\n",
    "    sync_indices = np.where(pinstate > np.median(pinstate))[0]\n",
    "\n",
    "    first_sync_idx = sync_indices[0]\n",
    "    print(f\"First sync pulse detected at frame index: {first_sync_idx}\")\n",
    "\n",
    "    # Trim the SLEAP data to remove rows before the first sync pulse\n",
    "    print(\"Trimming data to start after the first sync pulse...\")\n",
    "    HD_df_trimmed = HD_df[first_sync_idx:]\n",
    "\n",
    "    print(f\"Resampling data from {camera_fps} FPS to {target_fps} FPS...\")\n",
    "    resample_factor = target_fps / camera_fps\n",
    "\n",
    "    HD_resampled = pd.DataFrame()\n",
    "\n",
    "    for column in HD_df_trimmed.columns:\n",
    "        resampled_column = resample(HD_df_trimmed[column].values, resampled_length)\n",
    "        HD_resampled[column] = resampled_column\n",
    "    \n",
    "    return HD_resampled\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data file: bp01-2024-03-26-110648.txt\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from datetime import datetime, date\n",
    "\n",
    "\n",
    "Behaviourfile_paths = [r\"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/bp01/behaviour/bp01-2024-03-26-110648.txt\"]\n",
    "Structure_abstract = \"ABCD\"\n",
    "for Behaviourfile_path in Behaviourfile_paths:\n",
    "    print('Importing data file: '+os.path.split(Behaviourfile_path)[1])\n",
    "\n",
    "    with open(Behaviourfile_path, 'r') as f:\n",
    "        all_lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "    int_subject_IDs=True\n",
    "\n",
    "    # Extract and store session information.\n",
    "    file_name = os.path.split(Behaviourfile_path)[1]\n",
    "    Event = namedtuple('Event', ['time','name'])\n",
    "\n",
    "    info_lines = [line[2:] for line in all_lines if line[0]=='I']\n",
    "\n",
    "    experiment_name = next(line for line in info_lines if 'Experiment name' in line).split(' : ')[1]\n",
    "    task_name       = next(line for line in info_lines if 'Task name'       in line).split(' : ')[1]\n",
    "    subject_ID_string    = next(line for line in info_lines if 'Subject ID'      in line).split(' : ')[1]\n",
    "    datetime_string      = next(line for line in info_lines if 'Start date'      in line).split(' : ')[1]\n",
    "\n",
    "\n",
    "    if int_subject_IDs: # Convert subject ID string to integer.\n",
    "        subject_ID = int(''.join([i for i in subject_ID_string if i.isdigit()]))\n",
    "    else:\n",
    "        subject_ID = subject_ID_string\n",
    "\n",
    "    datetime = datetime.strptime(datetime_string, '%Y/%m/%d %H:%M:%S')\n",
    "    datetime_string = datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Extract and store session data.\n",
    "\n",
    "    state_IDs = eval(next(line for line in all_lines if line[0]=='S')[2:])\n",
    "    event_IDs = eval(next(line for line in all_lines if line[0]=='E')[2:])\n",
    "    variable_lines = [line[2:] for line in all_lines if line[0]=='V']\n",
    "\n",
    "    if Structure_abstract not in ['ABCD','AB','ABCDA2','ABCDE','ABCAD']:\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        structurexx = next(line for line in variable_lines if 'active_poke' in line).split(' active_poke ')[1]\n",
    "        if 'ot' in structurexx:\n",
    "            structurex=structurexx[:8]+']'\n",
    "        else:\n",
    "            structurex=structurexx\n",
    "\n",
    "        if Structure_abstract in ['ABCD','AB','ABCDE']:\n",
    "            structure=np.asarray((structurex[1:-1]).split(',')).astype(int)\n",
    "        else:\n",
    "            structure=structurex\n",
    "\n",
    "        ID2name = {v: k for k, v in {**state_IDs, **event_IDs}.items()}\n",
    "        data_lines = [line[2:].split(' ') for line in all_lines if line[0]=='D']\n",
    "        events = [Event(int(dl[0]), ID2name[int(dl[1])]) for dl in data_lines]\n",
    "        times = {event_name: np.array([ev.time for ev in events if ev.name == event_name])  \n",
    "                    for event_name in ID2name.values()}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'times' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m make_trial_times_array(\u001b[43mtimes\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'times' is not defined"
     ]
    }
   ],
   "source": [
    "make_trial_times_array(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('7', 'bp01')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_cohort_and_mouse_id(\"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/21032024_23032024_combined_all/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = r'/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/MetaData.xlsx - bp01.csv'\n",
    "metadata = pd.read_csv(metadata_path, delimiter=',',dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Maze</th>\n",
       "      <th>Structure_abstract</th>\n",
       "      <th>Structure_no</th>\n",
       "      <th>Structure</th>\n",
       "      <th>Structure_session</th>\n",
       "      <th>Session_time</th>\n",
       "      <th>Reward_time</th>\n",
       "      <th>Entry</th>\n",
       "      <th>Behaviour</th>\n",
       "      <th>...</th>\n",
       "      <th>Recording_node_HPC</th>\n",
       "      <th>Performance</th>\n",
       "      <th>Notes1</th>\n",
       "      <th>Notes2</th>\n",
       "      <th>Notes3</th>\n",
       "      <th>Weight_pre</th>\n",
       "      <th>DLC_Scorer</th>\n",
       "      <th>ROI_method</th>\n",
       "      <th>Sampling</th>\n",
       "      <th>ROI_boundaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2024-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>6</td>\n",
       "      <td>4-6-8-3</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>173555</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20/40, 32 trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>6</td>\n",
       "      <td>4-6-8-3</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>123213</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40 trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>6</td>\n",
       "      <td>4-6-8-3</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E</td>\n",
       "      <td>133806</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36 trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>6</td>\n",
       "      <td>4-6-8-3</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>154855</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22/40, 44 trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>6</td>\n",
       "      <td>4-6-8-3</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>164414</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22/40, 34 trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2024-03-24</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>28</td>\n",
       "      <td>1-5-3-9</td>\n",
       "      <td>--</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>105246</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2024-03-24</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>29</td>\n",
       "      <td>3-4-6-7</td>\n",
       "      <td>--</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>114225</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lots of trials but looping</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2024-03-24</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>30</td>\n",
       "      <td>5-1-9-2</td>\n",
       "      <td>--</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E</td>\n",
       "      <td>132941</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>motivation not great - over 10 trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2024-03-24</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>28</td>\n",
       "      <td>1-5-3-9</td>\n",
       "      <td>--</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>141823</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>asleep on node 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2024-03-25</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>31</td>\n",
       "      <td>8-3-5-4</td>\n",
       "      <td>--</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>094640</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abspolutely flying through trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date Maze Structure_abstract Structure_no Structure  \\\n",
       "100  2024-01-22    1               ABCD            6   4-6-8-3   \n",
       "101  2024-01-23    1               ABCD            6   4-6-8-3   \n",
       "102  2024-01-23    1               ABCD            6   4-6-8-3   \n",
       "103  2024-01-23    1               ABCD            6   4-6-8-3   \n",
       "104  2024-01-23    1               ABCD            6   4-6-8-3   \n",
       "..          ...  ...                ...          ...       ...   \n",
       "195  2024-03-24    1               ABCD           28   1-5-3-9   \n",
       "196  2024-03-24    1               ABCD           29   3-4-6-7   \n",
       "197  2024-03-24    1               ABCD           30   5-1-9-2   \n",
       "198  2024-03-24    1               ABCD           28   1-5-3-9   \n",
       "199  2024-03-25    1               ABCD           31   8-3-5-4   \n",
       "\n",
       "    Structure_session Session_time Reward_time Entry Behaviour  ...  \\\n",
       "100                24           17         NaN     S    173555  ...   \n",
       "101                25           20         NaN     S    123213  ...   \n",
       "102                26           20         NaN     E    133806  ...   \n",
       "103                27           20         NaN     W    154855  ...   \n",
       "104                28           20         NaN     S    164414  ...   \n",
       "..                ...          ...         ...   ...       ...  ...   \n",
       "195                --           20         NaN     S    105246  ...   \n",
       "196                --           20         NaN     W    114225  ...   \n",
       "197                --           20         NaN     E    132941  ...   \n",
       "198                --           20         NaN     N    141823  ...   \n",
       "199                --           20         NaN     S    094640  ...   \n",
       "\n",
       "    Recording_node_HPC Performance                                 Notes1  \\\n",
       "100                NaN         NaN                       20/40, 32 trials   \n",
       "101                NaN         NaN                              40 trials   \n",
       "102                NaN         NaN                              36 trials   \n",
       "103                NaN         NaN                       22/40, 44 trials   \n",
       "104                NaN         NaN                       22/40, 34 trials   \n",
       "..                 ...         ...                                    ...   \n",
       "195                NaN         NaN                                    NaN   \n",
       "196                NaN         NaN             lots of trials but looping   \n",
       "197                NaN         NaN  motivation not great - over 10 trials   \n",
       "198                NaN         NaN                       asleep on node 2   \n",
       "199                NaN         NaN      abspolutely flying through trials   \n",
       "\n",
       "    Notes2 Notes3 Weight_pre DLC_Scorer ROI_method Sampling ROI_boundaries  \n",
       "100    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "101    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "102    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "103    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "104    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "..     ...    ...        ...        ...        ...      ...            ...  \n",
       "195    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "196    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "197    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "198    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "199    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "\n",
       "[100 rows x 28 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36    10-09-41\n",
       "37    11-16-54\n",
       "38    12-08-06\n",
       "39    13-53-38\n",
       "40    14-41-25\n",
       "Name: Ephys, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awake_cols_of_interest = 'Behaviour', 'Tracking', 'Ephys', 'Maze', 'Structure_abstract'\n",
    "\n",
    "metadata.columns\n",
    "\n",
    "# metadata[metadata['Date']=='2024-03-24']['Ephys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Sleep_box', 'Structure_abstract', 'Structure_no', 'Structure',\n",
       "       'Structure_sleep_session', 'Stage', 'Session_time', 'Tracking', 'Ephys',\n",
       "       'Behaviour_analyzed', 'Spike_sorted', 'Include', 'Recording_node_PFC',\n",
       "       'Recording_node_HPC', 'Notes1', 'Notes2', 'Notes3', 'Weight_pre',\n",
       "       'DLC_Scorer', 'ROI_method', 'Sampling', 'ROI_boundaries'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Sleep_box</th>\n",
       "      <th>Structure_abstract</th>\n",
       "      <th>Structure_no</th>\n",
       "      <th>Structure</th>\n",
       "      <th>Structure_sleep_session</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Session_time</th>\n",
       "      <th>Tracking</th>\n",
       "      <th>Ephys</th>\n",
       "      <th>...</th>\n",
       "      <th>Recording_node_PFC</th>\n",
       "      <th>Recording_node_HPC</th>\n",
       "      <th>Notes1</th>\n",
       "      <th>Notes2</th>\n",
       "      <th>Notes3</th>\n",
       "      <th>Weight_pre</th>\n",
       "      <th>DLC_Scorer</th>\n",
       "      <th>ROI_method</th>\n",
       "      <th>Sampling</th>\n",
       "      <th>ROI_boundaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2024-03-25</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>post_new1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-09-50</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"A\",</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date Sleep_box Structure_abstract Structure_no Structure  \\\n",
       "42  2024-03-25         1                  -            -         -   \n",
       "\n",
       "   Structure_sleep_session Stage Session_time Tracking     Ephys  ...  \\\n",
       "42               post_new1   NaN           20      NaN  10-09-50  ...   \n",
       "\n",
       "   Recording_node_PFC Recording_node_HPC Notes1 Notes2 Notes3 Weight_pre  \\\n",
       "42                  A                NaN    NaN    NaN    NaN       \"A\",   \n",
       "\n",
       "   DLC_Scorer ROI_method Sampling ROI_boundaries  \n",
       "42        NaN        NaN      NaN            NaN  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = r'/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-25_10-09-50/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat'\n",
    "date_example, time_example = extract_ephys_date_time(test_path)\n",
    "\n",
    "\n",
    "date_filtered =  metadata[(metadata[\"Date\"] == date_example) & (metadata[\"Ephys\"] == time_example)]\n",
    "if len(date_filtered>1):\n",
    "    print('More than one entry found for this date and time')\n",
    "else:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_12-59-35/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28106707968, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_13-23-31/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28535519232, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_13-48-00/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28141166592, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_14-12-33/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27900291072, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_15-50-23/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28036426752, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_16-14-38/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28030823424, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_16-39-20/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28268375040, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_17-03-45/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28307146752, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_18-15-20/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 41690207232, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_11-08-32/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 44299321344, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_11-56-18/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28109703168, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-19-36/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27991913472, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-44-10/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 7314868224, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-50-05/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27885699072, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_13-13-48/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27876805632, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_14-38-18/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 42007790592, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_15-12-26/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27983803392, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_15-37-15/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28526736384, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_16-00-28/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28819178496}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv):\n",
    "    \"\"\"\n",
    "    Load cluster labels from cluster_group.tsv and cluster_KSlabel.tsv,\n",
    "    returning a dictionary: { cluster_id: label }.\n",
    "    \n",
    "    The priority is:\n",
    "      - cluster_group.tsv (if present)\n",
    "      - fallback to cluster_KSlabel.tsv (if the cluster is missing a label in the first)\n",
    "    \"\"\"\n",
    "    # cluster_group file\n",
    "    if os.path.exists(cluster_group_tsv):\n",
    "        df_group = pd.read_csv(cluster_group_tsv, sep='\\t')\n",
    "        # If needed, rename columns if they don't match exactly:\n",
    "        if 'cluster_id' not in df_group.columns or 'group' not in df_group.columns:\n",
    "            df_group.columns = ['cluster_id', 'group']\n",
    "    else:\n",
    "        df_group = pd.DataFrame(columns=['cluster_id', 'group'])\n",
    "    \n",
    "    # cluster_KSlabel file\n",
    "    if os.path.exists(cluster_kslabel_tsv):\n",
    "        df_ks = pd.read_csv(cluster_kslabel_tsv, sep='\\t')\n",
    "        if 'cluster_id' not in df_ks.columns or 'KSlabel' not in df_ks.columns:\n",
    "            df_ks.columns = ['cluster_id', 'KSlabel']\n",
    "    else:\n",
    "        df_ks = pd.DataFrame(columns=['cluster_id', 'KSlabel'])\n",
    "    \n",
    "    # Convert cluster_id to int in both for easier merges\n",
    "    df_group['cluster_id'] = df_group['cluster_id'].astype(int, errors='ignore')\n",
    "    df_ks['cluster_id']    = df_ks['cluster_id'].astype(int, errors='ignore')\n",
    "    \n",
    "    # Merge them into a single DataFrame, outer join so we keep all\n",
    "    df_merge = pd.merge(df_group, df_ks, on='cluster_id', how='outer')\n",
    "    \n",
    "    # Create a dictionary mapping cluster_id -> final label\n",
    "    cluster_label_dict = {}\n",
    "    for idx, row in df_merge.iterrows():\n",
    "        clust_id = int(row['cluster_id'])\n",
    "        \n",
    "        group_label = None\n",
    "        ks_label    = None\n",
    "        \n",
    "        if 'group' in row and pd.notnull(row['group']):\n",
    "            group_label = row['group']\n",
    "        if 'KSlabel' in row and pd.notnull(row['KSlabel']):\n",
    "            ks_label = row['KSlabel']\n",
    "        \n",
    "        # Priority: if group_label is available, use it; otherwise fallback to ks_label\n",
    "        final_label = group_label if group_label is not None else ks_label\n",
    "        cluster_label_dict[clust_id] = final_label\n",
    "    \n",
    "    return cluster_label_dict\n",
    "\n",
    "def get_good_clusters(cluster_label_dict):\n",
    "    \"\"\"\n",
    "    Return a sorted list of cluster IDs considered 'good'.\n",
    "    Adjust the condition below for your labeling convention:\n",
    "    e.g. 'good', 'Good', 'su', 'SU'\n",
    "    \"\"\"\n",
    "    good = []\n",
    "    for clust_id, label in cluster_label_dict.items():\n",
    "        if label in ['good', 'Good', 'su', 'SU']:\n",
    "            good.append(clust_id)\n",
    "    return sorted(good)\n",
    "\n",
    "def bin_spikes(spike_times_ms, spike_clusters, good_clusters,\n",
    "               bin_size_ms=25,\n",
    "               session_offset=0,\n",
    "               session_duration_ms=None):\n",
    "    \"\"\"\n",
    "    Bin 'good' clusters' spikes into 25 ms bins for one session.\n",
    "\n",
    "    - spike_times_ms: 1D array of spike times (ms, in concatenated timeline).\n",
    "    - spike_clusters: 1D array of cluster IDs for each spike_time.\n",
    "    - good_clusters: list of cluster IDs that are 'good'.\n",
    "    - bin_size_ms: size of each bin in ms (default = 25 ms).\n",
    "    - session_offset: the starting time (ms) of this session in the *concatenated* timeline.\n",
    "    - session_duration_ms: total duration of this session from that offset.\n",
    "    \n",
    "    Returns a 2D array of shape (n_good_clusters, n_time_bins).\n",
    "    \"\"\"\n",
    "    if session_duration_ms <= 0:\n",
    "        # If there's no valid time range, return an empty matrix\n",
    "        return np.zeros((len(good_clusters), 0), dtype=np.int32)\n",
    "    \n",
    "    t_start = session_offset\n",
    "    t_end   = session_offset + session_duration_ms\n",
    "    \n",
    "    spikes_remaining = sum(spike_times_ms>t_end)\n",
    "    print(f\"Spikes remaining after this session: {spikes_remaining}\")\n",
    "    # Boolean mask for spikes in [t_start, t_end)\n",
    "    in_session_mask = (spike_times_ms >= t_start) & (spike_times_ms < t_end)\n",
    "    \n",
    "    # Subset spike times & clusters\n",
    "    # (Make sure these arrays are 1D with .squeeze())\n",
    "    sess_spike_times    = spike_times_ms[in_session_mask].squeeze() - t_start\n",
    "    sess_spike_clusters = spike_clusters[in_session_mask].squeeze()\n",
    "    \n",
    "    # Figure out how many bins we need\n",
    "    n_bins = int(np.ceil(session_duration_ms / bin_size_ms))\n",
    "    \n",
    "    # We'll create a 2D array: shape = (len(good_clusters), n_bins)\n",
    "    spike_matrix = np.zeros((len(good_clusters), n_bins), dtype=np.int32)\n",
    "    \n",
    "    # Make a quick index from cluster_id -> row index in spike_matrix\n",
    "    cluster_index_map = {clust_id: i for i, clust_id in enumerate(good_clusters)}\n",
    "    \n",
    "    # Digitize times to bin indices\n",
    "    bin_indices = (sess_spike_times // bin_size_ms).astype(int)\n",
    "    \n",
    "    # Accumulate counts in each bin\n",
    "    for t_bin, clust in zip(bin_indices, sess_spike_clusters):\n",
    "        if 0 <= t_bin < n_bins:  # just a safety check\n",
    "            if clust in cluster_index_map:\n",
    "                spike_matrix[cluster_index_map[clust], t_bin] += 1\n",
    "    \n",
    "    return spike_matrix\n",
    "\n",
    "def main():\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Load JSON & Compute Session Boundaries\n",
    "    # -------------------------------------------------------------------------\n",
    "    kilosort_folder = r\"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/21032024_23032024_combined_all/\"\n",
    "\n",
    "    json_path =os.path.join(kilosort_folder, [i for i in os.listdir(kilosort_folder) if i.endswith('.json') and not i.startswith('.')][0])\n",
    "    \n",
    "    session_items = load_json(json_path)\n",
    "\n",
    "    bytes_per_sample = 768\n",
    "    sampling_rate_hz = 30000\n",
    "    bin_size_ms = 25  # keep the same bin size throughout\n",
    "    \n",
    "    session_info = []\n",
    "    cumulative_ms = 0.0\n",
    "    \n",
    "    for (file_path, file_size) in session_items:\n",
    "        n_samples = file_size / bytes_per_sample\n",
    "        duration_ms = n_samples / 30.0  # 30 samples per ms\n",
    "        start_ms = cumulative_ms\n",
    "        end_ms   = cumulative_ms + duration_ms\n",
    "        session_info.append((file_path, start_ms, end_ms))\n",
    "        cumulative_ms = end_ms\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Load Kilosort spike times & clusters\n",
    "    #    Convert spike_times from samples (30kHz) -> ms\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    spike_times_samples = np.load(os.path.join(kilosort_folder, 'spike_times.npy')).squeeze()\n",
    "    spike_clusters      = np.load(os.path.join(kilosort_folder, 'spike_clusters.npy')).squeeze()\n",
    "    \n",
    "    spike_times_ms = spike_times_samples / 30.0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Load cluster labels & pick 'good' clusters\n",
    "    # -------------------------------------------------------------------------\n",
    "    cluster_group_tsv   = os.path.join(kilosort_folder, 'cluster_group.tsv')\n",
    "    cluster_kslabel_tsv = os.path.join(kilosort_folder, 'cluster_KSlabel.tsv')\n",
    "    cluster_labels = load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv)\n",
    "    \n",
    "    good_clusters = get_good_clusters(cluster_labels)\n",
    "    print(f\"Found {len(good_clusters)} 'good' clusters (pre-FR-filter).\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) For each session, load sample_numbers and shift so first sync pulse is t=0\n",
    "    #    Then bin spikes in 25ms bins.\n",
    "    # -------------------------------------------------------------------------\n",
    "    session_spike_counts = []\n",
    "    \n",
    "    for i, (file_path, session_abs_start, session_abs_end) in enumerate(session_info):\n",
    "        # Replace /ceph/ with /Volumes/ to locate sync pulses\n",
    "        file_path_volumes = file_path.replace('/ceph/', '/Volumes/')\n",
    "       \n",
    "        session_dir = os.path.dirname(file_path_volumes)\n",
    "        \n",
    "        session_dir_sync = session_dir.replace('continuous', 'events')\n",
    "        \n",
    "        session_dir_sync = f\"{session_dir_sync}/TTL\"\n",
    "\n",
    "        sync_file  = os.path.join(session_dir_sync, 'sample_numbers.npy')\n",
    "        \n",
    "        ap_sample_numbers_path = file_path_volumes.replace('continuous.dat', 'sample_numbers.npy')\n",
    "        ap_events_sync_path = os.path.join(session_dir_sync, 'sample_numbers.npy')\n",
    "\n",
    "        # \n",
    "        # ap_sample_numbers_path = (recording_path / f\"continuous/Neuropix-PXI-100.{subject_id}-AP/sample_numbers.npy\")\n",
    "        # lfp_sample_numbers_path = ( recording_path / f\"continuous/Neuropix-PXI-100.{subject_id}-LFP/sample_numbers.npy\")\n",
    "        # ap_events_sync_path = (recording_path / f\"events/Neuropix-PXI-100.{subject_id}-AP\" / \"TTL/sample_numbers.npy\")\n",
    "        # lfp_events_sync_path = (recording_path / f\"events/Neuropix-PXI-100.{subject_id}-LFP\" / \"TTL/sample_numbers.npy\")\n",
    "        \n",
    "        sync_times = np.load(ap_events_sync_path)[::2]  # pulse pulse start & end times, just take start\n",
    "        sample_numbers = np.load(ap_sample_numbers_path)\n",
    "        adjusted_sync_times = adjusted_syncs = np.array([np.argmax(sample_numbers > sync) for sync in sync_times]) #note vectorising uses to much memory\n",
    "\n",
    "        if not os.path.exists(sync_file):\n",
    "            print(f\"Warning: no sync_file found for {file_path}.  Using entire session as is.\")\n",
    "            first_sync_ms = 0.0\n",
    "        else:\n",
    "            sync_times = np.load(ap_events_sync_path)[::2]  # pulse pulse start & end times, just take start\n",
    "            sample_numbers = np.load(ap_sample_numbers_path)\n",
    "            adjusted_sync_times = adjusted_syncs = np.array([np.argmax(sample_numbers > sync) for sync in sync_times]) #note vectorising uses to much me\n",
    "            \n",
    "            if len(adjusted_sync_times) != 0:\n",
    "                first_sync_sample = adjusted_sync_times[0]\n",
    "                first_sync_ms = first_sync_sample / 30.0\n",
    "                print(f\"Session {i}: First sync pulse at {first_sync_sample} samples => {first_sync_ms:.2f} ms\")\n",
    "            else:\n",
    "                print(f\"Warning: empty sync_file for {file_path}. Using entire session as is.\")\n",
    "                first_sync_ms = 0.0\n",
    "        \n",
    "        new_session_start = session_abs_start + first_sync_ms\n",
    "        new_session_duration = session_abs_end - new_session_start\n",
    "        \n",
    "        # bin the spikes\n",
    "        spike_count_mat = bin_spikes(\n",
    "            spike_times_ms=spike_times_ms,\n",
    "            spike_clusters=spike_clusters,\n",
    "            good_clusters=good_clusters,\n",
    "            bin_size_ms=bin_size_ms,\n",
    "            session_offset=new_session_start,\n",
    "            session_duration_ms=new_session_duration\n",
    "        )\n",
    "        \n",
    "        session_spike_counts.append(spike_count_mat)\n",
    "        \n",
    "        print(f\"Session {i}: {file_path}\")\n",
    "        print(f\"    final time range: [{new_session_start:.2f}, {new_session_start + new_session_duration:.2f}) ms\")\n",
    "        print(f\"    binned shape: {spike_count_mat.shape}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) (Optional) Save raw binned results BEFORE FR filtering\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_dir = '/Users/AdamHarris/Desktop/test_output_mats_2425'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, (file_path, _, _) in enumerate(session_info):\n",
    "        out_path = os.path.join(output_dir, f\"binnedSpikes_session{i}_unfiltered.npy\")\n",
    "        np.save(out_path, session_spike_counts[i])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) Compute mean firing rates and identify any session with total zero mean\n",
    "    # -------------------------------------------------------------------------\n",
    "    # We'll build an array of shape [n_sessions, n_good_clusters]\n",
    "    # Then we can see if a cluster's FR < threshold in ANY session.\n",
    "    \n",
    "    n_sessions = len(session_spike_counts)\n",
    "    n_good     = len(good_clusters)\n",
    "    FR_means   = np.zeros((n_sessions, n_good), dtype=np.float32)\n",
    "    \n",
    "    for s in range(n_sessions):\n",
    "        spike_count_mat = session_spike_counts[s]  # shape: (n_good, n_time_bins)\n",
    "        total_spikes = np.sum(spike_count_mat, axis=1)  # sum across bins => shape: (n_good,)\n",
    "        \n",
    "        # each bin is 25 ms => 0.025 s. So total session time in seconds:\n",
    "        total_time_s = spike_count_mat.shape[1] * (bin_size_ms / 1000.0)\n",
    "        \n",
    "        # Firing rate (spikes/sec) for each cluster\n",
    "        # if total_time_s=0, we'd get a divide-by-zero; we can handle that\n",
    "        if total_time_s > 0:\n",
    "            cluster_fr = total_spikes / total_time_s\n",
    "        else:\n",
    "            cluster_fr = np.zeros(n_good, dtype=np.float32)\n",
    "        \n",
    "        FR_means[s, :] = cluster_fr\n",
    "    \n",
    "    # session_means: overall FR across all clusters for each session\n",
    "    # if session_means == 0 => that session is considered invalid\n",
    "    session_means = np.mean(FR_means, axis=1)  # shape: (n_sessions,)\n",
    "    session_mask  = (session_means != 0.0)     # ignore sessions w/ zero overall FR\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Filter out clusters whose FR dips below threshold in ANY valid session\n",
    "    # -------------------------------------------------------------------------\n",
    "    FR_THRESHOLD = 0.002  # e.g. 0.002 spikes/s\n",
    "    # For each cluster, we want to see if FR < 0.002 in any session where session_mask==True\n",
    "    # => cluster fails if it is < 0.002 in ANY valid session.\n",
    "    \n",
    "    # FR_means[session_mask, :] => shape: (#valid_sessions, n_good)\n",
    "    # (FR_means[session_mask, :] < FR_THRESHOLD) => bool array of same shape\n",
    "    # .any(axis=0) => True for any cluster that fails\n",
    "    clusters_below_threshold = (FR_means[session_mask, :] < FR_THRESHOLD).any(axis=0)\n",
    "    # The final mask is True for clusters that pass\n",
    "    FR_mask = ~clusters_below_threshold  # shape: (n_good,)\n",
    "    \n",
    "    # We'll create a new list of truly-good clusters\n",
    "    filtered_good_clusters = [c for c, keep in zip(good_clusters, FR_mask) if keep]\n",
    "    print(f\"Filtering: {np.sum(~FR_mask)} clusters fail FR< {FR_THRESHOLD} criterion.\") \n",
    "    print(f\"Remaining: {np.sum(FR_mask)} clusters.\")\n",
    "    \n",
    "    # Now, also filter the session_spike_counts arrays\n",
    "    filtered_spike_counts = []\n",
    "    for s in range(n_sessions):\n",
    "        mat = session_spike_counts[s]  # shape: (n_good, n_time_bins)\n",
    "        mat_filtered = mat[FR_mask, :] # keep only clusters that pass\n",
    "        filtered_spike_counts.append(mat_filtered)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save final, filtered results\n",
    "    # -------------------------------------------------------------------------\n",
    "    # We can either overwrite the old output or store in new files\n",
    "    filtered_output_dir = '/Users/AdamHarris/Desktop/test_output_mats_filtered_2123'\n",
    "    os.makedirs(filtered_output_dir, exist_ok=True)\n",
    "    \n",
    "    for s, (file_path, _, _) in enumerate(session_info):\n",
    "        out_path = os.path.join(filtered_output_dir, f\"binnedSpikes_session{s}.npy\")\n",
    "        np.save(out_path, filtered_spike_counts[s])\n",
    "    \n",
    "    # We might also want to save the new list of clusters\n",
    "    cluster_out_path = os.path.join(filtered_output_dir, \"good_clusters_filtered.npy\")\n",
    "    np.save(cluster_out_path, np.array(filtered_good_clusters))\n",
    "    \n",
    "    print(\"All sessions binned and firing-rate-filtered. Final # of clusters:\", len(filtered_good_clusters))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy.signal import resample\n",
    "from collections import namedtuple\n",
    "from datetime import datetime, date\n",
    "import re\n",
    "\n",
    "def load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv):\n",
    "    \"\"\"\n",
    "    Load cluster labels from cluster_group.tsv and cluster_KSlabel.tsv,\n",
    "    returning a dictionary: { cluster_id: label }.\n",
    "    \n",
    "    The priority is:\n",
    "      - cluster_group.tsv (if present)\n",
    "      - fallback to cluster_KSlabel.tsv (if the cluster is missing a label in the first)\n",
    "    \"\"\"\n",
    "    # cluster_group file\n",
    "    if os.path.exists(cluster_group_tsv):\n",
    "        df_group = pd.read_csv(cluster_group_tsv, sep='\\t')\n",
    "        # If needed, rename columns if they don't match exactly:\n",
    "        if 'cluster_id' not in df_group.columns or 'group' not in df_group.columns:\n",
    "            df_group.columns = ['cluster_id', 'group']\n",
    "    else:\n",
    "        df_group = pd.DataFrame(columns=['cluster_id', 'group'])\n",
    "    \n",
    "    # cluster_KSlabel file\n",
    "    if os.path.exists(cluster_kslabel_tsv):\n",
    "        df_ks = pd.read_csv(cluster_kslabel_tsv, sep='\\t')\n",
    "        if 'cluster_id' not in df_ks.columns or 'KSlabel' not in df_ks.columns:\n",
    "            df_ks.columns = ['cluster_id', 'KSlabel']\n",
    "    else:\n",
    "        df_ks = pd.DataFrame(columns=['cluster_id', 'KSlabel'])\n",
    "    \n",
    "    # Convert cluster_id to int in both for easier merges\n",
    "    df_group['cluster_id'] = df_group['cluster_id'].astype(int, errors='ignore')\n",
    "    df_ks['cluster_id']    = df_ks['cluster_id'].astype(int, errors='ignore')\n",
    "    \n",
    "    # Merge them into a single DataFrame, outer join so we keep all\n",
    "    df_merge = pd.merge(df_group, df_ks, on='cluster_id', how='outer')\n",
    "    \n",
    "    # Create a dictionary mapping cluster_id -> final label\n",
    "    cluster_label_dict = {}\n",
    "    for idx, row in df_merge.iterrows():\n",
    "        clust_id = int(row['cluster_id'])\n",
    "        \n",
    "        group_label = None\n",
    "        ks_label    = None\n",
    "        \n",
    "        if 'group' in row and pd.notnull(row['group']):\n",
    "            group_label = row['group']\n",
    "        if 'KSlabel' in row and pd.notnull(row['KSlabel']):\n",
    "            ks_label = row['KSlabel']\n",
    "        \n",
    "        # Priority: if group_label is available, use it; otherwise fallback to ks_label\n",
    "        final_label = group_label if group_label is not None else ks_label\n",
    "        cluster_label_dict[clust_id] = final_label\n",
    "    \n",
    "    return cluster_label_dict\n",
    "\n",
    "def get_good_clusters(cluster_label_dict):\n",
    "    \"\"\"\n",
    "    Return a sorted list of cluster IDs considered 'good'.\n",
    "    Adjust the condition below for your labeling convention:\n",
    "    e.g. 'good', 'Good', 'su', 'SU'\n",
    "    \"\"\"\n",
    "    good = []\n",
    "    for clust_id, label in cluster_label_dict.items():\n",
    "        if label in ['good', 'Good', 'su', 'SU']:\n",
    "            good.append(clust_id)\n",
    "    return sorted(good)\n",
    "\n",
    "def bin_spikes(spike_times_ms, spike_clusters, good_clusters,\n",
    "               bin_size_ms=25,\n",
    "               session_offset=0,\n",
    "               session_duration_ms=None):\n",
    "    \"\"\"\n",
    "    Bin 'good' clusters' spikes into 25 ms bins for one session.\n",
    "\n",
    "    - spike_times_ms: 1D array of spike times (ms, in concatenated timeline).\n",
    "    - spike_clusters: 1D array of cluster IDs for each spike_time.\n",
    "    - good_clusters: list of cluster IDs that are 'good'.\n",
    "    - bin_size_ms: size of each bin in ms (default = 25 ms).\n",
    "    - session_offset: the starting time (ms) of this session in the *concatenated* timeline.\n",
    "    - session_duration_ms: total duration of this session from that offset.\n",
    "    \n",
    "    Returns a 2D array of shape (n_good_clusters, n_time_bins).\n",
    "    \"\"\"\n",
    "    if session_duration_ms <= 0:\n",
    "        # If there's no valid time range, return an empty matrix\n",
    "        return np.zeros((len(good_clusters), 0), dtype=np.int32)\n",
    "    \n",
    "    t_start = session_offset\n",
    "    t_end   = session_offset + session_duration_ms\n",
    "    \n",
    "    spikes_remaining = sum(spike_times_ms>t_end)\n",
    "    print(f\"Spikes remaining after this session: {spikes_remaining}\")\n",
    "    # Boolean mask for spikes in [t_start, t_end)\n",
    "    in_session_mask = (spike_times_ms >= t_start) & (spike_times_ms < t_end)\n",
    "    \n",
    "    # Subset spike times & clusters\n",
    "    # (Make sure these arrays are 1D with .squeeze())\n",
    "    sess_spike_times    = spike_times_ms[in_session_mask].squeeze() - t_start\n",
    "    sess_spike_clusters = spike_clusters[in_session_mask].squeeze()\n",
    "    \n",
    "    # Figure out how many bins we need\n",
    "    n_bins = int(np.ceil(session_duration_ms / bin_size_ms))\n",
    "    \n",
    "    # We'll create a 2D array: shape = (len(good_clusters), n_bins)\n",
    "    spike_matrix = np.zeros((len(good_clusters), n_bins), dtype=np.int32)\n",
    "    \n",
    "    # Make a quick index from cluster_id -> row index in spike_matrix\n",
    "    cluster_index_map = {clust_id: i for i, clust_id in enumerate(good_clusters)}\n",
    "    \n",
    "    # Digitize times to bin indices\n",
    "    bin_indices = (sess_spike_times // bin_size_ms).astype(int)\n",
    "    \n",
    "    # Accumulate counts in each bin\n",
    "    for t_bin, clust in zip(bin_indices, sess_spike_clusters):\n",
    "        if 0 <= t_bin < n_bins:  # just a safety check\n",
    "            if clust in cluster_index_map:\n",
    "                spike_matrix[cluster_index_map[clust], t_bin] += 1\n",
    "    \n",
    "    return spike_matrix\n",
    "\n",
    "\n",
    "def extract_cohort_and_mouse_id(filepath):\n",
    "    \"\"\"\n",
    "    Extracts the cohort number and mouse ID from a given file path.\n",
    "    Args:\n",
    "        filepath (str): The file path from which to extract the cohort number and mouse ID.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the cohort number (str) and mouse ID (str) if the pattern\n",
    "               is found, otherwise (None, None).\n",
    "    \"\"\"\n",
    "    pattern = r'cohort(\\d+).*?(bp\\d+)'\n",
    "\n",
    "    match = re.search(pattern, kilosort_folder)\n",
    "    if match:\n",
    "        cohort_number = match.group(1)\n",
    "        mouse_id = match.group(2)\n",
    "        print(f\"Cohort number: {cohort_number}\")\n",
    "        print(f\"Mouse ID: {mouse_id}\")\n",
    "        return cohort_number, mouse_id\n",
    "    else:\n",
    "        print(\"No match found.\")\n",
    "\n",
    "\n",
    "def load_json(json_path):\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        session_dict = json.load(f)\n",
    "\n",
    "    print(session_dict)\n",
    "    # session_dict: { file_path: file_size_in_bytes, ... }\n",
    "    # e.g. \"/path/to/2024-03-21_12-59-35/continuous.dat\": 28106707968\n",
    "    \n",
    "    # Sort the sessions by file_path or by date/time—adjust to your preference\n",
    "    session_items = sorted(session_dict.items(), key=lambda x: x[0])\n",
    "\n",
    "    return session_items\n",
    "    \n",
    "\n",
    "def load_metadata(cohort, mouse, data_folder):\n",
    "\n",
    "    \"\"\"\n",
    "    Load metadata for a given mouse in a given cohort.\n",
    "    Returns two DataFrames: one for awake, one for sleep.\n",
    "    \"\"\"\n",
    "    data_directory = f\"{data_folder}/cohort{cohort}/\"   \n",
    "    metadata_path= f\"{data_directory}/MetaData.xlsx - {mouse}.csv\"\n",
    "\n",
    "    metadata_awake = pd.read_csv(metadata_path, delimiter=',',dtype=str)\n",
    "    # filtered_metadata_awake=metadata_awake[metadata_awake['Include']==1]\n",
    "    \n",
    "    try:\n",
    "        metadata_path_sleep= f\"{data_directory}/MetaData.xlsx - {mouse}_sleep.csv\"\n",
    "        metadata_sleep = pd.read_csv(metadata_path_sleep, delimiter=',',dtype=str)\n",
    "        # filtered_metadata_sleep=metadata_sleep[metadata_sleep['Include']==1]\n",
    "        return metadata_awake, metadata_sleep\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print('No sleep metadata found for this mouse')\n",
    "        return metadata_awake, None\n",
    "    \n",
    "\n",
    "def extract_ephys_date_time(filepath):\n",
    "    \"\"\"\n",
    "    Extracts the date and time from a given file path using a regular expression.\n",
    "\n",
    "    The function searches for a date-time pattern in the format 'YYYY-MM-DD_HH-MM-SS'\n",
    "    within the provided file path. If a match is found, it returns the date and time\n",
    "    as separate strings. If no match is found, it returns (None, None).\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The file path containing the date-time information.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the date and time as strings (date, time).\n",
    "               If no match is found, returns (None, None).\n",
    "    \"\"\"\n",
    "    # Regular expression to capture date-time format\n",
    "    match = re.search(r\"(\\d{4}-\\d{2}-\\d{2})_(\\d{2}-\\d{2}-\\d{2})\", filepath)\n",
    "    if match:\n",
    "        date = match.group(1)\n",
    "        time = match.group(2)\n",
    "        return date, time\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def wake_or_sleep(ephys_filepath, metadata_awake, metadata_sleep):\n",
    "    \"\"\"\n",
    "    Determines whether an electrophysiology (ephys) session is an awake or sleep session based on metadata.\n",
    "    Args:\n",
    "        ephys_filepath (str): The file path to the ephys data file.\n",
    "        metadata_awake (pd.DataFrame): A DataFrame containing metadata for awake sessions.\n",
    "        metadata_sleep (pd.DataFrame): A DataFrame containing metadata for sleep sessions.\n",
    "    Returns:\n",
    "        str: 'awake' if the session is an awake session, 'sleep' if the session is a sleep session, or None if the session is not found in either metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    sleep_status = None\n",
    "\n",
    "    date, time = extract_ephys_date_time(ephys_filepath)\n",
    "    \n",
    "    # first try awake metadata\n",
    "\n",
    "    filtered_awake =  metadata_awake[(metadata_awake[\"Date\"] == date) & (metadata_awake[\"Ephys\"] == time)]\n",
    "    \n",
    "    if len(filtered_awake)>1:\n",
    "        print('More than one awake entry found for this date and time')\n",
    "    if len(filtered_awake)==0:\n",
    "        pass\n",
    "        # print('Not an awake session')\n",
    "    if len(filtered_awake)==1:\n",
    "        # print('awake session')\n",
    "        sleep_status = 'awake'\n",
    "\n",
    "    if sleep_status =='awake':\n",
    "        return sleep_status\n",
    "    \n",
    "    else:\n",
    "        # then try sleep metadata\n",
    "        filtered_sleep = metadata_sleep[(metadata_sleep[\"Date\"] == date) & (metadata_sleep[\"Ephys\"] == time)]\n",
    "\n",
    "        if len(filtered_sleep)>1:\n",
    "            print('More than one sleep entry found for this date and time')\n",
    "        if len(filtered_sleep)==0:\n",
    "            pass\n",
    "            # print('Not n sleep session')\n",
    "        if len(filtered_sleep)==1:\n",
    "            # print('sleep session')\n",
    "            sleep_status = 'sleep'\n",
    "\n",
    "        if sleep_status == None:\n",
    "            \n",
    "            raise Exception(\"NOT IN METADATA\")\n",
    "        \n",
    "        return sleep_status\n",
    "\n",
    "\n",
    "def get_awake_sessions_info(ephys_filepath, metadata_awake):\n",
    "\n",
    "    \"\"\"\n",
    "    uses ephys file path from json and metadata dataframe\n",
    "    to retrieve information about the session and timestamps\n",
    "    to find tracking data and pycontrol data\n",
    "    \"\"\"\n",
    "\n",
    "    date, time = extract_ephys_date_time(ephys_filepath)\n",
    "\n",
    "    filtered_awake =  metadata_awake[(metadata_awake[\"Date\"] == date) & (metadata_awake[\"Ephys\"] == time)]\n",
    "    \n",
    "    Structure = filtered_awake[\"Structure\"].values[0]\n",
    "    Structure_abstract = filtered_awake[\"Structure_abstract\"].values[0]\n",
    "    Structure_no = filtered_awake[\"Structure_no\"].values[0]\n",
    "    Tracking_timestamp = filtered_awake[\"Tracking\"].values[0]\n",
    "    Behaviour_timestamp = filtered_awake[\"Behaviour\"].values[0]\n",
    "\n",
    "    return Structure, Structure_abstract, Structure_no, Tracking_timestamp, Behaviour_timestamp\n",
    "\n",
    "\n",
    "def get_behaviour_txt(data_path, mouse, cohort, date, Behaviour_timestamp, Structure_abstract, int_subject_IDs=True):\n",
    "    \"\"\"\n",
    "    Retrieves the pycontrol output file for awake sessions and produces a dictionary \n",
    "    containing the raw pycontrol data.\n",
    "    Parameters:\n",
    "    data_path (str): The base directory path where the data is stored.\n",
    "    mouse (str): The identifier for the mouse.\n",
    "    cohort (str): The cohort number or identifier.\n",
    "    date (str): The date of the session in 'YYYYMMDD' format.\n",
    "    Behaviour_timestamp (str): The timestamp of the behaviour session.\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are event names and values are numpy arrays of event times.\n",
    "    The function performs the following steps:\n",
    "    1. Constructs the file path for the behaviour data file.\n",
    "    2. Reads the file and extracts relevant information.\n",
    "    3. Parses session information including experiment name, task name, subject ID, and start date.\n",
    "    4. Extracts state and event IDs, and session data.\n",
    "    5. Converts subject ID to integer if `int_subject_IDs` is True.\n",
    "    6. Returns a dictionary with event names as keys and numpy arrays of event times as values.\n",
    "    Note:\n",
    "    - The function assumes that the file format and structure are consistent with the expected format.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    Behaviourfile_path = f\"{data_path}/cohort{cohort}/{mouse}/behaviour/{mouse}-{date}-{Behaviour_timestamp}.txt\"\n",
    "\n",
    "    print('Importing data file: '+os.path.split(Behaviourfile_path)[1])\n",
    "\n",
    "    with open(Behaviourfile_path, 'r') as f:\n",
    "        all_lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "\n",
    "    # Extract and store session information.\n",
    "    file_name = os.path.split(Behaviourfile_path)[1]\n",
    "    Event = namedtuple('Event', ['time','name'])\n",
    "\n",
    "    info_lines = [line[2:] for line in all_lines if line[0]=='I']\n",
    "\n",
    "    experiment_name = next(line for line in info_lines if 'Experiment name' in line).split(' : ')[1]\n",
    "    task_name       = next(line for line in info_lines if 'Task name'       in line).split(' : ')[1]\n",
    "    subject_ID_string    = next(line for line in info_lines if 'Subject ID'      in line).split(' : ')[1]\n",
    "    datetime_string      = next(line for line in info_lines if 'Start date'      in line).split(' : ')[1]\n",
    "\n",
    "\n",
    "    if int_subject_IDs: # Convert subject ID string to integer.\n",
    "        subject_ID = int(''.join([i for i in subject_ID_string if i.isdigit()]))\n",
    "    else:\n",
    "        subject_ID = subject_ID_string\n",
    "\n",
    "    datetime_ = datetime.strptime(datetime_string, '%Y/%m/%d %H:%M:%S')\n",
    "    datetime_string = datetime_.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Extract and store session data.\n",
    "\n",
    "    state_IDs = eval(next(line for line in all_lines if line[0]=='S')[2:])\n",
    "    event_IDs = eval(next(line for line in all_lines if line[0]=='E')[2:])\n",
    "    variable_lines = [line[2:] for line in all_lines if line[0]=='V']\n",
    "\n",
    "    if Structure_abstract not in ['ABCD','AB','ABCDA2','ABCDE','ABCAD']:\n",
    "        pass\n",
    "    else:\n",
    "        structurexx = next(line for line in variable_lines if 'active_poke' in line).split(' active_poke ')[1]\n",
    "        if 'ot' in structurexx:\n",
    "            structurex=structurexx[:8]+']'\n",
    "        else:\n",
    "            structurex=structurexx\n",
    "\n",
    "        if Structure_abstract in ['ABCD','AB','ABCDE']:\n",
    "            structure=np.asarray((structurex[1:-1]).split(',')).astype(int)\n",
    "        else:\n",
    "            structure=structurex\n",
    "\n",
    "        ID2name = {v: k for k, v in {**state_IDs, **event_IDs}.items()}\n",
    "        data_lines = [line[2:].split(' ') for line in all_lines if line[0]=='D']\n",
    "        events = [Event(int(dl[0]), ID2name[int(dl[1])]) for dl in data_lines]\n",
    "        times = {event_name: np.array([ev.time for ev in events if ev.name == event_name])  \n",
    "                    for event_name in ID2name.values()}\n",
    "  \n",
    "    return times\n",
    "\n",
    "\n",
    "\n",
    "def make_trial_times_array(times_dic, target_binning=25, Structure_abstract='ABCD'):\n",
    "    \"\"\"\n",
    "    Generate an array of trial onset times from a dictionary of event timestamps.\n",
    "    Parameters:\n",
    "    times_dic (dict): Dictionary containing pycontrol trial events and their timestamps. \n",
    "                        The keys are event names, and the values are lists of timestamps.\n",
    "    target_binning (int, optional): The bin size to convert timestamps into. Default is 25 ms.\n",
    "    Structure_abstract (str, optional): The structure of the session. Common types are 'ABCD', 'ABCDE', 'AB', 'ABCAD'. \n",
    "                                        Default is 'ABCD'.\n",
    "    Returns:\n",
    "    tuple: A 2D array where each row corresponds to a trial and each column corresponds to a state onset time,\n",
    "           and an integer representing the number of full trials.\n",
    "           If the number of full trials is 0, returns (None, 0).\n",
    "    Raises:\n",
    "    ValueError: If an unknown Structure_abstract is provided or if a required state is not found in times_dic.\n",
    "    Notes:\n",
    "    - The function aligns the trial times so that the first state onset (e.g., 'A_on') is set to 0.\n",
    "    - The timestamps are converted from milliseconds to the specified bin size.\n",
    "    - For 'ot' sessions, the first 'A_on' event is registered as 'A_on_first', and subsequent 'A_on' events are standard.\n",
    "    \"\"\"\n",
    "   \n",
    "    trial_times = []\n",
    "\n",
    "    if Structure_abstract == 'ABCD':\n",
    "        states = ['A_on', 'B_on', 'C_on', 'D_on']\n",
    "    elif Structure_abstract == 'ABCDE':\n",
    "        states = ['A_on', 'B_on', 'C_on', 'D_on', 'E_on']\n",
    "    elif Structure_abstract == 'AB':\n",
    "        states = ['A_on', 'B_on']\n",
    "    elif Structure_abstract == 'ABCAD':\n",
    "        states = ['A_on', 'B_on', 'C_on', 'A2_on', 'D_on']\n",
    "    elif Structure_abstract == 'ABCD-ot':\n",
    "        states = ['A_on', 'B_on', 'C_on', 'A2_on', 'D_on']\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown Structure_abstract: {Structure_abstract}\")\n",
    "\n",
    "    for state in states:\n",
    "        if state not in times_dic:\n",
    "            print(f\"State {state} not found in times_dic\")\n",
    "            return None, 0\n",
    "\n",
    "    num_trials = min(len(times_dic[state]) for state in states)\n",
    "\n",
    "    if num_trials == 0:\n",
    "        return None, 0\n",
    "\n",
    "    first_A_on = times_dic.get('A_on_first', times_dic['A_on'])[0]\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        trial = []\n",
    "        for state in states:\n",
    "            if state == 'A_on' and 'A_on_first' in times_dic:\n",
    "                trial.append(times_dic['A_on_first'][i] - first_A_on)\n",
    "            else:\n",
    "                trial.append(times_dic[state][i] - first_A_on)\n",
    "        trial_times.append(trial)\n",
    "\n",
    "    trial_times = np.array(trial_times)\n",
    "    trial_times = (trial_times / target_binning).astype(int)\n",
    "\n",
    "    return trial_times, num_trials\n",
    "\n",
    "def get_sync_to_trial_offset(times_dic, target_binning=25):\n",
    "    \"\"\"\n",
    "    Calculate the offset between the first 'rsync' event and the first 'A_on' event in bins.\n",
    "    \n",
    "    Args:\n",
    "        times_dic (dict): Dictionary containing event times.\n",
    "        target_binning (int): The bin size to convert timestamps into. Default is 25 ms.\n",
    "    \n",
    "    Returns:\n",
    "        float: The offset between the first 'rsync' event and the first 'A_on' event in bins.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If 'rsync' or 'A_on' events are not found in times_dic.\n",
    "    \"\"\"\n",
    "    rsync_times = times_dic['rsync']\n",
    "    a_on_times = times_dic.get('A_on_first', times_dic['A_on'])\n",
    "\n",
    "    first_rsync = rsync_times[0]\n",
    "    first_a_on = a_on_times[0]\n",
    "\n",
    "    offset_ms = first_a_on - first_rsync\n",
    "    offset_bins = (offset_ms / target_binning).astype(int)\n",
    "\n",
    "    return offset_bins\n",
    "\n",
    "\n",
    "def make_pokes_dic(times_dic, target_binning = 25):\n",
    "\n",
    "    \"\"\"\n",
    "    Aligns poke times to the first 'A_on' event and bins them.\n",
    "    Parameters:\n",
    "    times_dic (dict): Dictionary containing event times.\n",
    "    target_binning (int, optional): The binning interval. Default is 25.\n",
    "    Returns:\n",
    "    dict: Dictionary with aligned and binned poke times.\n",
    "    \"\"\"\n",
    "\n",
    "    first_a_on = times_dic.get('A_on_first', times_dic['A_on'])[0]\n",
    "    pokes_dic = {}\n",
    "\n",
    "    for key in times_dic.keys():\n",
    "        if 'poke' in key and 'out' not in key:\n",
    "            aligned_pokes = times_dic[key] - first_a_on\n",
    "            aligned_pokes = aligned_pokes[aligned_pokes >= 0]\n",
    "            aligned_pokes = (aligned_pokes / target_binning).astype(int)\n",
    "            pokes_dic[key] = aligned_pokes\n",
    "\n",
    "    return pokes_dic\n",
    "\n",
    "\n",
    "def get_behaviour_tsv(data_path, mouse, cohort, Behaviour_timestamp):\n",
    "\n",
    "    \"\"\"\n",
    "    retrieves the pycontrol output file for bigmaze tsv sessions and produces a dictionary \n",
    "    containing the events in file. \n",
    "    for our bigmaze files this will just be RSYNC as we are just doing openfield or objectvector \n",
    "    sessions\n",
    "    IMPORTANT: the later version of pycontrol has timestamps in seconds instead of milliseconds\n",
    "    so we will here be multiplying by 1000 to keep units the same\n",
    "    \"\"\"\n",
    "    Behaviourfile_path =   f\"{data_path}/cohort{cohort}/{mouse}/behaviour/{mouse}-{date}-{Behaviour_timestamp}.tsv\"\n",
    "    behaviour_df = pd.read_csv(Behaviourfile_path, sep='\\t')\n",
    "    behaviour_df[\"time\"] = behaviour_df[\"time\"]*1000\n",
    "    df_rsync = behaviour_df.query(\"type == 'event' & subtype == 'sync' & content == 'rsync'\")\n",
    "\n",
    "    return behaviour_df, df_rsync[\"time\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_tracking(data_path, mouse, cohort, date, Tracking_timestamp):\n",
    "    \"\"\"\n",
    "    Retrieves tracking data for a given mouse from a specified cohort and date.\n",
    "    Parameters:\n",
    "    data_path (str): The base directory path where the data is stored.\n",
    "    mouse (str): The identifier for the mouse.\n",
    "    cohort (int): The cohort number.\n",
    "    date (str): The date of the tracking data in 'YYYY-MM-DD' format.\n",
    "    Tracking_timestamp (str): The timestamp associated with the tracking data.\n",
    "    Returns:\n",
    "    tuple: A tuple containing the following elements:\n",
    "        - xy (DataFrame or None): DataFrame containing the xy coordinates from the sleap file, or None if not found.\n",
    "        - pinstate (DataFrame or None): DataFrame containing the pinstate data, or None if not found.\n",
    "        - ROIs (DataFrame or None): DataFrame containing the ROIs data, or None if not found.\n",
    "        - hd (DataFrame or None): DataFrame containing the head direction data, or None if not found.\n",
    "    Raises:\n",
    "    FileNotFoundError: If any of the required files are not found, a message is printed and the corresponding return value is set to None.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    sleap_tag = 'cleaned_coordinates'\n",
    "    head_direction_tag = 'head_direction'\n",
    "    ROIs_tag = 'ROIs'\n",
    "\n",
    "    behaviour_folder = f\"{data_path}/cohort{cohort}/{mouse}/behaviour\"\n",
    "    files_in_behaviour = os.listdir(behaviour_folder)\n",
    "\n",
    "    print(f\"{date}_{Tracking_timestamp}\")\n",
    "\n",
    "    # pinstate\n",
    "    try:\n",
    "        print(f\"finding pinstate file for {mouse}_pinstate_{date}-{Tracking_timestamp}\")\n",
    "        pinstate_path = f\"{behaviour_folder}/{mouse}_pinstate_{date}-{Tracking_timestamp}.csv\"\n",
    "        pinstate = pd.read_csv(pinstate_path)\n",
    "        print(f\"found pinstate file for {mouse}_pinstate_{date}-{Tracking_timestamp}\")\n",
    "    except FileNotFoundError:\n",
    "        pinstate = None\n",
    "        print('Cannot find pinstate file')\n",
    "\n",
    "    # xy coords\n",
    "    try:\n",
    "        coords_search = [i for i in files_in_behaviour if date in i and Tracking_timestamp in i and sleap_tag in i and not i.startswith(\"._\")]\n",
    "        print(coords_search)\n",
    "        if len(coords_search)==1:\n",
    "            xy = pd.read_csv(os.path.join(behaviour_folder, coords_search[0]))\n",
    "        else:\n",
    "            print(f'check for duplicates or missing file for {date}_{Tracking_timestamp}')\n",
    "            xy = pd.read_csv(os.path.join(behaviour_folder, coords_search[0]))\n",
    "    except FileNotFoundError:\n",
    "        print('Cannot find sleap file')\n",
    "        xy = None\n",
    "\n",
    "    # ROIs:\n",
    "    try:\n",
    "        ROIs_search = [i for i in files_in_behaviour if date in i and Tracking_timestamp in i and ROIs_tag in i and not i.startswith(\"._\")]\n",
    "        print(ROIs_search)\n",
    "        if len(ROIs_search)==1:\n",
    "            ROIs = pd.read_csv(os.path.join(behaviour_folder, ROIs_search[0]))\n",
    "        else:\n",
    "            print(f'check for duplicates or missing file for {date}_{Tracking_timestamp} ROIs')\n",
    "            ROIs=None\n",
    "            # ROIs = pd.read_csv(os.path.join(behaviour_folder, ROIs_search[0]))\n",
    "    except FileNotFoundError:\n",
    "        print('Cannot find ROIs file')\n",
    "        ROIs=None\n",
    "    \n",
    "    # head_direction \n",
    "\n",
    "    try:\n",
    "        hd_search = [i for i in files_in_behaviour if date in i and Tracking_timestamp in i and head_direction_tag in i and not i.startswith(\"._\")]\n",
    "        print(hd_search)\n",
    "        if len(hd_search)==1:\n",
    "            hd = pd.read_csv(os.path.join(behaviour_folder, hd_search[0]))\n",
    "        else:\n",
    "            print(f'check for duplicates or missing file for {date}_{Tracking_timestamp} head directions')\n",
    "            hd = pd.read_csv(os.path.join(behaviour_folder, hd_search[0]))\n",
    "    except FileNotFoundError:\n",
    "        print('Cannot find head direction file')\n",
    "        hd=None\n",
    " \n",
    "\n",
    "    return  xy, pinstate, ROIs, hd\n",
    "\n",
    "\n",
    "def resample_tracking(sleap_df, pinstate, camera_fps=60, target_fps=40, body_part = 'head_back'):\n",
    "    \"\"\"\n",
    "    takes in sleap dataframe and pinstate and returns\n",
    "    a resampled and first sync pulse truncated\n",
    "    list of xy coordinates from a select stable body part\n",
    "    \"\"\"\n",
    "\n",
    "    sync_indices = np.where(pinstate > np.median(pinstate))[0]\n",
    "    if sum(sync_indices)==0:\n",
    "        print('no sync pulses')\n",
    "        first_sync_idx = 0\n",
    "    else:\n",
    "        first_sync_idx = sync_indices[0]\n",
    "        print(f\"First sync pulse detected at frame index: {first_sync_idx}\")\n",
    "\n",
    "    # Trim the SLEAP data to remove rows before the first sync pulse\n",
    "    print(\"Trimming data to start after the first sync pulse...\")\n",
    "    sleap_trimmed = sleap_df[first_sync_idx:]\n",
    "\n",
    "    print(f\"Resampling data from {camera_fps} FPS to {target_fps} FPS...\")\n",
    "    resample_factor = target_fps / camera_fps\n",
    "    resampled_length = int(len(sleap_trimmed) * resample_factor)\n",
    "\n",
    "    coords = []\n",
    "\n",
    "    for column in [f'{body_part}.x', f'{body_part}.y']:\n",
    "        resampled_column = resample(sleap_trimmed[column].values, resampled_length)\n",
    "        coords.append(resampled_column)\n",
    "    \n",
    "    return np.array(coords).T\n",
    "\n",
    "\n",
    "def resample_ROIs(ROIs_df, pinstate, camera_fps=60, target_fps=40, body_part='head_back'):\n",
    "    \"\"\"\n",
    "    Resamples the ROIs dataframe to match the target FPS.\n",
    "    Args:\n",
    "        ROIs_df (pd.DataFrame): DataFrame containing the ROIs data.\n",
    "        pinstate (np.array): Array containing the pinstate data.\n",
    "        camera_fps (int): Original frames per second of the camera.\n",
    "        target_fps (int): Target frames per second for resampling.\n",
    "        body_part (str): The column name in the ROIs_df to be resampled.\n",
    "    Returns:\n",
    "        pd.Series: Resampled series of the specified body part.\n",
    "    \"\"\"\n",
    "\n",
    "    sync_indices = np.where(pinstate > np.median(pinstate))[0]\n",
    "    if sum(sync_indices)==0:\n",
    "        print('no sync pulses')\n",
    "        first_sync_idx = 0\n",
    "    else:\n",
    "        first_sync_idx = sync_indices[0]\n",
    "        print(f\"First sync pulse detected at frame index: {first_sync_idx}\")\n",
    "\n",
    "    # Trim the ROIs data to remove rows before the first sync pulse\n",
    "    print(\"Trimming data to start after the first sync pulse...\")\n",
    "    ROIs_trimmed = ROIs_df.iloc[first_sync_idx:]\n",
    "\n",
    "    # Calculate the resampling factor\n",
    "    resample_factor = target_fps / camera_fps\n",
    "\n",
    "    # Resample the specified body part column\n",
    "    body_part_column = ROIs_trimmed[body_part].values\n",
    "    arr = np.arange(len(body_part_column))\n",
    "\n",
    "# Build a boolean mask that is True if i % 3 != 2\n",
    "    mask = (arr % 3 != 2)\n",
    "    # resampled_length = int(len(body_part_column) * resample_factor)\n",
    "    resampled_body_part = body_part_column[mask]\n",
    "\n",
    "    return pd.Series(resampled_body_part)\n",
    "\n",
    "def resample_headDirections(HD_df, pinstate, camera_fps=60, target_fps=40):\n",
    "    \"\"\"\n",
    "    Resamples head direction data from a given dataframe to a target frames per second (FPS) \n",
    "    and trims the data to start after the first synchronization pulse.\n",
    "    Parameters:\n",
    "    HD_df (pd.DataFrame): DataFrame containing head direction data for between ears and head front to back \n",
    "                            directions.\n",
    "    pinstate (np.array): Array representing the pin state used to detect synchronization pulses.\n",
    "    camera_fps (int, optional): The original frames per second of the camera. Default is 60.\n",
    "    target_fps (int, optional): The target frames per second to resample the data to. Default is 40.\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the resampled head direction data starting from the \n",
    "                    first synchronization pulse.\n",
    "    \"\"\"\n",
    "\n",
    "    sync_indices = np.where(pinstate > np.median(pinstate))[0]\n",
    "    if sum(sync_indices)==0:\n",
    "        print('no sync pulses')\n",
    "        first_sync_idx = 0\n",
    "    else:\n",
    "        first_sync_idx = sync_indices[0]\n",
    "        print(f\"First sync pulse detected at frame index: {first_sync_idx}\")\n",
    "\n",
    "    # Trim the SLEAP data to remove rows before the first sync pulse\n",
    "    print(\"Trimming data to start after the first sync pulse...\")\n",
    "    HD_df_trimmed = HD_df[first_sync_idx:]\n",
    "\n",
    "    print(f\"Resampling data from {camera_fps} FPS to {target_fps} FPS...\")\n",
    "    resample_factor = camera_fps / target_fps\n",
    "\n",
    "    HD_resampled = pd.DataFrame()\n",
    "\n",
    "    for column in HD_df_trimmed.columns:\n",
    "        # Convert angles from -180 to 180 to 0 to 359.999\n",
    "        HD_df_trimmed.loc[:, column] = (HD_df_trimmed.loc[:, column] + 180) % 360\n",
    "\n",
    "        # Interpolate to avoid boundary issues\n",
    "        x = np.arange(len(HD_df_trimmed[column]))\n",
    "        x_new = np.linspace(0, len(HD_df_trimmed[column]) - 1, int(len(HD_df_trimmed[column]) / resample_factor))\n",
    "        resampled_column = np.interp(x_new, x, HD_df_trimmed[column])\n",
    "\n",
    "        # Ensure resampled values are within 0 to 359.999\n",
    "        resampled_column = resampled_column % 360\n",
    "        HD_resampled.loc[:, column] = resampled_column\n",
    "    \n",
    "    return HD_resampled\n",
    "\n",
    "\n",
    "def align_to_first_A(offset_bin, xy, ROIs, head_direction, ephys_mat):\n",
    "    \"\"\"\n",
    "    Aligns the data to the first 'A' state by cutting everything before the offset bin.\n",
    "    \n",
    "    Args:\n",
    "        offset_bin (int): The offset bin to align to the first 'A' state.\n",
    "        xy (np.ndarray): Resampled xy coordinates.\n",
    "        ROIs (pd.DataFrame): Resampled ROIs data.\n",
    "        head_direction (pd.DataFrame): Resampled head direction data.\n",
    "        ephys_mat (np.ndarray): Ephys firing rate matrix.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Aligned xy coordinates, ROIs data, head direction data, and ephys firing rate matrix.\n",
    "    \"\"\"\n",
    "    aligned_xy = xy[offset_bin:]\n",
    "    aligned_ROIs = np.array(ROIs.iloc[offset_bin:].reset_index(drop=True))\n",
    "    aligned_head_direction = head_direction.iloc[offset_bin:].reset_index(drop=True)\n",
    "    if ephys_mat != None:\n",
    "        aligned_ephys_mat = ephys_mat[:, offset_bin:]\n",
    "    else:\n",
    "        aligned_ephys_mat = None\n",
    "\n",
    "    return aligned_xy, aligned_ROIs, aligned_head_direction, aligned_ephys_mat\n",
    "\n",
    "\n",
    "def get_datestring(kilosort_folder):\n",
    "    folder_name = os.path.basename(kilosort_folder.rstrip('/'))\n",
    "    # folder_name might look like: \"21032024_23032024_combined_all\"\n",
    "\n",
    "    # 2) Split on underscore. We know the first 2 parts correspond to the date chunk we want.\n",
    "    parts = folder_name.split('_', 2)  # split at most 2 times\n",
    "    # parts -> ['21032024', '23032024', 'combined_all']\n",
    "\n",
    "    # 3) Re-join the first two parts with an underscore\n",
    "    date_string = '_'.join(parts[:2])\n",
    "\n",
    "    return date_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/24032024_25032024_combined_all\n",
      "_________________\n",
      "{'/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-24_10-09-41/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 48524037120, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-24_10-52-32/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28326644736, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-24_11-16-54/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 30252672000, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-24_11-42-14/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28095621120, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-24_12-08-06/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28266872832, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-24_13-29-36/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27932110848, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-24_13-53-38/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28021644288, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-24_14-17-53/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28520718336, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-24_14-41-25/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28483246080, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-25_09-03-38/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 55303621632, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-25_09-46-37/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27837287424, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-25_10-09-50/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27974624256, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-25_10-34-02/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 33744291840, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-25_11-02-13/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28089271296, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-25_12-49-53/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27935649792, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-25_13-13-06/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28040942592, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-25_13-36-34/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 30809816064, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-25_14-01-19/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28101823488, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-25_15-32-17/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 41847524352}\n",
      "Cohort number: 7\n",
      "Mouse ID: bp01\n",
      "session number 0 is sleep\n",
      "session number 1 is awake\n",
      "Importing data file: bp01-2024-03-24-105246.txt\n",
      "2024-03-24_105214\n",
      "finding pinstate file for bp01_pinstate_2024-03-24-105214\n",
      "found pinstate file for bp01_pinstate_2024-03-24-105214\n",
      "['bp01_2024-03-24-105214.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-24-105214.analysis_ROIs.csv']\n",
      "['bp01_2024-03-24-105214.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1782\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1782\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1782\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 2 is sleep\n",
      "session number 3 is awake\n",
      "Importing data file: bp01-2024-03-24-114225.txt\n",
      "2024-03-24_114159\n",
      "finding pinstate file for bp01_pinstate_2024-03-24-114159\n",
      "found pinstate file for bp01_pinstate_2024-03-24-114159\n",
      "['bp01_2024-03-24-114159.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-24-114159.analysis_ROIs.csv']\n",
      "['bp01_2024-03-24-114159.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1409\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1409\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1409\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 4 is sleep\n",
      "session number 5 is awake\n",
      "Importing data file: bp01-2024-03-24-132941.txt\n",
      "2024-03-24_132918\n",
      "finding pinstate file for bp01_pinstate_2024-03-24-132918\n",
      "found pinstate file for bp01_pinstate_2024-03-24-132918\n",
      "['bp01_2024-03-24-132918.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-24-132918.analysis_ROIs.csv']\n",
      "['bp01_2024-03-24-132918.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1271\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1271\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1271\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 6 is sleep\n",
      "session number 7 is awake\n",
      "Importing data file: bp01-2024-03-24-141823.txt\n",
      "2024-03-24_141737\n",
      "finding pinstate file for bp01_pinstate_2024-03-24-141737\n",
      "found pinstate file for bp01_pinstate_2024-03-24-141737\n",
      "['bp01_2024-03-24-141737.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-24-141737.analysis_ROIs.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/8ckmhq153z793vnzd_h_50s40000gn/T/ipykernel_28891/3866249454.py:556: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ROIs = pd.read_csv(os.path.join(behaviour_folder, ROIs_search[0]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bp01_2024-03-24-141737.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 2638\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 2638\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 2638\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 8 is sleep\n",
      "session number 9 is sleep\n",
      "session number 10 is awake\n",
      "Importing data file: bp01-2024-03-25-094640.txt\n",
      "2024-03-25_094548\n",
      "finding pinstate file for bp01_pinstate_2024-03-25-094548\n",
      "found pinstate file for bp01_pinstate_2024-03-25-094548\n",
      "['bp01_2024-03-25-094548.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-25-094548.analysis_ROIs.csv']\n",
      "['bp01_2024-03-25-094548.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 2697\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 2697\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 2697\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 11 is sleep\n",
      "session number 12 is awake\n",
      "Importing data file: bp01-2024-03-25-103421.txt\n",
      "2024-03-25_103336\n",
      "finding pinstate file for bp01_pinstate_2024-03-25-103336\n",
      "found pinstate file for bp01_pinstate_2024-03-25-103336\n",
      "['bp01_2024-03-25-103336.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-25-103336.analysis_ROIs.csv']\n",
      "['bp01_2024-03-25-103336.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 2554\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 2554\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 2554\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 13 is sleep\n",
      "session number 14 is awake\n",
      "Importing data file: bp01-2024-03-25-124958.txt\n",
      "2024-03-25_124925\n",
      "finding pinstate file for bp01_pinstate_2024-03-25-124925\n",
      "found pinstate file for bp01_pinstate_2024-03-25-124925\n",
      "['bp01_2024-03-25-124925.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-25-124925.analysis_ROIs.csv']\n",
      "['bp01_2024-03-25-124925.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1781\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1781\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1781\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 15 is sleep\n",
      "session number 16 is awake\n",
      "Importing data file: bp01-2024-03-25-133641.txt\n",
      "2024-03-25_133620\n",
      "finding pinstate file for bp01_pinstate_2024-03-25-133620\n",
      "found pinstate file for bp01_pinstate_2024-03-25-133620\n",
      "['bp01_2024-03-25-133620.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-25-133620.analysis_ROIs.csv']\n",
      "['bp01_2024-03-25-133620.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1100\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1100\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1100\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 17 is sleep\n",
      "session number 18 is awake\n",
      "2024-03-25_153209\n",
      "finding pinstate file for bp01_pinstate_2024-03-25-153209\n",
      "found pinstate file for bp01_pinstate_2024-03-25-153209\n",
      "['bp01_2024-03-25-153209.analysis_cleaned_coordinates.csv']\n",
      "[]\n",
      "check for duplicates or missing file for 2024-03-25_153209 ROIs\n",
      "['bp01_2024-03-25-153209.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1026\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1026\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/21032024_23032024_combined_all\n",
      "_________________\n",
      "{'/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_12-59-35/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28106707968, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_13-23-31/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28535519232, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_13-48-00/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28141166592, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_14-12-33/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27900291072, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_15-50-23/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28036426752, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_16-14-38/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28030823424, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_16-39-20/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28268375040, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_17-03-45/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28307146752, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_18-15-20/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 41690207232, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_11-08-32/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 44299321344, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_11-56-18/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28109703168, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-19-36/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27991913472, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-44-10/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 7314868224, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-50-05/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27885699072, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_13-13-48/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27876805632, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_14-38-18/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 42007790592, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_15-12-26/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27983803392, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_15-37-15/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28526736384, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_16-00-28/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28819178496}\n",
      "Cohort number: 7\n",
      "Mouse ID: bp01\n",
      "session number 0 is awake\n",
      "Importing data file: bp01-2024-03-21-125946.txt\n",
      "2024-03-21_125921\n",
      "finding pinstate file for bp01_pinstate_2024-03-21-125921\n",
      "found pinstate file for bp01_pinstate_2024-03-21-125921\n",
      "['bp01_2024-03-21-125921.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-21-125921.analysis_ROIs.csv']\n",
      "['bp01_2024-03-21-125921.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1139\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1139\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1139\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 1 is sleep\n",
      "session number 2 is awake\n",
      "Importing data file: bp01-2024-03-21-134812.txt\n",
      "2024-03-21_134743\n",
      "finding pinstate file for bp01_pinstate_2024-03-21-134743\n",
      "found pinstate file for bp01_pinstate_2024-03-21-134743\n",
      "['bp01_2024-03-21-134743.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-21-134743.analysis_ROIs.csv']\n",
      "['bp01_2024-03-21-134743.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1547\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1547\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1547\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 3 is sleep\n",
      "session number 4 is awake\n",
      "Importing data file: bp01-2024-03-21-155031.txt\n",
      "2024-03-21_155000\n",
      "finding pinstate file for bp01_pinstate_2024-03-21-155000\n",
      "found pinstate file for bp01_pinstate_2024-03-21-155000\n",
      "['bp01_2024-03-21-155000.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-21-155000.analysis_ROIs.csv']\n",
      "['bp01_2024-03-21-155000.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1695\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1695\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1695\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 5 is sleep\n",
      "session number 6 is awake\n",
      "Importing data file: bp01-2024-03-21-163931.txt\n",
      "2024-03-21_163856\n",
      "finding pinstate file for bp01_pinstate_2024-03-21-163856\n",
      "found pinstate file for bp01_pinstate_2024-03-21-163856\n",
      "['bp01_2024-03-21-163856.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-21-163856.analysis_ROIs.csv']\n",
      "['bp01_2024-03-21-163856.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1934\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1934\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1934\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 7 is sleep\n",
      "session number 8 is awake\n",
      "2024-03-21_181423\n",
      "finding pinstate file for bp01_pinstate_2024-03-21-181423\n",
      "found pinstate file for bp01_pinstate_2024-03-21-181423\n",
      "['bp01_2024-03-21-181423.analysis_cleaned_coordinates.csv']\n",
      "[]\n",
      "check for duplicates or missing file for 2024-03-21_181423 ROIs\n",
      "['bp01_2024-03-21-181423.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 3511\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 3511\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 9 is sleep\n",
      "session number 10 is awake\n",
      "Importing data file: bp01-2024-03-23-115627.txt\n",
      "2024-03-23_115549\n",
      "finding pinstate file for bp01_pinstate_2024-03-23-115549\n",
      "found pinstate file for bp01_pinstate_2024-03-23-115549\n",
      "['bp01_2024-03-23-115549.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-23-115549.analysis_ROIs.csv']\n",
      "['bp01_2024-03-23-115549.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1885\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1885\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1885\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 11 is sleep\n",
      "session number 12 is awake\n",
      "Importing data file: bp01-2024-03-23-124414.txt\n",
      "2024-03-23_124341\n",
      "finding pinstate file for bp01_pinstate_2024-03-23-124341\n",
      "found pinstate file for bp01_pinstate_2024-03-23-124341\n",
      "['bp01_2024-03-23-124341.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-23-124341.analysis_ROIs.csv']\n",
      "['bp01_2024-03-23-124341.analysis_head_direction.csv']\n",
      "no sync pulses\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "no sync pulses\n",
      "Trimming data to start after the first sync pulse...\n",
      "no sync pulses\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 13 is awake\n",
      "Importing data file: bp01-2024-03-23-125006.txt\n",
      "2024-03-23_124959\n",
      "finding pinstate file for bp01_pinstate_2024-03-23-124959\n",
      "found pinstate file for bp01_pinstate_2024-03-23-124959\n",
      "['bp01_2024-03-23-124959.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-23-124959.analysis_ROIs.csv']\n",
      "['bp01_2024-03-23-124959.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 235\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 235\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 235\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 14 is sleep\n",
      "session number 15 is awake\n",
      "Importing data file: bp01-2024-03-23-143833.txt\n",
      "2024-03-23_143804\n",
      "finding pinstate file for bp01_pinstate_2024-03-23-143804\n",
      "found pinstate file for bp01_pinstate_2024-03-23-143804\n",
      "['bp01_2024-03-23-143804.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-23-143804.analysis_ROIs.csv']\n",
      "['bp01_2024-03-23-143804.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1630\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1630\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1630\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 16 is sleep\n",
      "session number 17 is awake\n",
      "Importing data file: bp01-2024-03-23-153744.txt\n",
      "2024-03-23_153658\n",
      "finding pinstate file for bp01_pinstate_2024-03-23-153658\n",
      "found pinstate file for bp01_pinstate_2024-03-23-153658\n",
      "['bp01_2024-03-23-153658.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-03-23-153658.analysis_ROIs.csv']\n",
      "['bp01_2024-03-23-153658.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 2617\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 2617\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 2617\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 18 is sleep\n",
      "/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/02042024_03042024_combined_all\n",
      "_________________\n",
      "{'/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-02_10-32-39/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 45174260736, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-02_11-07-20/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 20742755328, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-02_11-24-26/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 29121361920, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-02_11-47-23/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 25198295040, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-02_12-07-16/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 19174210560, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-02_13-50-20/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28163091456, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-02_14-13-16/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27753200640, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-02_14-35-44/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28136481792, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-02_14-58-15/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 34620466176, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-02_16-54-25/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 42952651776, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-03_11-01-20/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 49626731520, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-03_11-39-22/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28635909120, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-03_12-01-46/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27818366976, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-03_12-23-33/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 21135098880, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-03_13-55-48/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28121131008, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-03_14-17-34/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27655188480, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-03_14-39-10/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 15135123456, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-04-03_14-51-44/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27655031808}\n",
      "Cohort number: 7\n",
      "Mouse ID: bp01\n",
      "session number 0 is sleep\n",
      "session number 1 is sleep\n",
      "session number 2 is awake\n",
      "Importing data file: bp01-2024-04-02-112517.txt\n",
      "2024-04-02_112456\n",
      "finding pinstate file for bp01_pinstate_2024-04-02-112456\n",
      "found pinstate file for bp01_pinstate_2024-04-02-112456\n",
      "['bp01_2024-04-02-112456.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-04-02-112456.analysis_ROIs.csv']\n",
      "['bp01_2024-04-02-112456.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1090\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1090\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1090\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 3 is sleep\n",
      "session number 4 is awake\n",
      "Importing data file: bp01-2024-04-02-120723.txt\n",
      "2024-04-02_120654\n",
      "finding pinstate file for bp01_pinstate_2024-04-02-120654\n",
      "found pinstate file for bp01_pinstate_2024-04-02-120654\n",
      "['bp01_2024-04-02-120654.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-04-02-120654.analysis_ROIs.csv']\n",
      "['bp01_2024-04-02-120654.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1609\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1609\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1609\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 5 is awake\n",
      "Importing data file: bp01-2024-04-02-135028.txt\n",
      "2024-04-02_134957\n",
      "finding pinstate file for bp01_pinstate_2024-04-02-134957\n",
      "found pinstate file for bp01_pinstate_2024-04-02-134957\n",
      "['bp01_2024-04-02-134957.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-04-02-134957.analysis_ROIs.csv']\n",
      "['bp01_2024-04-02-134957.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1695\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1695\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1695\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 6 is sleep\n",
      "session number 7 is awake\n",
      "Importing data file: bp01-2024-04-02-143556.txt\n",
      "2024-04-02_143528\n",
      "finding pinstate file for bp01_pinstate_2024-04-02-143528\n",
      "found pinstate file for bp01_pinstate_2024-04-02-143528\n",
      "['bp01_2024-04-02-143528.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-04-02-143528.analysis_ROIs.csv']\n",
      "['bp01_2024-04-02-143528.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1494\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1494\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1494\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 8 is sleep\n",
      "session number 9 is awake\n",
      "2024-04-02_165440\n",
      "finding pinstate file for bp01_pinstate_2024-04-02-165440\n",
      "found pinstate file for bp01_pinstate_2024-04-02-165440\n",
      "['bp01_2024-04-02-165440.analysis_cleaned_coordinates.csv']\n",
      "[]\n",
      "check for duplicates or missing file for 2024-04-02_165440 ROIs\n",
      "['bp01_2024-04-02-165440.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 2353\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 2353\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 10 is sleep\n",
      "session number 11 is awake\n",
      "Importing data file: bp01-2024-04-03-113956.txt\n",
      "2024-04-03_113931\n",
      "finding pinstate file for bp01_pinstate_2024-04-03-113931\n",
      "found pinstate file for bp01_pinstate_2024-04-03-113931\n",
      "['bp01_2024-04-03-113931.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-04-03-113931.analysis_ROIs.csv']\n",
      "['bp01_2024-04-03-113931.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1158\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1158\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1158\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 12 is sleep\n",
      "session number 13 is awake\n",
      "Importing data file: bp01-2024-04-03-122338.txt\n",
      "2024-04-03_122320\n",
      "finding pinstate file for bp01_pinstate_2024-04-03-122320\n",
      "found pinstate file for bp01_pinstate_2024-04-03-122320\n",
      "['bp01_2024-04-03-122320.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-04-03-122320.analysis_ROIs.csv']\n",
      "['bp01_2024-04-03-122320.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 931\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 931\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 931\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 14 is awake\n",
      "Importing data file: bp01-2024-04-03-135557.txt\n",
      "2024-04-03_135530\n",
      "finding pinstate file for bp01_pinstate_2024-04-03-135530\n",
      "found pinstate file for bp01_pinstate_2024-04-03-135530\n",
      "['bp01_2024-04-03-135530.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-04-03-135530.analysis_ROIs.csv']\n",
      "['bp01_2024-04-03-135530.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1432\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1432\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1432\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 15 is sleep\n",
      "session number 16 is awake\n",
      "Importing data file: bp01-2024-04-03-143924.txt\n",
      "2024-04-03_143849\n",
      "finding pinstate file for bp01_pinstate_2024-04-03-143849\n",
      "found pinstate file for bp01_pinstate_2024-04-03-143849\n",
      "['bp01_2024-04-03-143849.analysis_cleaned_coordinates.csv']\n",
      "['bp01_2024-04-03-143849.analysis_ROIs.csv']\n",
      "['bp01_2024-04-03-143849.analysis_head_direction.csv']\n",
      "First sync pulse detected at frame index: 1977\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "First sync pulse detected at frame index: 1977\n",
      "Trimming data to start after the first sync pulse...\n",
      "First sync pulse detected at frame index: 1977\n",
      "Trimming data to start after the first sync pulse...\n",
      "Resampling data from 60 FPS to 40 FPS...\n",
      "session number 17 is sleep\n"
     ]
    }
   ],
   "source": [
    "data_root = \"/Volumes/\"\n",
    "data_folder = \"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/\"\n",
    "output_folder = '/Users/AdamHarris/Desktop/cohort7_preprocessed'\n",
    "\n",
    "kilosort_folders = [\n",
    "\"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/28032024_31032024_combined_all\", \n",
    "\"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/24032024_25032024_combined_all\",\n",
    "\"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/21032024_23032024_combined_all\",\n",
    "\"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/02042024_03042024_combined_all\",\n",
    "]\n",
    "\n",
    "for kilosort_folder in kilosort_folders[1:]:\n",
    "    print(kilosort_folder)\n",
    "    print(\"_________________\")\n",
    "    date_string = get_datestring(kilosort_folder)\n",
    "\n",
    "    big_maze_tasks = ['openfield', 'objectvector']\n",
    "\n",
    "    json_path = os.path.join(kilosort_folder, [i for i in os.listdir(kilosort_folder) if i.endswith('.json') and not i.startswith('.')][0])\n",
    "\n",
    "    session_items = load_json(json_path)\n",
    "\n",
    "\n",
    "    bin_size_ms = 25\n",
    "\n",
    "    sleep_list = []\n",
    "\n",
    "    tasks_abstract = []\n",
    "    tasks = []\n",
    "\n",
    "    cohort, mouse_id = extract_cohort_and_mouse_id(kilosort_folder)\n",
    "\n",
    "    awake_metadata, sleeep_metadata = load_metadata(cohort, mouse_id, data_folder)\n",
    "\n",
    "    awake_session = 0\n",
    "\n",
    "    for session_num, session in enumerate(session_items):\n",
    "        \n",
    "        file_path = session[0]\n",
    "        file_size = session[1]\n",
    "        date, time = extract_ephys_date_time(file_path)\n",
    "        \n",
    "        sleep_status = wake_or_sleep(file_path, awake_metadata, sleeep_metadata)\n",
    "        sleep_list.append(sleep_status)\n",
    "        print(f\"session number {session_num} is {sleep_status}\")\n",
    "\n",
    "        if sleep_status == 'awake':\n",
    "            \n",
    "            Structure, Structure_abstract, Structure_no, Tracking_timestamp, Behaviour_timestamp =  get_awake_sessions_info(file_path, awake_metadata)\n",
    "            tasks.append(Structure)\n",
    "            tasks_abstract.append(Structure_abstract)\n",
    "\n",
    "            if Structure_abstract not in big_maze_tasks:\n",
    "                \n",
    "                pycontrol_outputs =  get_behaviour_txt(data_folder, \n",
    "                                                    mouse_id, \n",
    "                                                    cohort, \n",
    "                                                    date, \n",
    "                                                    Behaviour_timestamp, \n",
    "                                                    Structure_abstract, \n",
    "                                                    int_subject_IDs=True)\n",
    "\n",
    "\n",
    "                \n",
    "                xy, pinstate, ROIs, hd = get_tracking(data_folder, mouse_id, cohort, date, Tracking_timestamp)\n",
    "\n",
    "                xy_resample_syncaligned = resample_tracking(xy, pinstate)\n",
    "                ROIs_resample_syncaligned = resample_ROIs(ROIs, pinstate)\n",
    "                hd_resample_syncaligned =  resample_headDirections(hd, pinstate)\n",
    "\n",
    "                if (('A_on' in pycontrol_outputs.keys() and len(pycontrol_outputs['A_on']) > 0) or ('A_on_first' in pycontrol_outputs.keys() and len(pycontrol_outputs['A_on_first']) > 0)):\n",
    "                    \n",
    "                    trial_times, num_trials = make_trial_times_array(pycontrol_outputs, target_binning=25, Structure_abstract=Structure_abstract)\n",
    "    \n",
    "                    pokes_dic = make_pokes_dic(pycontrol_outputs)\n",
    "                    np.save(f\"{output_folder}/Trial_times_{mouse_id}_{date_string}_{awake_session}.npy\", trial_times)\n",
    "                    first_trial_bin_offset = get_sync_to_trial_offset(pycontrol_outputs, target_binning = 25)\n",
    "\n",
    "                    xy_trialaligned, ROIs_trialaligned, hd_trialaligned, ephys_mat = align_to_first_A(first_trial_bin_offset,\\\n",
    "                                                                                            xy_resample_syncaligned, \\\n",
    "                                                                                            ROIs_resample_syncaligned, \\\n",
    "                                                                                            hd_resample_syncaligned, \\\n",
    "                                                                                            ephys_mat=None)\n",
    "                \n",
    "                    np.save(f\"{output_folder}/XY_raw_{mouse_id}_{date_string}_{awake_session}.npy\", xy_trialaligned)\n",
    "                    np.save(f\"{output_folder}/Locs_raw_{mouse_id}_{date_string}_{awake_session}.npy\", ROIs_trialaligned)\n",
    "                    hd_trialaligned.to_csv(f\"{output_folder}/HD_raw_{mouse_id}_{date_string}_{awake_session}.csv\")\n",
    "                    np.save(f\"{output_folder}/Tasks_{mouse_id}_{date_string}_{awake_session}.npy\", np.array(Structure))\n",
    "                    np.save(f\"{output_folder}/Tasks_abstract_{mouse_id}_{date_string}_{awake_session}.npy\", np.array(Structure_abstract))\n",
    "\n",
    "                awake_session +=1\n",
    "\n",
    "            elif Structure_abstract in big_maze_tasks:\n",
    "\n",
    "                pycontrol_outputs, rsync = get_behaviour_tsv(data_folder, mouse_id, cohort, Behaviour_timestamp)\n",
    "                xy, pinstate, _, hd = get_tracking(data_folder, mouse_id, cohort, date, Tracking_timestamp)\n",
    "                \n",
    "                xy_resample_syncaligned = resample_tracking(xy, pinstate)\n",
    "                hd_resample_syncaligned =  resample_headDirections(hd, pinstate)\n",
    "\n",
    "                np.save(f\"{output_folder}/XY_raw_{mouse_id}_{date_string}_{awake_session}.npy\", xy_resample_syncaligned)\n",
    "               \n",
    "                hd_resample_syncaligned.to_csv(f\"{output_folder}/HD_raw_{mouse_id}_{date_string}_{awake_session}.csv\")\n",
    "                np.save(f\"{output_folder}/Tasks_{mouse_id}_{date_string}_{awake_session}.npy\", np.array(Structure))\n",
    "                np.save(f\"{output_folder}/Tasks_abstract_{mouse_id}_{date_string}_{awake_session}.npy\", np.array(Structure_abstract))\n",
    "\n",
    "                awake_session += 1\n",
    "\n",
    "    np.save(f\"{output_folder}/Tasks_abstract_{mouse_id}_{date_string}_all.npy\", np.array(tasks_abstract))\n",
    "    np.save(f\"{output_folder}/Tasks_{mouse_id}_{date_string}_all.npy\", np.array(tasks))\n",
    "    np.save(f\"{output_folder}/Wake_sleep_{mouse_id}_{date_string}_all.npy\", sleep_list)\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prime_A': array([0]),\n",
       " 'tone': array([], dtype=float64),\n",
       " 'prime_C': array([], dtype=float64),\n",
       " 'prime_B': array([], dtype=float64),\n",
       " 'C_on': array([], dtype=float64),\n",
       " 'prime_D': array([], dtype=float64),\n",
       " 'B_on': array([], dtype=float64),\n",
       " 'init_state': array([], dtype=float64),\n",
       " 'houselight_state': array([], dtype=float64),\n",
       " 'A_on': array([], dtype=float64),\n",
       " 'D_on': array([], dtype=float64),\n",
       " 'poke_8': array([45910]),\n",
       " 'poke_3_out': array([57127]),\n",
       " 'poke_8_out': array([46304]),\n",
       " 'poke_6_out': array([55778]),\n",
       " 'poke_3': array([56946]),\n",
       " 'poke_2': array([  58657,   63623,   63824,   66221,   99033,  100915,  101304,\n",
       "         131035,  131098,  131254,  131731,  131909,  132209,  210392,\n",
       "         210682,  453558,  454904,  458727,  478993,  599059,  601600,\n",
       "        1191733, 1193141, 1193698]),\n",
       " 'poke_1': array([], dtype=float64),\n",
       " 'poke_1_out': array([], dtype=float64),\n",
       " 'poke_7': array([42195]),\n",
       " 'poke_6': array([55531]),\n",
       " 'poke_5': array([], dtype=float64),\n",
       " 'poke_4': array([], dtype=float64),\n",
       " 'poke_4_out': array([], dtype=float64),\n",
       " 'poke_5_out': array([], dtype=float64),\n",
       " 'poke_7_out': array([42513]),\n",
       " 'poke_9_out': array([49662]),\n",
       " 'tone_off': array([], dtype=float64),\n",
       " 'poke_2_out': array([  59105,   63651,   65101,   98533,   99043,  101043,  129278,\n",
       "         131046,  131165,  131579,  131841,  131974,  208290,  210528,\n",
       "         453404,  453564,  458239,  478896,  596454,  601255, 1190146,\n",
       "        1192883, 1193415]),\n",
       " 'SOL_off': array([], dtype=float64),\n",
       " 'rsync': array([      0,    7224,    9015,   13932,   21362,   29394,   33034,\n",
       "          38165,   44144,   52930,   55523,   58044,   64361,   68670,\n",
       "          74172,   81305,   89770,   92016,   95881,   98499,  105012,\n",
       "         111894,  118460,  124484,  128916,  136023,  138065,  140009,\n",
       "         148593,  157694,  162296,  163068,  167572,  169363,  175359,\n",
       "         184174,  185811,  186852,  192454,  193663,  196203,  204755,\n",
       "         210316,  212331,  213343,  215390,  218312,  220504,  223140,\n",
       "         225850,  231942,  238021,  246006,  254230,  257954,  264244,\n",
       "         272368,  278803,  285237,  287488,  289662,  292519,  299439,\n",
       "         302699,  311387,  314478,  318515,  327391,  329646,  332205,\n",
       "         334942,  342050,  351010,  352863,  361344,  370253,  374843,\n",
       "         383618,  387283,  393148,  398010,  398598,  401747,  409055,\n",
       "         411927,  414334,  423505,  425369,  433772,  437253,  439642,\n",
       "         446535,  450252,  458528,  466989,  473089,  477513,  482262,\n",
       "         491416,  500476,  502203,  507120,  514591,  519481,  527458,\n",
       "         532052,  539347,  547905,  555082,  556582,  563339,  567104,\n",
       "         571904,  579630,  588284,  597465,  598893,  603623,  604706,\n",
       "         606790,  611659,  613267,  619983,  625279,  632764,  640142,\n",
       "         646570,  652813,  654043,  663530,  665653,  670816,  673888,\n",
       "         677010,  682072,  683257,  691286,  700321,  702820,  709452,\n",
       "         715772,  720722,  727558,  732519,  740026,  742089,  745411,\n",
       "         749690,  758367,  760491,  766155,  767028,  775706,  783778,\n",
       "         790227,  791423,  795379,  801006,  802607,  808995,  816005,\n",
       "         822319,  823163,  828927,  833246,  838691,  842926,  845970,\n",
       "         852838,  854229,  860744,  865420,  872329,  878234,  885110,\n",
       "         885765,  894193,  901959,  909071,  910438,  919885,  923279,\n",
       "         926172,  928608,  933728,  943267,  950454,  955702,  962519,\n",
       "         967728,  969826,  978354,  986437,  995480,  998272, 1004897,\n",
       "        1011379, 1020732, 1027325, 1031425, 1038678, 1043345, 1044272,\n",
       "        1049024, 1052710, 1057304, 1058604, 1065268, 1070391, 1071315,\n",
       "        1076996, 1082753, 1085998, 1092659, 1095334, 1098201, 1106597,\n",
       "        1108653, 1111379, 1120278, 1128666, 1136558, 1141659, 1146032,\n",
       "        1148138, 1155688, 1164683, 1173278, 1179445, 1181794, 1188151,\n",
       "        1190839, 1199791]),\n",
       " 'poke_9': array([48934])}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pycontrol_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# below we start to incorporate ephys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuropixels = True\n",
    "\n",
    "data_root = \"/Volumes/\"\n",
    "data_folder = \"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/\"\n",
    "\n",
    "kilosort_folder = r\"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/21032024_23032024_combined_all/\"\n",
    "\n",
    "big_maze_tasks = ['openfield', 'objectector']\n",
    "\n",
    "json_path = os.path.join(kilosort_folder, [i for i in os.listdir(kilosort_folder) if i.endswith('.json') and not i.startswith('.')][0])\n",
    "\n",
    "session_items = load_json(json_path)\n",
    "\n",
    "if neuropixels:\n",
    "    bytes_per_sample = 768\n",
    "    sampling_rate_hz = 30000\n",
    "    bin_size_ms = 25  # keep the same bin size throughout\n",
    "\n",
    "session_info = []\n",
    "\n",
    "cumulative_ms = 0.0\n",
    "\n",
    "for (file_path, file_size) in session_items:\n",
    "    n_samples = file_size / bytes_per_sample\n",
    "    duration_ms = n_samples / 30.0  # 30 samples per ms\n",
    "    start_ms = cumulative_ms\n",
    "    end_ms   = cumulative_ms + duration_ms\n",
    "    session_info.append((file_path, start_ms, end_ms))\n",
    "    cumulative_ms = end_ms\n",
    "\n",
    "# Load Ephys \n",
    "\n",
    "spike_times_samples = np.load(os.path.join(kilosort_folder, 'spike_times.npy'))\n",
    "spike_clusters      = np.load(os.path.join(kilosort_folder, 'spike_clusters.npy'))\n",
    "\n",
    "# Make sure they are both squeezed to 1D\n",
    "spike_times_samples = spike_times_samples.squeeze()\n",
    "spike_clusters      = spike_clusters.squeeze()\n",
    "\n",
    "# Convert from samples to ms\n",
    "spike_times_ms = spike_times_samples / 30.0\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4) Load cluster labels from TSVs & choose 'good' clusters\n",
    "# -------------------------------------------------------------------------\n",
    "cluster_group_tsv   = os.path.join(kilosort_folder, 'cluster_group.tsv')\n",
    "cluster_kslabel_tsv = os.path.join(kilosort_folder, 'cluster_KSlabel.tsv')\n",
    "cluster_labels = load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv)\n",
    "\n",
    "good_clusters = get_good_clusters(cluster_labels)\n",
    "print(f\"Found {len(good_clusters)} 'good' clusters.\")\n",
    "\n",
    "bin_size_ms = 25\n",
    "\n",
    "session_spike_counts = []  # will hold one array per session\n",
    "awake_spike_counts = []\n",
    "\n",
    "\n",
    "sleep_list = []\n",
    "\n",
    "\n",
    "\n",
    "cohort, mouse_id = extract_cohort_and_mouse_id(kilosort_folder)\n",
    "\n",
    "awake_metadata, sleeep_metadata = load_metadata(cohort, mouse_id, data_folder)\n",
    "\n",
    "awake_session = 0\n",
    "\n",
    "for session_num, (file_path, file_size) in enumerate(session_info):\n",
    " \n",
    "    date, time = extract_ephys_date_time(file_path)\n",
    "    sleep_status = wake_or_sleep(file_path, awake_metadata, sleeep_metadata)\n",
    "    sleep_list.append(sleep_status)\n",
    "\n",
    "    file_path_volumes = file_path.replace('/ceph/', '/Volumes/')\n",
    "    session_dir = os.path.dirname(file_path_volumes)\n",
    "    sync_file   = os.path.join(session_dir, 'sample_numbers.npy')\n",
    "\n",
    "    if sleep_status == 'sleep':\n",
    "        print('Sleep session, no sync pulses needed')\n",
    "        first_sync_ms = 0.0\n",
    "    elif not os.path.exists(sync_file):\n",
    "        print(f\"Warning: sync_file not found for {file_path}. Skipping first-sync truncation.\")\n",
    "        # We'll treat the entire session from session_abs_start to session_abs_end\n",
    "        # i.e. no extra offset.\n",
    "        first_sync_ms = 0.0\n",
    "    else:\n",
    "        # Load the sample_numbers array\n",
    "        sample_numbers = np.load(sync_file).squeeze()\n",
    "        # Typically it's 1D. We'll assume the first sync pulse is sample_numbers[0].\n",
    "        first_sync_sample = sample_numbers[0]\n",
    "        # Convert that to ms\n",
    "        first_sync_ms = first_sync_sample / 30.0\n",
    "\n",
    "    \n",
    "    spike_count_mat = bin_spikes(\n",
    "        spike_times_ms=spike_times_ms,\n",
    "        spike_clusters=spike_clusters,\n",
    "        good_clusters=good_clusters,\n",
    "        bin_size_ms=bin_size_ms,\n",
    "        session_offset=new_session_start,\n",
    "        session_duration_ms=new_session_duration\n",
    "    )\n",
    "    \n",
    "    session_spike_counts.append(spike_count_mat)\n",
    "\n",
    "    if sleep_status == 'awake':\n",
    "        awake_spike_counts.append(spike_count_mat)\n",
    "        awake_session += 1\n",
    "        Structure, Structure_abstract, Structure_no, Tracking_timestamp, Behaviour_timestamp =  get_awake_sessions_info(file_path, awake_metadata)\n",
    "        if Structure_abstract not in big_maze_tasks:\n",
    "            \n",
    "            pycontrol_outputs =  get_behaviour_txt(data_folder, \n",
    "                                                   mouse_id, \n",
    "                                                   cohort, \n",
    "                                                   date, \n",
    "                                                   Behaviour_timestamp, \n",
    "                                                   Structure_abstract, \n",
    "                                                   int_subject_IDs=True)\n",
    "            \n",
    "            trial_times = make_trial_times_array(pycontrol_outputs, target_binning=25, Structure_abstract=Structure_abstract)\n",
    "            pokes_dic = make_pokes_dic(pycontrol_outputs)\n",
    "            first_trial_bin_offset = get_sync_to_trial_offset(pycontrol_outputs, target_binning = 25)\n",
    "            \n",
    "            xy, pinstate, ROIs, hd = get_tracking(data_folder, mouse_id, cohort, date, Tracking_timestamp)\n",
    "\n",
    "            xy_resample_syncaligned = resample_tracking(xy, pinstate)\n",
    "            ROIs_resample_syncaligned = resample_ROIs(ROIs, pinstate)\n",
    "            hd_resample_syncaligned =  resample_headDirections(hd, pinstate)\n",
    "\n",
    "            xy_trialaligned, ROIs_trialaligned, hd_trialaligned, ephys_mat = align_to_first_A(first_trial_bin_offset,\\\n",
    "                                                                                    xy_resample_syncaligned, \\\n",
    "                                                                                    ROIs_resample_syncaligned, \\\n",
    "                                                                                    hd_resample_syncaligned, \\\n",
    "                                                                                    ephys_mat=None)\n",
    "        \n",
    "        elif Structure_abstract in big_maze_tasks:\n",
    "\n",
    "            pycontrol_outputs, rsync = get_behaviour_tsv(data_folder, mouse_id, cohort, Behaviour_timestamp)\n",
    "            xy, pinstate, ROIs, hd = get_tracking(data_folder, mouse_id, cohort, date, Tracking_timestamp)\n",
    "            \n",
    "            xy_resample_syncaligned = resample_tracking(xy, pinstate)\n",
    "            ROIs_resample_syncaligned = resample_ROIs(ROIs, pinstate)\n",
    "            hd_resample_syncaligned =  resample_headDirections(hd, pinstate)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>back2mid_deg</th>\n",
       "      <th>earL2earR_deg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>317.809680</td>\n",
       "      <td>233.047461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>314.440063</td>\n",
       "      <td>229.580727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>313.782598</td>\n",
       "      <td>227.129775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>311.011634</td>\n",
       "      <td>222.686783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>307.214151</td>\n",
       "      <td>219.084297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46327</th>\n",
       "      <td>164.190972</td>\n",
       "      <td>62.720045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46328</th>\n",
       "      <td>163.972052</td>\n",
       "      <td>62.935186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46329</th>\n",
       "      <td>163.852150</td>\n",
       "      <td>63.190550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46330</th>\n",
       "      <td>163.820797</td>\n",
       "      <td>63.242404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46331</th>\n",
       "      <td>163.839405</td>\n",
       "      <td>63.277902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46332 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       back2mid_deg  earL2earR_deg\n",
       "0        317.809680     233.047461\n",
       "1        314.440063     229.580727\n",
       "2        313.782598     227.129775\n",
       "3        311.011634     222.686783\n",
       "4        307.214151     219.084297\n",
       "...             ...            ...\n",
       "46327    164.190972      62.720045\n",
       "46328    163.972052      62.935186\n",
       "46329    163.852150      63.190550\n",
       "46330    163.820797      63.242404\n",
       "46331    163.839405      63.277902\n",
       "\n",
       "[46332 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd_resample_syncaligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        node_2\n",
       "1        node_2\n",
       "2        node_2\n",
       "3        node_2\n",
       "4        node_2\n",
       "          ...  \n",
       "46328    node_9\n",
       "46329    node_9\n",
       "46330    node_9\n",
       "46331    node_9\n",
       "46332    node_9\n",
       "Length: 46333, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROIs_resample_syncaligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 46332)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(xy_resample_syncaligned).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Sleep_box</th>\n",
       "      <th>Structure_abstract</th>\n",
       "      <th>Structure_no</th>\n",
       "      <th>Structure</th>\n",
       "      <th>Structure_sleep_session</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Session_time</th>\n",
       "      <th>Tracking</th>\n",
       "      <th>Ephys</th>\n",
       "      <th>...</th>\n",
       "      <th>Recording_node_PFC</th>\n",
       "      <th>Recording_node_HPC</th>\n",
       "      <th>Notes1</th>\n",
       "      <th>Notes2</th>\n",
       "      <th>Notes3</th>\n",
       "      <th>Weight_pre</th>\n",
       "      <th>DLC_Scorer</th>\n",
       "      <th>ROI_method</th>\n",
       "      <th>Sampling</th>\n",
       "      <th>ROI_boundaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, Sleep_box, Structure_abstract, Structure_no, Structure, Structure_sleep_session, Stage, Session_time, Tracking, Ephys, Behaviour_analyzed, Spike_sorted, Include, Recording_node_PFC, Recording_node_HPC, Notes1, Notes2, Notes3, Weight_pre, DLC_Scorer, ROI_method, Sampling, ROI_boundaries]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 23 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleeep_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>536870912</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536870912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536870912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536870912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536870912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536870912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74884</th>\n",
       "      <td>536870912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74885</th>\n",
       "      <td>536870912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74886</th>\n",
       "      <td>536870912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74887</th>\n",
       "      <td>536870912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74888</th>\n",
       "      <td>536870912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74889 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       536870912\n",
       "0      536870912\n",
       "1      536870912\n",
       "2      536870912\n",
       "3      536870912\n",
       "4      536870912\n",
       "...          ...\n",
       "74884  536870912\n",
       "74885  536870912\n",
       "74886  536870912\n",
       "74887  536870912\n",
       "74888  536870912\n",
       "\n",
       "[74889 rows x 1 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
