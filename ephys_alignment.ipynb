{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy.signal import resample\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "from datetime import datetime, date\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating making binned_FR files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/AdamHarris/Desktop/drive-download-20250122T174732Z-001/bp01-2024-03-15-144724.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>info</td>\n",
       "      <td>experiment_name</td>\n",
       "      <td>Adam\\ABCD_mEC_open_field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>info</td>\n",
       "      <td>task_name</td>\n",
       "      <td>Peter\\7x7open_field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>info</td>\n",
       "      <td>task_file_hash</td>\n",
       "      <td>2633371478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>info</td>\n",
       "      <td>setup_id</td>\n",
       "      <td>Big maze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>info</td>\n",
       "      <td>framework_version</td>\n",
       "      <td>2.0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1788.100</td>\n",
       "      <td>event</td>\n",
       "      <td>sync</td>\n",
       "      <td>rsync</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1791.942</td>\n",
       "      <td>event</td>\n",
       "      <td>sync</td>\n",
       "      <td>rsync</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>1800.000</td>\n",
       "      <td>event</td>\n",
       "      <td>timer</td>\n",
       "      <td>session_timer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>1800.000</td>\n",
       "      <td>variable</td>\n",
       "      <td>run_end</td>\n",
       "      <td>{\"session_duration\": 30}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1800.000</td>\n",
       "      <td>info</td>\n",
       "      <td>end_time</td>\n",
       "      <td>2024-03-15T15:17:24.312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>355 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time      type            subtype                   content\n",
       "0       0.000      info    experiment_name  Adam\\ABCD_mEC_open_field\n",
       "1       0.000      info          task_name       Peter\\7x7open_field\n",
       "2       0.000      info     task_file_hash                2633371478\n",
       "3       0.000      info           setup_id                  Big maze\n",
       "4       0.000      info  framework_version                     2.0.1\n",
       "..        ...       ...                ...                       ...\n",
       "350  1788.100     event               sync                     rsync\n",
       "351  1791.942     event               sync                     rsync\n",
       "352  1800.000     event              timer             session_timer\n",
       "353  1800.000  variable            run_end  {\"session_duration\": 30}\n",
       "354  1800.000      info           end_time   2024-03-15T15:17:24.312\n",
       "\n",
       "[355 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv):\n",
    "    \"\"\"\n",
    "    Load cluster labels from cluster_group.tsv and cluster_KSlabel.tsv,\n",
    "    returning a dictionary: { cluster_id: label }.\n",
    "    \n",
    "    The priority is:\n",
    "      - cluster_group.tsv if available\n",
    "      - fallback to cluster_KSlabel.tsv if the cluster is missing or 'unsorted' in the first\n",
    "    \"\"\"\n",
    "    # Read cluster_group.tsv if it exists\n",
    "    if os.path.exists(cluster_group_tsv):\n",
    "        df_group = pd.read_csv(cluster_group_tsv, sep='\\t')\n",
    "        df_group.columns = ['cluster_id', 'group']  # Usually: cluster_id, group\n",
    "    else:\n",
    "        df_group = pd.DataFrame(columns=['cluster_id', 'group'])\n",
    "    \n",
    "    # Read cluster_KSlabel.tsv if it exists\n",
    "    if os.path.exists(cluster_kslabel_tsv):\n",
    "        df_ks = pd.read_csv(cluster_kslabel_tsv, sep='\\t')\n",
    "        df_ks.columns = ['cluster_id', 'KSlabel']  # Usually: cluster_id, KSlabel\n",
    "    else:\n",
    "        df_ks = pd.DataFrame(columns=['cluster_id', 'KSlabel'])\n",
    "    \n",
    "    # Convert cluster_id to int in both for easier merges\n",
    "    df_group['cluster_id'] = df_group['cluster_id'].astype(int, errors='ignore')\n",
    "    df_ks['cluster_id']    = df_ks['cluster_id'].astype(int, errors='ignore')\n",
    "    \n",
    "    # Merge them into a single DataFrame, outer join so we keep all\n",
    "    df_merge = pd.merge(df_group, df_ks, on='cluster_id', how='outer')\n",
    "    \n",
    "    # Create a dictionary mapping cluster_id -> final label\n",
    "    # We consider 'group' as the primary label if present, else use 'KSlabel'\n",
    "    cluster_label_dict = {}\n",
    "    for idx, row in df_merge.iterrows():\n",
    "        cluster_id = int(row['cluster_id'])\n",
    "        \n",
    "        group_label = row['group'] if 'group' in row and pd.notnull(row['group']) else None\n",
    "        ks_label    = row['KSlabel'] if 'KSlabel' in row and pd.notnull(row['KSlabel']) else None\n",
    "        \n",
    "        # Priority: if group_label is available (e.g. \"good\", \"mua\", etc.), use it, else use ks_label\n",
    "        if group_label is not None:\n",
    "            cluster_label_dict[cluster_id] = group_label\n",
    "        else:\n",
    "            cluster_label_dict[cluster_id] = ks_label\n",
    "    \n",
    "    return cluster_label_dict\n",
    "\n",
    "def get_good_clusters(cluster_label_dict):\n",
    "    \"\"\"\n",
    "    Return a sorted list of cluster IDs considered 'good'.\n",
    "    This function can be adapted depending on how you define 'good' or 'SU'.\n",
    "    \"\"\"\n",
    "    good = []\n",
    "    for clust_id, label in cluster_label_dict.items():\n",
    "        # Some Kilosort pipelines use 'good' or 'single' or 'SU'. Adjust as appropriate.\n",
    "        if label in ['good', 'Good', 'su', 'SU']:\n",
    "            good.append(clust_id)\n",
    "    return sorted(good)\n",
    "\n",
    "def bin_spikes(spike_times, spike_clusters, good_clusters, bin_size_ms=25, session_offset=0, session_duration_ms=None):\n",
    "    \"\"\"\n",
    "    For a set of spikes that fall within [session_offset, session_offset+session_duration_ms),\n",
    "    shift them to start at 0 and bin them at bin_size_ms. Return a 2D array: (n_clusters x n_time_bins).\n",
    "    \n",
    "    - spike_times: 1D array of spike times (ms).\n",
    "    - spike_clusters: 1D array of cluster IDs for each spike_time.\n",
    "    - good_clusters: list of cluster IDs that are 'good'.\n",
    "    - bin_size_ms: size of each bin in ms.\n",
    "    - session_offset: start time of session in ms within the concatenated data.\n",
    "    - session_duration_ms: total duration of this session in ms.\n",
    "    \"\"\"\n",
    "    # Identify the spikes for this session\n",
    "    t_start = session_offset\n",
    "    t_end   = session_offset + session_duration_ms\n",
    "    \n",
    "    # Boolean mask for spikes in this session\n",
    "    in_session_mask = (spike_times >= t_start) & (spike_times < t_end)\n",
    "    \n",
    "    # Subset spike times & clusters\n",
    "    sess_spike_times    = spike_times[in_session_mask] - t_start  # shift to 0-based\n",
    "    sess_spike_clusters = spike_clusters[in_session_mask]\n",
    "    \n",
    "    # Figure out how many bins we need\n",
    "    n_bins = int(np.ceil(session_duration_ms / bin_size_ms))\n",
    "    \n",
    "    # We'll create a 2D array: shape = (len(good_clusters), n_bins)\n",
    "    spike_matrix = np.zeros((len(good_clusters), n_bins), dtype=np.int32)\n",
    "    \n",
    "    # Make a quick index from cluster_id -> row index in spike_matrix\n",
    "    cluster_index_map = {clust_id: i for i, clust_id in enumerate(good_clusters)}\n",
    "    \n",
    "    # Digitize times to bin indices\n",
    "    bin_indices = (sess_spike_times // bin_size_ms).astype(int)\n",
    "    \n",
    "    # Accumulate counts\n",
    "    for t_bin, clust in zip(bin_indices, sess_spike_clusters):\n",
    "        if clust in cluster_index_map:\n",
    "            # increment the corresponding cluster row, bin column\n",
    "            spike_matrix[cluster_index_map[clust], t_bin] += 1\n",
    "    \n",
    "    return spike_matrix\n",
    "\n",
    "def main():\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Load the JSON file containing session file sizes in bytes\n",
    "    # -------------------------------------------------------------------------\n",
    "    json_path = '/path/to/session_sizes.json'\n",
    "    with open(json_path, 'r') as f:\n",
    "        session_dict = json.load(f)\n",
    "    # session_dict: { file_path: file_size_in_bytes, ... }\n",
    "\n",
    "    # Sort the sessions by file_path or, if you prefer, by recording time\n",
    "    # (In your example, the keys appear in chronological order, but let's force a sorted list anyway.)\n",
    "    session_items = sorted(session_dict.items(), key=lambda x: x[0])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Compute session boundaries in ms\n",
    "    # -------------------------------------------------------------------------\n",
    "    bytes_per_sample = 768\n",
    "    sampling_rate_hz = 30000\n",
    "    # each second has 30000 samples, each sample is 768 bytes\n",
    "    #  => 30000 samples per sec => 30000 * 768 bytes per second\n",
    "    #  => 1 sample = 768 bytes => 1 ms = 30 samples\n",
    "    session_boundaries = []\n",
    "    cumulative_ms = 0.0\n",
    "    \n",
    "    session_info = []  # will hold (file_path, start_ms, end_ms)\n",
    "    \n",
    "    for (file_path, file_size) in session_items:\n",
    "        # number of raw samples in this session\n",
    "        n_samples = file_size / bytes_per_sample\n",
    "        \n",
    "        # convert to ms (1 second = 1000 ms -> 30000 samples => 1 ms = 30 samples)\n",
    "        duration_ms = n_samples / 30.0  # because 30 samples per ms at 30 kHz\n",
    "        start_ms = cumulative_ms\n",
    "        end_ms   = cumulative_ms + duration_ms\n",
    "        \n",
    "        session_info.append((file_path, start_ms, end_ms))\n",
    "        cumulative_ms = end_ms\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Load the Kilosort spike_times, spike_clusters\n",
    "    # -------------------------------------------------------------------------\n",
    "    kilosort_folder = '/path/to/kilosort_output'  # where spike_times.npy etc. reside\n",
    "    \n",
    "    spike_times = np.load(os.path.join(kilosort_folder, 'spike_times.npy'))  # confirm if truly in ms or samples\n",
    "    spike_clusters = np.load(os.path.join(kilosort_folder, 'spike_clusters.npy'))\n",
    "    \n",
    "    # If your spike_times are actually in samples at 30 kHz, do:\n",
    "    # spike_times = spike_times / 30.0  # to convert to ms\n",
    "    # Adjust if needed to keep consistent with how your pipeline is set.\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Load cluster labels (cluster_group.tsv, cluster_KSlabel.tsv) & get \"good\" clusters\n",
    "    # -------------------------------------------------------------------------\n",
    "    cluster_group_tsv   = os.path.join(kilosort_folder, 'cluster_group.tsv')\n",
    "    cluster_kslabel_tsv = os.path.join(kilosort_folder, 'cluster_KSlabel.tsv')\n",
    "    cluster_labels = load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv)\n",
    "    \n",
    "    good_clusters = get_good_clusters(cluster_labels)\n",
    "    print(f\"Found {len(good_clusters)} 'good' clusters.\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) For each session, partition the spikes and bin into 25 ms bins\n",
    "    # -------------------------------------------------------------------------\n",
    "    bin_size_ms = 25\n",
    "    session_spike_counts = []  # will hold one array for each session\n",
    "    \n",
    "    for i, (file_path, start_ms, end_ms) in enumerate(session_info):\n",
    "        duration_ms = end_ms - start_ms\n",
    "        print(f\"Session {i}: {file_path}\")\n",
    "        print(f\"   start_ms = {start_ms:.2f}, end_ms = {end_ms:.2f}, duration_ms = {duration_ms:.2f}\")\n",
    "        \n",
    "        # bin the spikes for this session\n",
    "        spike_count_mat = bin_spikes(\n",
    "            spike_times=spike_times,\n",
    "            spike_clusters=spike_clusters,\n",
    "            good_clusters=good_clusters,\n",
    "            bin_size_ms=bin_size_ms,\n",
    "            session_offset=start_ms,\n",
    "            session_duration_ms=duration_ms\n",
    "        )\n",
    "        \n",
    "        session_spike_counts.append(spike_count_mat)\n",
    "    \n",
    "    # session_spike_counts is now a list of arrays of shape [n_good_clusters, n_time_bins_for_that_session].\n",
    "    # You can save them or keep them in memory for further analysis.\n",
    "    \n",
    "    # Example: store in a dictionary, then maybe save to npy or pickle\n",
    "    results_dict = {}\n",
    "    for i, (file_path, start_ms, end_ms) in enumerate(session_info):\n",
    "        results_dict[file_path] = session_spike_counts[i]\n",
    "    \n",
    "    # If you wish to save them individually:\n",
    "    output_dir = '/path/to/output_matrices'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for i, (file_path, start_ms, end_ms) in enumerate(session_info):\n",
    "        # A nice name for the output file\n",
    "        session_filename = os.path.basename(file_path)  # or parse the date/time from the path\n",
    "        out_path = os.path.join(output_dir, f\"binnedSpikes_session{i}.npy\")\n",
    "        np.save(out_path, session_spike_counts[i])\n",
    "        print(f\"Saved binned spikes for session {i} to {out_path}\")\n",
    "    \n",
    "    print(\"All sessions processed and binned spike arrays saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 'good' clusters.\n",
      "Session 0: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_12-59-35/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "   start_ms = 0.00, end_ms = 1219909.20, duration_ms = 1219909.20\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 220\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll sessions processed and binned spike arrays saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 187\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   start_ms = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_ms\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, end_ms = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_ms\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, duration_ms = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration_ms\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# bin the spikes for this session\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     spike_count_mat \u001b[38;5;241m=\u001b[39m \u001b[43mbin_spikes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspike_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspike_times_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspike_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspike_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgood_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgood_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbin_size_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbin_size_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession_duration_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration_ms\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     session_spike_counts\u001b[38;5;241m.\u001b[39mappend(spike_count_mat)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# session_spike_counts is now a list of arrays of shape [n_good_clusters, n_time_bins_for_that_session].\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# You can save them or keep them in memory for further analysis.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Example: store in a dictionary, then maybe save to npy or pickle\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 94\u001b[0m, in \u001b[0;36mbin_spikes\u001b[0;34m(spike_times, spike_clusters, good_clusters, bin_size_ms, session_offset, session_duration_ms)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Subset spike times & clusters\u001b[39;00m\n\u001b[1;32m     93\u001b[0m sess_spike_times    \u001b[38;5;241m=\u001b[39m spike_times[in_session_mask] \u001b[38;5;241m-\u001b[39m t_start  \u001b[38;5;66;03m# shift to 0-based\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m sess_spike_clusters \u001b[38;5;241m=\u001b[39m \u001b[43mspike_clusters\u001b[49m\u001b[43m[\u001b[49m\u001b[43min_session_mask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Figure out how many bins we need\u001b[39;00m\n\u001b[1;32m     97\u001b[0m n_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(session_duration_ms \u001b[38;5;241m/\u001b[39m bin_size_ms))\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv):\n",
    "    \"\"\"\n",
    "    Load cluster labels from cluster_group.tsv and cluster_KSlabel.tsv,\n",
    "    returning a dictionary: { cluster_id: label }.\n",
    "    \"\"\"\n",
    "\n",
    "    # Try to read with a header\n",
    "    if os.path.exists(cluster_group_tsv):\n",
    "        df_group = pd.read_csv(cluster_group_tsv, sep='\\t')\n",
    "        # If the file indeed has columns named something else, rename:\n",
    "        # e.g. many labs have these columns: 'cluster_id', 'group'\n",
    "        # If your file is correct, you can skip or adapt the rename logic below\n",
    "        if list(df_group.columns) != ['cluster_id', 'group']:\n",
    "            # Attempt a rename; adjust logic if your columns differ\n",
    "            df_group.columns = ['cluster_id', 'group']\n",
    "    else:\n",
    "        df_group = pd.DataFrame(columns=['cluster_id', 'group'])\n",
    "    \n",
    "    if os.path.exists(cluster_kslabel_tsv):\n",
    "        df_ks = pd.read_csv(cluster_kslabel_tsv, sep='\\t')\n",
    "        # e.g. columns might be 'cluster_id', 'KSlabel'\n",
    "        if list(df_ks.columns) != ['cluster_id', 'KSlabel']:\n",
    "            df_ks.columns = ['cluster_id', 'KSlabel']\n",
    "    else:\n",
    "        df_ks = pd.DataFrame(columns=['cluster_id', 'KSlabel'])\n",
    "    \n",
    "    # Ensure correct types\n",
    "    df_group['cluster_id'] = df_group['cluster_id'].astype(int, errors='ignore')\n",
    "    df_ks['cluster_id']    = df_ks['cluster_id'].astype(int, errors='ignore')\n",
    "    \n",
    "    # Merge\n",
    "    df_merge = pd.merge(df_group, df_ks, on='cluster_id', how='outer')\n",
    "    \n",
    "    # Build the dictionary\n",
    "    cluster_label_dict = {}\n",
    "    for idx, row in df_merge.iterrows():\n",
    "        clust_id = int(row['cluster_id'])\n",
    "        \n",
    "        # \"group\" could be \"good\", \"mua\", etc.\n",
    "        group_label = row['group']   if 'group'   in row and pd.notnull(row['group'])   else None\n",
    "        ks_label    = row['KSlabel'] if 'KSlabel' in row and pd.notnull(row['KSlabel']) else None\n",
    "        \n",
    "        # Priority: if group_label is available, use it, otherwise fallback to ks_label\n",
    "        if group_label is not None:\n",
    "            cluster_label_dict[clust_id] = group_label\n",
    "        else:\n",
    "            cluster_label_dict[clust_id] = ks_label\n",
    "    \n",
    "    return cluster_label_dict\n",
    "\n",
    "    \n",
    "\n",
    "def get_good_clusters(cluster_label_dict):\n",
    "    \"\"\"\n",
    "    Return a sorted list of cluster IDs considered 'good'.\n",
    "    This function can be adapted depending on how you define 'good' or 'SU'.\n",
    "    \"\"\"\n",
    "    good = []\n",
    "    for clust_id, label in cluster_label_dict.items():\n",
    "        # Some Kilosort pipelines use 'good' or 'SU'; adjust as needed\n",
    "        if label in ['good', 'Good', 'su', 'SU']:\n",
    "            good.append(clust_id)\n",
    "    return sorted(good)\n",
    "\n",
    "def bin_spikes(spike_times, spike_clusters, good_clusters,\n",
    "               bin_size_ms=25,\n",
    "               session_offset=0,\n",
    "               session_duration_ms=None):\n",
    "    \"\"\"\n",
    "    For a set of spikes that fall within [session_offset, session_offset+session_duration_ms),\n",
    "    shift them to start at 0 and bin them at bin_size_ms. Return a 2D array: (n_clusters x n_time_bins).\n",
    "    \n",
    "    - spike_times: 1D array of spike times (ms).\n",
    "    - spike_clusters: 1D array of cluster IDs for each spike_time.\n",
    "    - good_clusters: list of cluster IDs that are 'good'.\n",
    "    - bin_size_ms: size of each bin in ms.\n",
    "    - session_offset: start time of session in ms within the concatenated data.\n",
    "    - session_duration_ms: total duration of this session in ms.\n",
    "    \"\"\"\n",
    "    # Identify the spikes for this session\n",
    "    t_start = session_offset\n",
    "    t_end   = session_offset + session_duration_ms\n",
    "    \n",
    "    # Boolean mask for spikes in this session\n",
    "    in_session_mask = (spike_times >= t_start) & (spike_times < t_end)\n",
    "    \n",
    "    # Subset spike times & clusters\n",
    "    sess_spike_times    = spike_times[in_session_mask] - t_start  # shift to 0-based\n",
    "    sess_spike_clusters = spike_clusters[in_session_mask]\n",
    "    \n",
    "    # Figure out how many bins we need\n",
    "    n_bins = int(np.ceil(session_duration_ms / bin_size_ms))\n",
    "    \n",
    "    # We'll create a 2D array: shape = (len(good_clusters), n_bins)\n",
    "    spike_matrix = np.zeros((len(good_clusters), n_bins), dtype=np.int32)\n",
    "    \n",
    "    # Make a quick index from cluster_id -> row index in spike_matrix\n",
    "    cluster_index_map = {clust_id: i for i, clust_id in enumerate(good_clusters)}\n",
    "    \n",
    "    # Digitize times to bin indices\n",
    "    bin_indices = (sess_spike_times // bin_size_ms).astype(int)\n",
    "    \n",
    "    # Accumulate counts\n",
    "    for t_bin, clust in zip(bin_indices, sess_spike_clusters):\n",
    "        if clust in cluster_index_map:\n",
    "            spike_matrix[cluster_index_map[clust], t_bin] += 1\n",
    "    \n",
    "    return spike_matrix\n",
    "\n",
    "def main():\n",
    "\n",
    "\n",
    "    kilosort_folder = \"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/28032024_31032024_combined_all\"\n",
    "    json__path =os.path.join(kilosort_folder, [i for i in os.listdir(kilosort_folder) if i.endswith('.json')][0])\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Load the JSON file containing session file sizes in bytes\n",
    "    # -------------------------------------------------------------------------\n",
    "    # json_path = '/path/to/session_sizes.json'\n",
    "    with open(json_path, 'r') as f:\n",
    "        session_dict = json.load(f)\n",
    "    # session_dict: { file_path: file_size_in_bytes, ... }\n",
    "\n",
    "    # Sort the sessions by file_path or, if you prefer, by recording time\n",
    "    session_items = sorted(session_dict.items(), key=lambda x: x[0])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Compute session boundaries in ms\n",
    "    # -------------------------------------------------------------------------\n",
    "    bytes_per_sample = 768\n",
    "    sampling_rate_hz = 30000\n",
    "    # each second has 30000 samples, each sample is 768 bytes\n",
    "    # => 1 sample = 768 bytes => 1 second = 30000 samples => 30000 * 768 bytes\n",
    "    \n",
    "    session_info = []\n",
    "    cumulative_ms = 0.0\n",
    "    \n",
    "    for (file_path, file_size) in session_items:\n",
    "        # number of raw samples in this session\n",
    "        n_samples = file_size / bytes_per_sample\n",
    "        \n",
    "        # convert to ms (1 second = 1000 ms -> 30000 samples => 1 ms = 30 samples)\n",
    "        duration_ms = n_samples / 30.0  # 30 samples per ms\n",
    "        start_ms = cumulative_ms\n",
    "        end_ms   = cumulative_ms + duration_ms\n",
    "        \n",
    "        session_info.append((file_path, start_ms, end_ms))\n",
    "        cumulative_ms = end_ms\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Load the Kilosort spike_times, spike_clusters\n",
    "    #    Convert spike_times from samples to ms\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    spike_times_samples = np.load(os.path.join(kilosort_folder, 'spike_times.npy'))\n",
    "    spike_clusters = np.load(os.path.join(kilosort_folder, 'spike_clusters.npy'))\n",
    "    \n",
    "    # Convert from samples to ms, since each sample = 1/30000 sec = 0.0333... ms\n",
    "    spike_times_ms = spike_times_samples / 30.0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Load cluster labels (cluster_group.tsv, cluster_KSlabel.tsv) & get \"good\" clusters\n",
    "    # -------------------------------------------------------------------------\n",
    "    cluster_group_tsv   = os.path.join(kilosort_folder, 'cluster_group.tsv')\n",
    "    cluster_kslabel_tsv = os.path.join(kilosort_folder, 'cluster_KSlabel.tsv')\n",
    "    cluster_labels = load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv)\n",
    "    \n",
    "    good_clusters = get_good_clusters(cluster_labels)\n",
    "    print(f\"Found {len(good_clusters)} 'good' clusters.\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) For each session, partition the spikes and bin into 25 ms bins\n",
    "    # -------------------------------------------------------------------------\n",
    "    bin_size_ms = 25\n",
    "    session_spike_counts = []  # will hold one array for each session\n",
    "    \n",
    "    for i, (file_path, start_ms, end_ms) in enumerate(session_info):\n",
    "        duration_ms = end_ms - start_ms\n",
    "        print(f\"Session {i}: {file_path}\")\n",
    "        print(f\"   start_ms = {start_ms:.2f}, end_ms = {end_ms:.2f}, duration_ms = {duration_ms:.2f}\")\n",
    "        \n",
    "        # bin the spikes for this session\n",
    "        spike_count_mat = bin_spikes(\n",
    "            spike_times=spike_times_ms,\n",
    "            spike_clusters=spike_clusters,\n",
    "            good_clusters=good_clusters,\n",
    "            bin_size_ms=bin_size_ms,\n",
    "            session_offset=start_ms,\n",
    "            session_duration_ms=duration_ms\n",
    "        )\n",
    "        \n",
    "        session_spike_counts.append(spike_count_mat)\n",
    "    \n",
    "    # session_spike_counts is now a list of arrays of shape [n_good_clusters, n_time_bins_for_that_session].\n",
    "    # You can save them or keep them in memory for further analysis.\n",
    "    \n",
    "    # Example: store in a dictionary, then maybe save to npy or pickle\n",
    "    results_dict = {}\n",
    "    for i, (file_path, start_ms, end_ms) in enumerate(session_info):\n",
    "        results_dict[file_path] = session_spike_counts[i]\n",
    "    \n",
    "    # If you wish to save them individually:\n",
    "    output_dir = '/Users/AdamHarris/Desktop/test_output_mats'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, (file_path, start_ms, end_ms) in enumerate(session_info):\n",
    "        # A nice name for the output file\n",
    "        session_filename = os.path.basename(file_path)  # or parse the date/time from the path\n",
    "        out_path = os.path.join(output_dir, f\"binnedSpikes_session{i}.npy\")\n",
    "        np.save(out_path, session_spike_counts[i])\n",
    "        print(f\"Saved binned spikes for session {i} to {out_path}\")\n",
    "    \n",
    "    print(\"All sessions processed and binned spike arrays saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(cohort, mouse, data_folder):\n",
    "    data_directory = f\"{data_folder}/cohort{cohort}/\"       Data_folderx+'/cohort'+str(cohort)+'/'#+mouse[:4]+'/'\n",
    "    MetaDatafile_path= f\"{data_directory}/MetaData.xlsx - {mouse}.csv\"\n",
    "    \n",
    "    with open(MetaDatafile_path, 'r') as f:\n",
    "        MetaData = np.genfromtxt(f, delimiter=',',dtype=str, usecols=np.arange(0,18))\n",
    "    MetaData_structure=MetaData[0]\n",
    "    Include_mask=MetaData[1:,np.where(MetaData[0]=='Include')[0][0]]=='1' ##added 06/07/2021\n",
    "    for indx, variable in enumerate(MetaData_structure):\n",
    "        if variable != 'Weight_pre':\n",
    "            Variable_dic[mouse][variable]=np.asarray(remove_empty(MetaData[1:,indx][Include_mask]))\n",
    "        else:\n",
    "            Variable_dic[mouse][variable]=MetaData[1:,indx][Include_mask]\n",
    "\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/28032024_31032024_combined_all/bp01_28032024_31032024_02042024_20241213162222_ALL_20241213162222.json\n",
      "{'/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_12-59-35/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28106707968, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_13-23-31/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28535519232, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_13-48-00/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28141166592, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_14-12-33/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27900291072, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_15-50-23/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28036426752, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_16-14-38/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28030823424, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_16-39-20/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28268375040, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_17-03-45/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28307146752, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_18-15-20/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 41690207232, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_11-08-32/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 44299321344, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_11-56-18/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28109703168, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-19-36/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27991913472, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-44-10/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 7314868224, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-50-05/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27885699072, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_13-13-48/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27876805632, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_14-38-18/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 42007790592, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_15-12-26/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 27983803392, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_15-37-15/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28526736384, '/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_16-00-28/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat': 28819178496}\n",
      "Found 27 'good' clusters.\n",
      "/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_12-59-35/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/sample_numbers.npy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 254\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll sessions processed and binned spike arrays saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 254\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 203\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m sync_file   \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(session_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_numbers.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mprint\u001b[39m(sync_file)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43msync_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: sync_file not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Skipping first-sync truncation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# We'll treat the entire session from session_abs_start to session_abs_end\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# i.e. no extra offset.\u001b[39;00m\n",
      "File \u001b[0;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv):\n",
    "    \"\"\"\n",
    "    Load cluster labels from cluster_group.tsv and cluster_KSlabel.tsv,\n",
    "    returning a dictionary: { cluster_id: label }.\n",
    "    \n",
    "    The priority is:\n",
    "      - cluster_group.tsv (if present)\n",
    "      - fallback to cluster_KSlabel.tsv (if the cluster is missing a label in the first)\n",
    "    \"\"\"\n",
    "    # cluster_group file\n",
    "    if os.path.exists(cluster_group_tsv):\n",
    "        df_group = pd.read_csv(cluster_group_tsv, sep='\\t')\n",
    "        # If needed, rename columns if they don't match exactly:\n",
    "        if 'cluster_id' not in df_group.columns or 'group' not in df_group.columns:\n",
    "            df_group.columns = ['cluster_id', 'group']\n",
    "    else:\n",
    "        df_group = pd.DataFrame(columns=['cluster_id', 'group'])\n",
    "    \n",
    "    # cluster_KSlabel file\n",
    "    if os.path.exists(cluster_kslabel_tsv):\n",
    "        df_ks = pd.read_csv(cluster_kslabel_tsv, sep='\\t')\n",
    "        if 'cluster_id' not in df_ks.columns or 'KSlabel' not in df_ks.columns:\n",
    "            df_ks.columns = ['cluster_id', 'KSlabel']\n",
    "    else:\n",
    "        df_ks = pd.DataFrame(columns=['cluster_id', 'KSlabel'])\n",
    "    \n",
    "    # Convert cluster_id to int in both for easier merges\n",
    "    df_group['cluster_id'] = df_group['cluster_id'].astype(int, errors='ignore')\n",
    "    df_ks['cluster_id']    = df_ks['cluster_id'].astype(int, errors='ignore')\n",
    "    \n",
    "    # Merge them into a single DataFrame, outer join so we keep all\n",
    "    df_merge = pd.merge(df_group, df_ks, on='cluster_id', how='outer')\n",
    "    \n",
    "    # Create a dictionary mapping cluster_id -> final label\n",
    "    cluster_label_dict = {}\n",
    "    for idx, row in df_merge.iterrows():\n",
    "        clust_id = int(row['cluster_id'])\n",
    "        \n",
    "        group_label = None\n",
    "        ks_label    = None\n",
    "        \n",
    "        if 'group' in row and pd.notnull(row['group']):\n",
    "            group_label = row['group']\n",
    "        if 'KSlabel' in row and pd.notnull(row['KSlabel']):\n",
    "            ks_label = row['KSlabel']\n",
    "        \n",
    "        # Priority: if group_label is available, use it; otherwise fallback to ks_label\n",
    "        final_label = group_label if group_label is not None else ks_label\n",
    "        cluster_label_dict[clust_id] = final_label\n",
    "    \n",
    "    return cluster_label_dict\n",
    "\n",
    "def get_good_clusters(cluster_label_dict):\n",
    "    \"\"\"\n",
    "    Return a sorted list of cluster IDs considered 'good'.\n",
    "    Adjust the condition below for your labeling convention:\n",
    "    e.g. 'good', 'Good', 'su', 'SU'\n",
    "    \"\"\"\n",
    "    good = []\n",
    "    for clust_id, label in cluster_label_dict.items():\n",
    "        if label in ['good', 'Good', 'su', 'SU']:\n",
    "            good.append(clust_id)\n",
    "    return sorted(good)\n",
    "\n",
    "def bin_spikes(spike_times_ms, spike_clusters, good_clusters,\n",
    "               bin_size_ms=25,\n",
    "               session_offset=0,\n",
    "               session_duration_ms=None):\n",
    "    \"\"\"\n",
    "    Bin 'good' clusters' spikes into 25 ms bins for one session.\n",
    "\n",
    "    - spike_times_ms: 1D array of spike times (ms, in concatenated timeline).\n",
    "    - spike_clusters: 1D array of cluster IDs for each spike_time.\n",
    "    - good_clusters: list of cluster IDs that are 'good'.\n",
    "    - bin_size_ms: size of each bin in ms (default = 25 ms).\n",
    "    - session_offset: the starting time (ms) of this session in the *concatenated* timeline.\n",
    "    - session_duration_ms: total duration of this session from that offset.\n",
    "    \n",
    "    Returns a 2D array of shape (n_good_clusters, n_time_bins).\n",
    "    \"\"\"\n",
    "    if session_duration_ms <= 0:\n",
    "        # If there's no valid time range, return an empty matrix\n",
    "        return np.zeros((len(good_clusters), 0), dtype=np.int32)\n",
    "    \n",
    "    t_start = session_offset\n",
    "    t_end   = session_offset + session_duration_ms\n",
    "    \n",
    "    # Boolean mask for spikes in [t_start, t_end)\n",
    "    in_session_mask = (spike_times_ms >= t_start) & (spike_times_ms < t_end)\n",
    "    \n",
    "    # Subset spike times & clusters\n",
    "    # (Make sure these arrays are 1D with .squeeze())\n",
    "    sess_spike_times    = spike_times_ms[in_session_mask].squeeze() - t_start\n",
    "    sess_spike_clusters = spike_clusters[in_session_mask].squeeze()\n",
    "    \n",
    "    # Figure out how many bins we need\n",
    "    n_bins = int(np.ceil(session_duration_ms / bin_size_ms))\n",
    "    \n",
    "    # We'll create a 2D array: shape = (len(good_clusters), n_bins)\n",
    "    spike_matrix = np.zeros((len(good_clusters), n_bins), dtype=np.int32)\n",
    "    \n",
    "    # Make a quick index from cluster_id -> row index in spike_matrix\n",
    "    cluster_index_map = {clust_id: i for i, clust_id in enumerate(good_clusters)}\n",
    "    \n",
    "    # Digitize times to bin indices\n",
    "    bin_indices = (sess_spike_times // bin_size_ms).astype(int)\n",
    "    \n",
    "    # Accumulate counts in each bin\n",
    "    for t_bin, clust in zip(bin_indices, sess_spike_clusters):\n",
    "        if 0 <= t_bin < n_bins:  # just a safety check\n",
    "            if clust in cluster_index_map:\n",
    "                spike_matrix[cluster_index_map[clust], t_bin] += 1\n",
    "    \n",
    "    return spike_matrix\n",
    "\n",
    "def main():\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Load the JSON file containing session file sizes in bytes\n",
    "    # -------------------------------------------------------------------------\n",
    "    kilosort_folder = \"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/28032024_31032024_combined_all\"\n",
    "    json_path =os.path.join(kilosort_folder, [i for i in os.listdir(kilosort_folder) if i.endswith('.json')][0])\n",
    "    print(json_path)\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        session_dict = json.load(f)\n",
    "\n",
    "    print(session_dict)\n",
    "    # session_dict: { file_path: file_size_in_bytes, ... }\n",
    "    # e.g. \"/path/to/2024-03-21_12-59-35/continuous.dat\": 28106707968\n",
    "    \n",
    "    # Sort the sessions by file_path or by date/time—adjust to your preference\n",
    "    session_items = sorted(session_dict.items(), key=lambda x: x[0])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Compute absolute session boundaries in the concatenated timeline (ms)\n",
    "    # -------------------------------------------------------------------------\n",
    "    bytes_per_sample = 768\n",
    "    sampling_rate_hz = 30000  # 30 kHz\n",
    "    # => each sample = 768 bytes\n",
    "    # => 1 ms corresponds to 30 samples at 30 kHz\n",
    "    \n",
    "    session_info = []  # will hold (file_path, start_ms, end_ms) in the *concatenated* timeline\n",
    "    cumulative_ms = 0.0\n",
    "    \n",
    "    for (file_path, file_size) in session_items:\n",
    "        # number of raw samples in this session\n",
    "        n_samples = file_size / bytes_per_sample\n",
    "        \n",
    "        # convert to ms:  n_samples / 30  (since 30 samples = 1 ms)\n",
    "        duration_ms = n_samples / 30.0\n",
    "        start_ms = cumulative_ms\n",
    "        end_ms   = cumulative_ms + duration_ms\n",
    "        \n",
    "        session_info.append((file_path, start_ms, end_ms))\n",
    "        cumulative_ms = end_ms\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Load spike_times and spike_clusters from Kilosort output\n",
    "    #    Convert from sample indices (30 kHz) to ms\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    spike_times_samples = np.load(os.path.join(kilosort_folder, 'spike_times.npy'))\n",
    "    spike_clusters      = np.load(os.path.join(kilosort_folder, 'spike_clusters.npy'))\n",
    "    \n",
    "    # Make sure they are both squeezed to 1D\n",
    "    spike_times_samples = spike_times_samples.squeeze()\n",
    "    spike_clusters      = spike_clusters.squeeze()\n",
    "    \n",
    "    # Convert from samples to ms\n",
    "    spike_times_ms = spike_times_samples / 30.0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Load cluster labels from TSVs & choose 'good' clusters\n",
    "    # -------------------------------------------------------------------------\n",
    "    cluster_group_tsv   = os.path.join(kilosort_folder, 'cluster_group.tsv')\n",
    "    cluster_kslabel_tsv = os.path.join(kilosort_folder, 'cluster_KSlabel.tsv')\n",
    "    cluster_labels = load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv)\n",
    "    \n",
    "    good_clusters = get_good_clusters(cluster_labels)\n",
    "    print(f\"Found {len(good_clusters)} 'good' clusters.\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) For each session, load sample_numbers.npy to find the first sync pulse,\n",
    "    #    then truncate everything prior to that pulse for binning.\n",
    "    # -------------------------------------------------------------------------\n",
    "    bin_size_ms = 25\n",
    "    session_spike_counts = []  # will hold one array per session\n",
    "    \n",
    "    for i, (file_path, session_abs_start, session_abs_end) in enumerate(session_info):\n",
    "        # The absolute session boundaries in the concatenated timeline are\n",
    "        #  [session_abs_start, session_abs_end).\n",
    "        file_path_volumes = file_path.replace('/ceph/', '/Volumes/')\n",
    "        session_dir = os.path.dirname(file_path_volumes)\n",
    "        sync_file   = os.path.join(session_dir, 'sample_numbers.npy')\n",
    "        print(sync_file)\n",
    "        \n",
    "        if not os.path.exists(sync_file):\n",
    "            print(f\"Warning: sync_file not found for {file_path}. Skipping first-sync truncation.\")\n",
    "            # We'll treat the entire session from session_abs_start to session_abs_end\n",
    "            # i.e. no extra offset.\n",
    "            first_sync_ms = 0.0\n",
    "        else:\n",
    "            # Load the sample_numbers array\n",
    "            sample_numbers = np.load(sync_file).squeeze()\n",
    "            # Typically it's 1D. We'll assume the first sync pulse is sample_numbers[0].\n",
    "            first_sync_sample = sample_numbers[0]\n",
    "            # Convert that to ms\n",
    "            first_sync_ms = first_sync_sample / 30.0\n",
    "        \n",
    "        # Now we want to shift the session's start forward by first_sync_ms\n",
    "        # So that time=0 in our local bins corresponds to the first sync pulse\n",
    "        new_session_start = session_abs_start + first_sync_ms\n",
    "        new_session_duration = session_abs_end - new_session_start\n",
    "        \n",
    "        # Bin the spikes for this truncated session\n",
    "        spike_count_mat = bin_spikes(\n",
    "            spike_times_ms=spike_times_ms,\n",
    "            spike_clusters=spike_clusters,\n",
    "            good_clusters=good_clusters,\n",
    "            bin_size_ms=bin_size_ms,\n",
    "            session_offset=new_session_start,\n",
    "            session_duration_ms=new_session_duration\n",
    "        )\n",
    "        \n",
    "        session_spike_counts.append(spike_count_mat)\n",
    "        \n",
    "        print(f\"Session {i}: {file_path}\")\n",
    "        print(f\"   Original: start_ms={session_abs_start:.2f}, end_ms={session_abs_end:.2f}\")\n",
    "        print(f\"   First sync pulse at {first_sync_sample} samples => {first_sync_ms:.2f} ms\")\n",
    "        print(f\"   Final binning range: [{new_session_start:.2f}, {new_session_start + new_session_duration:.2f}) ms\")\n",
    "        print(f\"   Output shape: {spike_count_mat.shape}\\n\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) Save each session’s 2D spike count matrix\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_dir = '/Users/AdamHarris/Desktop/test_output_mats'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, (file_path, _, _) in enumerate(session_info):\n",
    "        out_name = f\"binnedSpikes_session{i}.npy\"\n",
    "        out_path = os.path.join(output_dir, out_name)\n",
    "        np.save(out_path, session_spike_counts[i])\n",
    "        print(f\"Saved binned spikes for session {i} -> {out_path}\")\n",
    "    \n",
    "    print(\"All sessions processed and binned spike arrays saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = r'/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/MetaData.xlsx - bp01_sleep.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2957872130.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[53], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    def load_json(json_path)\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "def extract_cohort_and_mouse_id(filepath):\n",
    "    match = re.search(r\"cohort(\\d+)/.*?/([^/]+)/\", filepath)\n",
    "    if match:\n",
    "        cohort = match.group(1)  # Extract the cohort number\n",
    "        mouse_id = match.group(2)  # Extract the mouse ID\n",
    "        return cohort, mouse_id\n",
    "    else:\n",
    "        print('cannot retrieve mouse and cohort from filepath')\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def load_json(json_path):\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        session_dict = json.load(f)\n",
    "    \n",
    "    session_items = session_dict.items()\n",
    "\n",
    "    return session_items\n",
    "    \n",
    "\n",
    "def load_metadata(cohort, mouse, data_folder):\n",
    "\n",
    "    \"\"\"\n",
    "    Load metadata for a given mouse in a given cohort.\n",
    "    Returns two DataFrames: one for awake, one for sleep.\n",
    "    \"\"\"\n",
    "    data_directory = f\"{data_folder}/cohort{cohort}/\"   \n",
    "    metadata_path= f\"{data_directory}/MetaData.xlsx - {mouse}.csv\"\n",
    "\n",
    "    metadata_awake = pd.read_csv(metadata_path, delimiter=',',dtype=str)\n",
    "    filtered_metadata_awake=metadata_awake[metadata_awake['Include']==1]\n",
    "    \n",
    "    try:\n",
    "        metadata_path_sleep= f\"{data_directory}/MetaData.xlsx - {mouse}_sleep.csv\"\n",
    "        metadata_sleep = pd.read_csv(metadata_path_sleep, delimiter=',',dtype=str)\n",
    "        filtered_metadata_sleep=metadata_sleep[metadata_sleep['Include']==1]\n",
    "        return filtered_metadata_awake, filtered_metadata_sleep\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print('No sleep metadata found for this mouse')\n",
    "        return filtered_metadata_awake, None\n",
    "    \n",
    "\n",
    "def extract_ephys_date_time(filepath):\n",
    "    # Regular expression to capture date-time format\n",
    "    match = re.search(r\"(\\d{4}-\\d{2}-\\d{2})_(\\d{2}-\\d{2}-\\d{2})\", filepath)\n",
    "    if match:\n",
    "        date = match.group(1)\n",
    "        time = match.group(2)\n",
    "        return date, time\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def wake_or_sleep(ephys_filepath, metadata_awake, metadata_sleep):\n",
    "\n",
    "    sleep_status = None\n",
    "\n",
    "    date, time = extract_ephys_date_time(ephys_filepath)\n",
    "    \n",
    "    # first try awake metadata\n",
    "\n",
    "    filtered_awake =  metadata_awake[(metadata_awake[\"Date\"] == date) & (metadata_awake[\"Ephys\"] == time)]\n",
    "    \n",
    "    if len(filtered_awake>1):\n",
    "        print('More than one awake entry found for this date and time')\n",
    "    if len(filtered_awake==0):\n",
    "        print('Not an awake session')\n",
    "    if len(filtered_awake==1):\n",
    "        print('awake session')\n",
    "        sleep_status = 'awake'\n",
    "\n",
    "    if sleep_status =='awake':\n",
    "        return sleep_status\n",
    "    \n",
    "    else:\n",
    "        # then try sleep metadata\n",
    "        filtered_sleep = metadata_sleep[(metadata_sleep[\"Date\"] == date) & (metadata_sleep[\"Ephys\"] == time)]\n",
    "\n",
    "        if len(filtered_sleep>1):\n",
    "            print('More than one sleep entry found for this date and time')\n",
    "        if len(filtered_sleep==0):\n",
    "            print('Not an sleep session')\n",
    "        if len(filtered_sleep==1):\n",
    "            print('sleep session')\n",
    "            sleep_status = 'sleep'\n",
    "\n",
    "        return sleep_status\n",
    "\n",
    "\n",
    "def get_awake_sessions_info(ephys_filepath, metadata_awake):\n",
    "\n",
    "    \"\"\"\n",
    "    uses ephys file path from json and metadata dataframe\n",
    "    to retrieve information about the session and timestamps\n",
    "    to find tracking data and pycontrol data\n",
    "    \"\"\"\n",
    "\n",
    "    date, time = extract_ephys_date_time(ephys_filepath)\n",
    "\n",
    "    filtered_awake =  metadata_awake[(metadata_awake[\"Date\"] == date) & (metadata_awake[\"Ephys\"] == time)]\n",
    "    \n",
    "    Structure = filtered_awake[\"Structure\"]\n",
    "    Structure_abstract = filtered_awake[\"Structure_abstract\"]\n",
    "    Structure_no = filtered_awake[\"Structure_no\"]\n",
    "    Tracking_timestamp = filtered_awake[\"Tracking\"]\n",
    "    Behaviour_timestamp = filtered_awake[\"Behaviour\"]\n",
    "\n",
    "    return date, Structure, Structure_abstract, Structure_no, Tracking_timestamp, Behaviour_timestamp\n",
    "\n",
    "\n",
    "def get_behaviour_txt(data_path, mouse, cohort, date, Behaviour_timestamp):\n",
    "\n",
    "    \"\"\"\n",
    "    retrieves the pycontrol output file for awake sessions and produces a dictionary \n",
    "    containing the raw pycontrol data\n",
    "    \"\"\"\n",
    "    \n",
    "    Behaviourfile_path = f\"{data_path}/cohort{cohort}/{mouse}/behaviour/{mouse}-{date}-{Behaviour_timestamp}.txt\"\n",
    "\n",
    "    print('Importing data file: '+os.path.split(Behaviourfile_path)[1])\n",
    "\n",
    "    with open(Behaviourfile_path, 'r') as f:\n",
    "        all_lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "    int_subject_IDs=True\n",
    "\n",
    "    # Extract and store session information.\n",
    "    file_name = os.path.split(Behaviourfile_path)[1]\n",
    "    Event = namedtuple('Event', ['time','name'])\n",
    "\n",
    "    info_lines = [line[2:] for line in all_lines if line[0]=='I']\n",
    "\n",
    "    experiment_name = next(line for line in info_lines if 'Experiment name' in line).split(' : ')[1]\n",
    "    task_name       = next(line for line in info_lines if 'Task name'       in line).split(' : ')[1]\n",
    "    subject_ID_string    = next(line for line in info_lines if 'Subject ID'      in line).split(' : ')[1]\n",
    "    datetime_string      = next(line for line in info_lines if 'Start date'      in line).split(' : ')[1]\n",
    "\n",
    "\n",
    "    if int_subject_IDs: # Convert subject ID string to integer.\n",
    "        subject_ID = int(''.join([i for i in subject_ID_string if i.isdigit()]))\n",
    "    else:\n",
    "        subject_ID = subject_ID_string\n",
    "\n",
    "    datetime = datetime.strptime(datetime_string, '%Y/%m/%d %H:%M:%S')\n",
    "    datetime_string = datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Extract and store session data.\n",
    "\n",
    "    state_IDs = eval(next(line for line in all_lines if line[0]=='S')[2:])\n",
    "    event_IDs = eval(next(line for line in all_lines if line[0]=='E')[2:])\n",
    "    variable_lines = [line[2:] for line in all_lines if line[0]=='V']\n",
    "\n",
    "    if Structure_abstract not in ['ABCD','AB','ABCDA2','ABCDE','ABCAD']:\n",
    "        pass\n",
    "    else:\n",
    "        structurexx = next(line for line in variable_lines if 'active_poke' in line).split(' active_poke ')[1]\n",
    "        if 'ot' in structurexx:\n",
    "            structurex=structurexx[:8]+']'\n",
    "        else:\n",
    "            structurex=structurexx\n",
    "\n",
    "        if Structure_abstract in ['ABCD','AB','ABCDE']:\n",
    "            structure=np.asarray((structurex[1:-1]).split(',')).astype(int)\n",
    "        else:\n",
    "            structure=structurex\n",
    "\n",
    "        ID2name = {v: k for k, v in {**state_IDs, **event_IDs}.items()}\n",
    "        data_lines = [line[2:].split(' ') for line in all_lines if line[0]=='D']\n",
    "        events = [Event(int(dl[0]), ID2name[int(dl[1])]) for dl in data_lines]\n",
    "        times = {event_name: np.array([ev.time for ev in events if ev.name == event_name])  \n",
    "                    for event_name in ID2name.values()}\n",
    "  \n",
    "    return times\n",
    "\n",
    "\n",
    "def get_behaviour_tsv(data_path, mouse, cohort, Behaviour_timestamp):\n",
    "\n",
    "    \"\"\"\n",
    "    retrieves the pycontrol output file for bigmaze tsv sessions and produces a dictionary \n",
    "    containing the events in file. \n",
    "    for our bigmaze files this will just be RSYNC as we are just doing openfield or objectvector \n",
    "    sessions\n",
    "    IMPORTANT: the later version of pycontrol has timestamps in seconds instead of milliseconds\n",
    "    so we will here be multiplying by 1000 to keep units the same\n",
    "    \"\"\"\n",
    "    Behaviourfile_path =   f\"{data_path}/cohort{cohort}/{mouse}/behaviour/{mouse}-{date}-{Behaviour_timestamp}.tsv\"\n",
    "    behaviour_df = pd.read_csv(Behaviourfile_path, sep='\\t')\n",
    "    behaviour_df[\"time\"] = behaviour_df[\"time\"]*1000\n",
    "    df_rsync = of_behaviour.query(\"type == 'event' & subtype == 'sync' & content == 'rsync'\")\n",
    "\n",
    "\n",
    "    return behaviour_df, df_rsync[\"time\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_tracking(data_path, mouse, cohort, date, Tracking_timestamp):\n",
    "\n",
    "    if cohort == 7:\n",
    "        sleap_tag = 'cleaned_coordinates'\n",
    "        head_direction_tag = 'head_direction'\n",
    "        ROIs_tag = 'ROIs'\n",
    "\n",
    "    behaviour_folder = f\"{data_path}/cohort{cohort}/{mouse}/behaviour\"\n",
    "    files_in_behaviour = os.listdir(behaviour_folder)\n",
    "\n",
    "    # pinstate\n",
    "    \n",
    "    try:\n",
    "        print(f\"finding pinstate file for {mouse}_pinstate_{date}-{Tracking_timestamp}\")\n",
    "        pinstate_path = f\"{behaviour_folder}/{mouse}_pinstate_{date}-{Tracking_timestamp}.csv\"\n",
    "        pinstate = pd.read_csv(pinstate_path)\n",
    "    except FileNotFoundError:\n",
    "        print('Cannot find pinstate file')\n",
    "\n",
    "    # xy coords\n",
    "    try:\n",
    "        \n",
    "\n",
    "    ROIs =\n",
    "    head_direction = \n",
    "    xy = \n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "def resample_tracking(sleap_df, pinstate, camera_fps=60, target_fps=40, body_part = 'head_back'):\n",
    "\n",
    "    \"\"\"\n",
    "    takes in sleap dataframe and pinstate and returna\n",
    "    a resampled and first sync pulse truncated\n",
    "    list of xy coordinates from a select stable bodypart\n",
    "    \"\"\"\n",
    "\n",
    "    sync_indices = np.where(pinstate > np.median(pinstate))[0]\n",
    "\n",
    "    first_sync_idx = sync_indices[0]\n",
    "    print(f\"First sync pulse detected at frame index: {first_sync_idx}\")\n",
    "\n",
    "    # Trim the SLEAP data to remove rows before the first sync pulse\n",
    "    print(\"Trimming data to start after the first sync pulse...\")\n",
    "    sleap_trimmed = sleap_df[first_sync_idx:]\n",
    "\n",
    "    print(f\"Resampling data from {camera_fps} FPS to {target} FPS...\")\n",
    "    resample_factor = target_fps / camera_fps\n",
    "\n",
    "    coords = []\n",
    "\n",
    "    for column in [f'{body_part}.x', f'{body_part}.y']:\n",
    "        resampled_column = resample(sleap_trimmed[column].values, resampled_length)\n",
    "        coords.append(resampled_column)\n",
    "    \n",
    "    return coords\n",
    "\n",
    "\n",
    "def resample_ROIs(ROIs_df, pinstate, camera_fps=60, target_fps=40, body_part = 'head_back'):\n",
    "   \n",
    "    sync_indices = np.where(pinstate > np.median(pinstate))[0]\n",
    "    first_sync_idx = sync_indices[0]\n",
    "    print(f\"First sync pulse detected at frame index: {first_sync_idx}\")\n",
    "\n",
    "    ROIs_trimmed = ROIs_df[first_sync_idx:]\n",
    "\n",
    "    return\n",
    "\n",
    "def resample_headDirections(HD_df, pinstate, camera_fps=60, target_fps=40, body_part = 'head_back'):\n",
    "    \n",
    "    \"\"\"\n",
    "    takes in HD dataframe and pinstate and returns\n",
    "    a resampled and first sync pulse truncated\n",
    "    list of xy coordinates from a select stable bodypart\n",
    "    \"\"\"\n",
    "\n",
    "    sync_indices = np.where(pinstate > np.median(pinstate))[0]\n",
    "\n",
    "    first_sync_idx = sync_indices[0]\n",
    "    print(f\"First sync pulse detected at frame index: {first_sync_idx}\")\n",
    "\n",
    "    # Trim the SLEAP data to remove rows before the first sync pulse\n",
    "    print(\"Trimming data to start after the first sync pulse...\")\n",
    "    HD_df_trimmed = HD_df[first_sync_idx:]\n",
    "\n",
    "    print(f\"Resampling data from {camera_fps} FPS to {target} FPS...\")\n",
    "    resample_factor = target_fps / camera_fps\n",
    "\n",
    "    HD_resampled = pd.DataFrame()\n",
    "\n",
    "    for column in HD_df_trimmed.columns:\n",
    "        resampled_column = resample(HD_df_trimmed[column].values, resampled_length)\n",
    "        HD_resampled[column] = resampled_column\n",
    "    \n",
    "    return HD_resampled\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data file: bp01-2024-03-26-110648.txt\n",
      "[8 2 1 9]\n",
      "{'prime_A': array([     0, 542257, 681929, 907833]), 'tone': array([  80014,  546273,  696675, 1193857]), 'prime_C': array([279423, 579433, 724479]), 'prime_B': array([  82015,  548274,  698676, 1195858]), 'C_on': array([526544, 639034, 878323]), 'prime_D': array([526558, 639047, 878337]), 'B_on': array([279409, 579420, 724466]), 'init_state': array([], dtype=float64), 'houselight_state': array([], dtype=float64), 'A_on': array([  80001,  546259,  696662, 1193844]), 'D_on': array([542244, 681915, 907820]), 'poke_8': array([  80000,   83014,  187817,  187887,  443555,  466346,  514058,\n",
      "        532506,  546258,  547856,  548043,  559990,  572377,  573393,\n",
      "        584738,  585319,  620544,  649306,  664368,  696661,  707083,\n",
      "        719071,  739016,  754108, 1193843, 1198321, 1198989, 1199670,\n",
      "       1199952, 1200308, 1200605, 1200703, 1201041]), 'poke_3_out': array([ 281572,  522493,  554421,  568147,  580675,  606791,  628758,\n",
      "        645750,  703532,  726226,  731282,  747823,  893518, 1028185,\n",
      "       1028194, 1162003]), 'poke_8_out': array([  80959,   83056,  187831,  187893,  443910,  466387,  514171,\n",
      "        532763,  547115,  548025,  548073,  560184,  572614,  573508,\n",
      "        584874,  585511,  620615,  649500,  664561,  697493,  707237,\n",
      "        719345,  739150,  754230, 1195756, 1198932, 1199443, 1199921,\n",
      "       1200275, 1200543, 1200685, 1200883, 1201364]), 'poke_6_out': array([  37202,   43853,   44065,   63257,  284671,  299016,  299357,\n",
      "        300192,  300338,  300533,  302638,  302853,  304737,  304785,\n",
      "        305382,  332113,  520448,  538608,  557175,  566695,  582247,\n",
      "        608305,  617876,  627537,  631921,  647153,  690329,  702175,\n",
      "        716274,  734890,  746603,  859335,  906484,  955297,  960106,\n",
      "       1073017, 1085790, 1159026]), 'poke_3': array([ 281405,  522350,  554300,  568002,  580561,  606657,  628582,\n",
      "        645626,  703370,  726125,  731185,  747641,  893335, 1028178,\n",
      "       1028191, 1161878]), 'poke_2': array([ 27546, 279409, 579419, 724465]), 'poke_1': array([  19885,  277288,  526544,  551715,  577932,  639033,  656493,\n",
      "        711405,  722926,  878322, 1166414]), 'poke_1_out': array([  20156,  277552,  527092,  551890,  578075,  640133,  656764,\n",
      "        712106,  723045,  882044, 1166574]), 'poke_7': array([  6376, 231492, 231688, 360124, 468818, 530839, 561379, 575249,\n",
      "       598024, 636380, 650584, 659599, 663273, 663455, 695275, 708705,\n",
      "       752759, 867962]), 'poke_6': array([  36909,   43792,   44002,   63124,  284456,  298984,  299344,\n",
      "        299796,  300315,  300381,  301265,  302824,  304719,  304775,\n",
      "        305318,  327835,  518804,  537359,  555634,  566218,  581895,\n",
      "        607899,  617009,  627237,  631570,  646971,  689883,  701710,\n",
      "        715891,  734373,  746321,  858350,  906103,  954831,  960040,\n",
      "       1072731, 1084327, 1158703]), 'poke_5': array([ 32144,  33567, 193607, 333275, 535529, 548745, 564634, 570667,\n",
      "       601421, 619108, 654120, 691691, 705710, 737468, 749633, 861240]), 'poke_4': array([ 17371, 199935, 204118, 217738, 217782, 228742, 274596, 336218,\n",
      "       529028, 550111, 563041, 576645, 599797, 637844, 652126, 658204,\n",
      "       692863, 710104, 721484, 750934, 862828, 876625]), 'poke_4_out': array([ 17769, 200174, 206593, 217751, 217794, 228798, 274901, 336498,\n",
      "       529262, 550319, 563375, 576815, 600017, 638036, 652320, 658335,\n",
      "       693075, 710249, 721676, 751111, 863030, 876866]), 'poke_5_out': array([ 32742,  33579, 193972, 334089, 535838, 548960, 564932, 570794,\n",
      "       601855, 619297, 654476, 691865, 705829, 737861, 749824, 861488]), 'poke_7_out': array([  6757, 231675, 231744, 360371, 469166, 531022, 561604, 575421,\n",
      "       598299, 636622, 650814, 659753, 663283, 663463, 695436, 708958,\n",
      "       752955, 868176]), 'poke_9_out': array([  72724,  447935,  516708,  543317,  558638,  583470,  609881,\n",
      "        625635,  686706,  700431,  717766,  745151,  852796,  908140,\n",
      "        961961, 1155335]), 'tone_off': array([  82015,  548274,  698676, 1195858]), 'poke_2_out': array([ 27858, 279714, 579489, 724844]), 'SOL_off': array([  80014,  279423,  526558,  542257,  546273,  579433,  639047,\n",
      "        681929,  696675,  724479,  878337,  907833, 1193857]), 'rsync': array([      0,    9271,   15561,   22278,   23326,   24690,   30485,\n",
      "         39710,   42469,   47939,   56455,   65875,   71593,   76820,\n",
      "         84220,   91766,   94224,  101701,  105113,  111282,  113437,\n",
      "        114635,  124048,  128959,  133015,  136973,  142917,  149457,\n",
      "        157343,  162685,  166422,  171684,  175475,  183359,  185726,\n",
      "        188688,  189874,  198723,  202964,  205349,  211889,  217057,\n",
      "        219448,  228838,  234511,  235867,  236636,  237278,  242105,\n",
      "        243125,  247965,  252890,  254041,  258290,  260750,  267561,\n",
      "        273867,  282773,  284487,  285510,  290905,  300399,  305580,\n",
      "        314107,  315431,  316204,  323135,  325453,  334583,  341805,\n",
      "        343043,  346883,  353762,  354418,  355659,  358601,  362114,\n",
      "        370863,  378386,  381268,  381999,  383124,  390217,  397292,\n",
      "        404606,  410907,  416050,  424360,  431051,  437520,  440827,\n",
      "        442164,  446337,  448129,  454065,  460122,  465955,  475066,\n",
      "        482907,  485360,  490804,  499012,  503376,  510823,  512981,\n",
      "        514014,  521234,  522852,  528553,  531175,  535175,  541677,\n",
      "        550810,  559392,  568495,  575006,  581591,  582912,  583846,\n",
      "        588905,  591805,  595159,  596993,  603756,  606906,  612526,\n",
      "        618868,  623065,  626316,  629434,  634383,  636212,  637423,\n",
      "        641985,  649018,  651742,  657588,  664541,  672068,  680060,\n",
      "        684087,  684644,  685593,  694771,  695634,  702341,  706784,\n",
      "        711121,  720276,  722794,  726702,  731667,  735600,  738666,\n",
      "        747359,  752801,  759448,  764558,  770155,  776936,  786264,\n",
      "        787802,  795685,  798064,  800319,  801900,  805215,  806237,\n",
      "        809573,  815770,  819700,  825938,  831141,  840286,  846701,\n",
      "        853300,  860565,  864679,  868755,  875660,  882030,  883139,\n",
      "        885031,  889412,  894715,  898669,  900917,  903422,  904864,\n",
      "        914148,  915452,  924058,  928855,  936216,  937773,  943052,\n",
      "        948180,  949874,  950798,  957942,  960781,  968098,  971116,\n",
      "        977045,  981242,  987608,  995771, 1003802, 1011594, 1020611,\n",
      "       1023781, 1031204, 1034706, 1037018, 1042738, 1047226, 1049457,\n",
      "       1056504, 1061494, 1064330, 1068650, 1073138, 1075226, 1080327,\n",
      "       1083961, 1086689, 1094405, 1099496, 1102203, 1105035, 1111345,\n",
      "       1117590, 1126929, 1133747, 1139841, 1148695, 1155985, 1157794,\n",
      "       1164635, 1170108, 1176499, 1180661, 1183197, 1187023, 1193250,\n",
      "       1198614]), 'poke_9': array([  71529,  447705,  516452,  542243,  558466,  583379,  609344,\n",
      "        625320,  681914,  700241,  717590,  744908,  852546,  907819,\n",
      "        961643, 1155329])}\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from datetime import datetime, date\n",
    "\n",
    "\n",
    "Behaviourfile_paths = [r\"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/bp01/behaviour/bp01-2024-03-26-110648.txt\"]\n",
    "Structure_abstract = \"ABCD\"\n",
    "for Behaviourfile_path in Behaviourfile_paths:\n",
    "    print('Importing data file: '+os.path.split(Behaviourfile_path)[1])\n",
    "\n",
    "    with open(Behaviourfile_path, 'r') as f:\n",
    "        all_lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "    int_subject_IDs=True\n",
    "\n",
    "    # Extract and store session information.\n",
    "    file_name = os.path.split(Behaviourfile_path)[1]\n",
    "    Event = namedtuple('Event', ['time','name'])\n",
    "\n",
    "    info_lines = [line[2:] for line in all_lines if line[0]=='I']\n",
    "\n",
    "    experiment_name = next(line for line in info_lines if 'Experiment name' in line).split(' : ')[1]\n",
    "    task_name       = next(line for line in info_lines if 'Task name'       in line).split(' : ')[1]\n",
    "    subject_ID_string    = next(line for line in info_lines if 'Subject ID'      in line).split(' : ')[1]\n",
    "    datetime_string      = next(line for line in info_lines if 'Start date'      in line).split(' : ')[1]\n",
    "\n",
    "\n",
    "    if int_subject_IDs: # Convert subject ID string to integer.\n",
    "        subject_ID = int(''.join([i for i in subject_ID_string if i.isdigit()]))\n",
    "    else:\n",
    "        subject_ID = subject_ID_string\n",
    "\n",
    "    datetime = datetime.strptime(datetime_string, '%Y/%m/%d %H:%M:%S')\n",
    "    datetime_string = datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Extract and store session data.\n",
    "\n",
    "    state_IDs = eval(next(line for line in all_lines if line[0]=='S')[2:])\n",
    "    event_IDs = eval(next(line for line in all_lines if line[0]=='E')[2:])\n",
    "    variable_lines = [line[2:] for line in all_lines if line[0]=='V']\n",
    "\n",
    "    if Structure_abstract not in ['ABCD','AB','ABCDA2','ABCDE','ABCAD']:\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        structurexx = next(line for line in variable_lines if 'active_poke' in line).split(' active_poke ')[1]\n",
    "        if 'ot' in structurexx:\n",
    "            structurex=structurexx[:8]+']'\n",
    "        else:\n",
    "            structurex=structurexx\n",
    "\n",
    "        if Structure_abstract in ['ABCD','AB','ABCDE']:\n",
    "            structure=np.asarray((structurex[1:-1]).split(',')).astype(int)\n",
    "        else:\n",
    "            structure=structurex\n",
    "\n",
    "        ID2name = {v: k for k, v in {**state_IDs, **event_IDs}.items()}\n",
    "        data_lines = [line[2:].split(' ') for line in all_lines if line[0]=='D']\n",
    "        events = [Event(int(dl[0]), ID2name[int(dl[1])]) for dl in data_lines]\n",
    "        times = {event_name: np.array([ev.time for ev in events if ev.name == event_name])  \n",
    "                    for event_name in ID2name.values()}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prime_A', 'tone', 'prime_C', 'prime_B', 'C_on', 'prime_D', 'B_on', 'init_state', 'houselight_state', 'A_on', 'D_on', 'poke_8', 'poke_3_out', 'poke_8_out', 'poke_6_out', 'poke_3', 'poke_2', 'poke_1', 'poke_1_out', 'poke_7', 'poke_6', 'poke_5', 'poke_4', 'poke_4_out', 'poke_5_out', 'poke_7_out', 'poke_9_out', 'tone_off', 'poke_2_out', 'SOL_off', 'rsync', 'poke_9'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('7', 'bp01')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_cohort_and_mouse_id(\"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/21032024_23032024_combined_all/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = r'/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/MetaData.xlsx - bp01.csv'\n",
    "metadata = pd.read_csv(metadata_path, delimiter=',',dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Maze</th>\n",
       "      <th>Structure_abstract</th>\n",
       "      <th>Structure_no</th>\n",
       "      <th>Structure</th>\n",
       "      <th>Structure_session</th>\n",
       "      <th>Session_time</th>\n",
       "      <th>Reward_time</th>\n",
       "      <th>Entry</th>\n",
       "      <th>Behaviour</th>\n",
       "      <th>...</th>\n",
       "      <th>Recording_node_HPC</th>\n",
       "      <th>Performance</th>\n",
       "      <th>Notes1</th>\n",
       "      <th>Notes2</th>\n",
       "      <th>Notes3</th>\n",
       "      <th>Weight_pre</th>\n",
       "      <th>DLC_Scorer</th>\n",
       "      <th>ROI_method</th>\n",
       "      <th>Sampling</th>\n",
       "      <th>ROI_boundaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2024-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>6</td>\n",
       "      <td>4-6-8-3</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>173555</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20/40, 32 trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>6</td>\n",
       "      <td>4-6-8-3</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>123213</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40 trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>6</td>\n",
       "      <td>4-6-8-3</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E</td>\n",
       "      <td>133806</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36 trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>6</td>\n",
       "      <td>4-6-8-3</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>154855</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22/40, 44 trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>6</td>\n",
       "      <td>4-6-8-3</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>164414</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22/40, 34 trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2024-03-24</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>28</td>\n",
       "      <td>1-5-3-9</td>\n",
       "      <td>--</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>105246</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2024-03-24</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>29</td>\n",
       "      <td>3-4-6-7</td>\n",
       "      <td>--</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>114225</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lots of trials but looping</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2024-03-24</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>30</td>\n",
       "      <td>5-1-9-2</td>\n",
       "      <td>--</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E</td>\n",
       "      <td>132941</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>motivation not great - over 10 trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2024-03-24</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>28</td>\n",
       "      <td>1-5-3-9</td>\n",
       "      <td>--</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>141823</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>asleep on node 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2024-03-25</td>\n",
       "      <td>1</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>31</td>\n",
       "      <td>8-3-5-4</td>\n",
       "      <td>--</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>094640</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abspolutely flying through trials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date Maze Structure_abstract Structure_no Structure  \\\n",
       "100  2024-01-22    1               ABCD            6   4-6-8-3   \n",
       "101  2024-01-23    1               ABCD            6   4-6-8-3   \n",
       "102  2024-01-23    1               ABCD            6   4-6-8-3   \n",
       "103  2024-01-23    1               ABCD            6   4-6-8-3   \n",
       "104  2024-01-23    1               ABCD            6   4-6-8-3   \n",
       "..          ...  ...                ...          ...       ...   \n",
       "195  2024-03-24    1               ABCD           28   1-5-3-9   \n",
       "196  2024-03-24    1               ABCD           29   3-4-6-7   \n",
       "197  2024-03-24    1               ABCD           30   5-1-9-2   \n",
       "198  2024-03-24    1               ABCD           28   1-5-3-9   \n",
       "199  2024-03-25    1               ABCD           31   8-3-5-4   \n",
       "\n",
       "    Structure_session Session_time Reward_time Entry Behaviour  ...  \\\n",
       "100                24           17         NaN     S    173555  ...   \n",
       "101                25           20         NaN     S    123213  ...   \n",
       "102                26           20         NaN     E    133806  ...   \n",
       "103                27           20         NaN     W    154855  ...   \n",
       "104                28           20         NaN     S    164414  ...   \n",
       "..                ...          ...         ...   ...       ...  ...   \n",
       "195                --           20         NaN     S    105246  ...   \n",
       "196                --           20         NaN     W    114225  ...   \n",
       "197                --           20         NaN     E    132941  ...   \n",
       "198                --           20         NaN     N    141823  ...   \n",
       "199                --           20         NaN     S    094640  ...   \n",
       "\n",
       "    Recording_node_HPC Performance                                 Notes1  \\\n",
       "100                NaN         NaN                       20/40, 32 trials   \n",
       "101                NaN         NaN                              40 trials   \n",
       "102                NaN         NaN                              36 trials   \n",
       "103                NaN         NaN                       22/40, 44 trials   \n",
       "104                NaN         NaN                       22/40, 34 trials   \n",
       "..                 ...         ...                                    ...   \n",
       "195                NaN         NaN                                    NaN   \n",
       "196                NaN         NaN             lots of trials but looping   \n",
       "197                NaN         NaN  motivation not great - over 10 trials   \n",
       "198                NaN         NaN                       asleep on node 2   \n",
       "199                NaN         NaN      abspolutely flying through trials   \n",
       "\n",
       "    Notes2 Notes3 Weight_pre DLC_Scorer ROI_method Sampling ROI_boundaries  \n",
       "100    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "101    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "102    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "103    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "104    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "..     ...    ...        ...        ...        ...      ...            ...  \n",
       "195    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "196    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "197    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "198    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "199    NaN    NaN        NaN        NaN        NaN      NaN            NaN  \n",
       "\n",
       "[100 rows x 28 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36    10-09-41\n",
       "37    11-16-54\n",
       "38    12-08-06\n",
       "39    13-53-38\n",
       "40    14-41-25\n",
       "Name: Ephys, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awake_cols_of_interest = 'Behaviour', 'Tracking', 'Ephys', 'Maze', 'Structure_abstract'\n",
    "\n",
    "metadata.columns\n",
    "\n",
    "# metadata[metadata['Date']=='2024-03-24']['Ephys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Sleep_box', 'Structure_abstract', 'Structure_no', 'Structure',\n",
       "       'Structure_sleep_session', 'Stage', 'Session_time', 'Tracking', 'Ephys',\n",
       "       'Behaviour_analyzed', 'Spike_sorted', 'Include', 'Recording_node_PFC',\n",
       "       'Recording_node_HPC', 'Notes1', 'Notes2', 'Notes3', 'Weight_pre',\n",
       "       'DLC_Scorer', 'ROI_method', 'Sampling', 'ROI_boundaries'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Sleep_box</th>\n",
       "      <th>Structure_abstract</th>\n",
       "      <th>Structure_no</th>\n",
       "      <th>Structure</th>\n",
       "      <th>Structure_sleep_session</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Session_time</th>\n",
       "      <th>Tracking</th>\n",
       "      <th>Ephys</th>\n",
       "      <th>...</th>\n",
       "      <th>Recording_node_PFC</th>\n",
       "      <th>Recording_node_HPC</th>\n",
       "      <th>Notes1</th>\n",
       "      <th>Notes2</th>\n",
       "      <th>Notes3</th>\n",
       "      <th>Weight_pre</th>\n",
       "      <th>DLC_Scorer</th>\n",
       "      <th>ROI_method</th>\n",
       "      <th>Sampling</th>\n",
       "      <th>ROI_boundaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2024-03-25</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>post_new1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-09-50</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"A\",</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date Sleep_box Structure_abstract Structure_no Structure  \\\n",
       "42  2024-03-25         1                  -            -         -   \n",
       "\n",
       "   Structure_sleep_session Stage Session_time Tracking     Ephys  ...  \\\n",
       "42               post_new1   NaN           20      NaN  10-09-50  ...   \n",
       "\n",
       "   Recording_node_PFC Recording_node_HPC Notes1 Notes2 Notes3 Weight_pre  \\\n",
       "42                  A                NaN    NaN    NaN    NaN       \"A\",   \n",
       "\n",
       "   DLC_Scorer ROI_method Sampling ROI_boundaries  \n",
       "42        NaN        NaN      NaN            NaN  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = r'/ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-25_10-09-50/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat'\n",
    "date_example, time_example = extract_ephys_date_time(test_path)\n",
    "\n",
    "\n",
    "date_filtered =  metadata[(metadata[\"Date\"] == date_example) & (metadata[\"Ephys\"] == time_example)]\n",
    "if len(date_filtered>1):\n",
    "    print('More than one entry found for this date and time')\n",
    "else:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/21032024_23032024_combined_all/bp01_21032024_23032024_20241127122138_ALL_20241127122138.json\n",
      "Found 40 'good' clusters (pre-FR-filter).\n",
      "Session 0: First sync pulse at 369163 samples => 12305.43 ms\n",
      "Spikes remaining after this session: 29302436\n",
      "Session 0: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_12-59-35/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [12305.43, 1219909.20) ms\n",
      "    binned shape: (40, 48305)\n",
      "Session 1: First sync pulse at 987966 samples => 32932.20 ms\n",
      "Spikes remaining after this session: 27610989\n",
      "Session 1: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_13-23-31/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [1252841.40, 2458430.00) ms\n",
      "    binned shape: (40, 48224)\n",
      "Session 2: First sync pulse at 394921 samples => 13164.03 ms\n",
      "Spikes remaining after this session: 25676243\n",
      "Session 2: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_13-48-00/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [2471594.03, 3679834.80) ms\n",
      "    binned shape: (40, 48330)\n",
      "Session 3: First sync pulse at 131974 samples => 4399.13 ms\n",
      "Spikes remaining after this session: 24097807\n",
      "Session 3: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_14-12-33/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [3684233.93, 4890784.93) ms\n",
      "    binned shape: (40, 48263)\n",
      "Session 4: First sync pulse at 307454 samples => 10248.47 ms\n",
      "Spikes remaining after this session: 22660455\n",
      "Session 4: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_15-50-23/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [4901033.40, 6107643.73) ms\n",
      "    binned shape: (40, 48265)\n",
      "Session 5: First sync pulse at 62645 samples => 2088.17 ms\n",
      "Spikes remaining after this session: 21380978\n",
      "Session 5: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_16-14-38/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [6109731.90, 7324259.33) ms\n",
      "    binned shape: (40, 48582)\n",
      "Session 6: First sync pulse at 406221 samples => 13540.70 ms\n",
      "Spikes remaining after this session: 19577393\n",
      "Session 6: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_16-39-20/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [7337800.03, 8551185.33) ms\n",
      "    binned shape: (40, 48536)\n",
      "Session 7: First sync pulse at 651563 samples => 21718.77 ms\n",
      "Spikes remaining after this session: 18261927\n",
      "Session 7: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_17-03-45/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [8572904.10, 9779794.13) ms\n",
      "    binned shape: (40, 48276)\n",
      "Session 8: First sync pulse at 133705 samples => 4456.83 ms\n",
      "Spikes remaining after this session: 14681770\n",
      "Session 8: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-21_18-15-20/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [9784250.97, 11589264.93) ms\n",
      "    binned shape: (40, 72201)\n",
      "Session 9: First sync pulse at 20132672 samples => 671089.07 ms\n",
      "Spikes remaining after this session: 12249406\n",
      "Session 9: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_11-08-32/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [12260354.00, 13511978.53) ms\n",
      "    binned shape: (40, 50065)\n",
      "Session 10: First sync pulse at 327987 samples => 10932.90 ms\n",
      "Spikes remaining after this session: 10392609\n",
      "Session 10: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_11-56-18/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [13522911.43, 14732017.73) ms\n",
      "    binned shape: (40, 48365)\n",
      "Session 11: First sync pulse at 324728 samples => 10824.27 ms\n",
      "Spikes remaining after this session: 8733393\n",
      "Session 11: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-19-36/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [14742842.00, 15946944.53) ms\n",
      "    binned shape: (40, 48165)\n",
      "Session 12: First sync pulse at 195452 samples => 6515.07 ms\n",
      "Spikes remaining after this session: 8284004\n",
      "Session 12: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-44-10/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [15953459.60, 16264430.13) ms\n",
      "    binned shape: (40, 12439)\n",
      "Session 13: First sync pulse at 72687 samples => 2422.90 ms\n",
      "Spikes remaining after this session: 6536258\n",
      "Session 13: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_12-50-05/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [16266853.03, 17474746.93) ms\n",
      "    binned shape: (40, 48316)\n",
      "Session 14: First sync pulse at 164698 samples => 5489.93 ms\n",
      "Spikes remaining after this session: 5038734\n",
      "Session 14: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_13-13-48/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [17480236.87, 18684677.73) ms\n",
      "    binned shape: (40, 48178)\n",
      "Session 15: First sync pulse at 503842 samples => 16794.73 ms\n",
      "Spikes remaining after this session: 3854752\n",
      "Session 15: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_14-38-18/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [18701472.47, 20507932.53) ms\n",
      "    binned shape: (40, 72259)\n",
      "Session 16: First sync pulse at 0 samples => 0.00 ms\n",
      "Spikes remaining after this session: 2723768\n",
      "Session 16: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_15-12-26/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [20507932.53, 21722507.33) ms\n",
      "    binned shape: (40, 48583)\n",
      "Session 17: First sync pulse at 924236 samples => 30807.87 ms\n",
      "Spikes remaining after this session: 1214606\n",
      "Session 17: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_15-37-15/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [21753315.20, 22960646.93) ms\n",
      "    binned shape: (40, 48294)\n",
      "Session 18: First sync pulse at 1261742 samples => 42058.07 ms\n",
      "Spikes remaining after this session: 0\n",
      "Session 18: /ceph/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Ephys/2024-03-23_16-00-28/Record Node 101/experiment1/recording1/continuous/Neuropix-PXI-100.ProbeA-AP/continuous.dat\n",
      "    final time range: [23002705.00, 24211479.33) ms\n",
      "    binned shape: (40, 48351)\n",
      "Filtering: 0 clusters fail FR< 0.002 criterion.\n",
      "Remaining: 40 clusters.\n",
      "All sessions binned and firing-rate-filtered. Final # of clusters: 40\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv):\n",
    "    \"\"\"\n",
    "    Load cluster labels from cluster_group.tsv and cluster_KSlabel.tsv,\n",
    "    returning a dictionary: { cluster_id: label }.\n",
    "    \n",
    "    The priority is:\n",
    "      - cluster_group.tsv (if present)\n",
    "      - fallback to cluster_KSlabel.tsv (if the cluster is missing a label in the first)\n",
    "    \"\"\"\n",
    "    # cluster_group file\n",
    "    if os.path.exists(cluster_group_tsv):\n",
    "        df_group = pd.read_csv(cluster_group_tsv, sep='\\t')\n",
    "        # If needed, rename columns if they don't match exactly:\n",
    "        if 'cluster_id' not in df_group.columns or 'group' not in df_group.columns:\n",
    "            df_group.columns = ['cluster_id', 'group']\n",
    "    else:\n",
    "        df_group = pd.DataFrame(columns=['cluster_id', 'group'])\n",
    "    \n",
    "    # cluster_KSlabel file\n",
    "    if os.path.exists(cluster_kslabel_tsv):\n",
    "        df_ks = pd.read_csv(cluster_kslabel_tsv, sep='\\t')\n",
    "        if 'cluster_id' not in df_ks.columns or 'KSlabel' not in df_ks.columns:\n",
    "            df_ks.columns = ['cluster_id', 'KSlabel']\n",
    "    else:\n",
    "        df_ks = pd.DataFrame(columns=['cluster_id', 'KSlabel'])\n",
    "    \n",
    "    # Convert cluster_id to int in both for easier merges\n",
    "    df_group['cluster_id'] = df_group['cluster_id'].astype(int, errors='ignore')\n",
    "    df_ks['cluster_id']    = df_ks['cluster_id'].astype(int, errors='ignore')\n",
    "    \n",
    "    # Merge them into a single DataFrame, outer join so we keep all\n",
    "    df_merge = pd.merge(df_group, df_ks, on='cluster_id', how='outer')\n",
    "    \n",
    "    # Create a dictionary mapping cluster_id -> final label\n",
    "    cluster_label_dict = {}\n",
    "    for idx, row in df_merge.iterrows():\n",
    "        clust_id = int(row['cluster_id'])\n",
    "        \n",
    "        group_label = None\n",
    "        ks_label    = None\n",
    "        \n",
    "        if 'group' in row and pd.notnull(row['group']):\n",
    "            group_label = row['group']\n",
    "        if 'KSlabel' in row and pd.notnull(row['KSlabel']):\n",
    "            ks_label = row['KSlabel']\n",
    "        \n",
    "        # Priority: if group_label is available, use it; otherwise fallback to ks_label\n",
    "        final_label = group_label if group_label is not None else ks_label\n",
    "        cluster_label_dict[clust_id] = final_label\n",
    "    \n",
    "    return cluster_label_dict\n",
    "\n",
    "def get_good_clusters(cluster_label_dict):\n",
    "    \"\"\"\n",
    "    Return a sorted list of cluster IDs considered 'good'.\n",
    "    Adjust the condition below for your labeling convention:\n",
    "    e.g. 'good', 'Good', 'su', 'SU'\n",
    "    \"\"\"\n",
    "    good = []\n",
    "    for clust_id, label in cluster_label_dict.items():\n",
    "        if label in ['good', 'Good', 'su', 'SU']:\n",
    "            good.append(clust_id)\n",
    "    return sorted(good)\n",
    "\n",
    "def bin_spikes(spike_times_ms, spike_clusters, good_clusters,\n",
    "               bin_size_ms=25,\n",
    "               session_offset=0,\n",
    "               session_duration_ms=None):\n",
    "    \"\"\"\n",
    "    Bin 'good' clusters' spikes into 25 ms bins for one session.\n",
    "\n",
    "    - spike_times_ms: 1D array of spike times (ms, in concatenated timeline).\n",
    "    - spike_clusters: 1D array of cluster IDs for each spike_time.\n",
    "    - good_clusters: list of cluster IDs that are 'good'.\n",
    "    - bin_size_ms: size of each bin in ms (default = 25 ms).\n",
    "    - session_offset: the starting time (ms) of this session in the *concatenated* timeline.\n",
    "    - session_duration_ms: total duration of this session from that offset.\n",
    "    \n",
    "    Returns a 2D array of shape (n_good_clusters, n_time_bins).\n",
    "    \"\"\"\n",
    "    if session_duration_ms <= 0:\n",
    "        # If there's no valid time range, return an empty matrix\n",
    "        return np.zeros((len(good_clusters), 0), dtype=np.int32)\n",
    "    \n",
    "    t_start = session_offset\n",
    "    t_end   = session_offset + session_duration_ms\n",
    "    \n",
    "    spikes_remaining = sum(spike_times_ms>t_end)\n",
    "    print(f\"Spikes remaining after this session: {spikes_remaining}\")\n",
    "    # Boolean mask for spikes in [t_start, t_end)\n",
    "    in_session_mask = (spike_times_ms >= t_start) & (spike_times_ms < t_end)\n",
    "    \n",
    "    # Subset spike times & clusters\n",
    "    # (Make sure these arrays are 1D with .squeeze())\n",
    "    sess_spike_times    = spike_times_ms[in_session_mask].squeeze() - t_start\n",
    "    sess_spike_clusters = spike_clusters[in_session_mask].squeeze()\n",
    "    \n",
    "    # Figure out how many bins we need\n",
    "    n_bins = int(np.ceil(session_duration_ms / bin_size_ms))\n",
    "    \n",
    "    # We'll create a 2D array: shape = (len(good_clusters), n_bins)\n",
    "    spike_matrix = np.zeros((len(good_clusters), n_bins), dtype=np.int32)\n",
    "    \n",
    "    # Make a quick index from cluster_id -> row index in spike_matrix\n",
    "    cluster_index_map = {clust_id: i for i, clust_id in enumerate(good_clusters)}\n",
    "    \n",
    "    # Digitize times to bin indices\n",
    "    bin_indices = (sess_spike_times // bin_size_ms).astype(int)\n",
    "    \n",
    "    # Accumulate counts in each bin\n",
    "    for t_bin, clust in zip(bin_indices, sess_spike_clusters):\n",
    "        if 0 <= t_bin < n_bins:  # just a safety check\n",
    "            if clust in cluster_index_map:\n",
    "                spike_matrix[cluster_index_map[clust], t_bin] += 1\n",
    "    \n",
    "    return spike_matrix\n",
    "\n",
    "def main():\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Load JSON & Compute Session Boundaries\n",
    "    # -------------------------------------------------------------------------\n",
    "    kilosort_folder = r\"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/21032024_23032024_combined_all/\"\n",
    "\n",
    "    json_path =os.path.join(kilosort_folder, [i for i in os.listdir(kilosort_folder) if i.endswith('.json') and not i.startswith('.')][0])\n",
    "    \n",
    "    session_items = load_json(json_path)\n",
    "\n",
    "    bytes_per_sample = 768\n",
    "    sampling_rate_hz = 30000\n",
    "    bin_size_ms = 25  # keep the same bin size throughout\n",
    "    \n",
    "    session_info = []\n",
    "    cumulative_ms = 0.0\n",
    "    \n",
    "    for (file_path, file_size) in session_items:\n",
    "        n_samples = file_size / bytes_per_sample\n",
    "        duration_ms = n_samples / 30.0  # 30 samples per ms\n",
    "        start_ms = cumulative_ms\n",
    "        end_ms   = cumulative_ms + duration_ms\n",
    "        session_info.append((file_path, start_ms, end_ms))\n",
    "        cumulative_ms = end_ms\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Load Kilosort spike times & clusters\n",
    "    #    Convert spike_times from samples (30kHz) -> ms\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    spike_times_samples = np.load(os.path.join(kilosort_folder, 'spike_times.npy')).squeeze()\n",
    "    spike_clusters      = np.load(os.path.join(kilosort_folder, 'spike_clusters.npy')).squeeze()\n",
    "    \n",
    "    spike_times_ms = spike_times_samples / 30.0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Load cluster labels & pick 'good' clusters\n",
    "    # -------------------------------------------------------------------------\n",
    "    cluster_group_tsv   = os.path.join(kilosort_folder, 'cluster_group.tsv')\n",
    "    cluster_kslabel_tsv = os.path.join(kilosort_folder, 'cluster_KSlabel.tsv')\n",
    "    cluster_labels = load_cluster_labels(cluster_group_tsv, cluster_kslabel_tsv)\n",
    "    \n",
    "    good_clusters = get_good_clusters(cluster_labels)\n",
    "    print(f\"Found {len(good_clusters)} 'good' clusters (pre-FR-filter).\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) For each session, load sample_numbers and shift so first sync pulse is t=0\n",
    "    #    Then bin spikes in 25ms bins.\n",
    "    # -------------------------------------------------------------------------\n",
    "    session_spike_counts = []\n",
    "    \n",
    "    for i, (file_path, session_abs_start, session_abs_end) in enumerate(session_info):\n",
    "        # Replace /ceph/ with /Volumes/ to locate sync pulses\n",
    "        file_path_volumes = file_path.replace('/ceph/', '/Volumes/')\n",
    "       \n",
    "        session_dir = os.path.dirname(file_path_volumes)\n",
    "        \n",
    "        session_dir_sync = session_dir.replace('continuous', 'events')\n",
    "        \n",
    "        session_dir_sync = f\"{session_dir_sync}/TTL\"\n",
    "\n",
    "        sync_file  = os.path.join(session_dir_sync, 'sample_numbers.npy')\n",
    "        \n",
    "        ap_sample_numbers_path = file_path_volumes.replace('continuous.dat', 'sample_numbers.npy')\n",
    "        ap_events_sync_path = os.path.join(session_dir_sync, 'sample_numbers.npy')\n",
    "\n",
    "        # \n",
    "        # ap_sample_numbers_path = (recording_path / f\"continuous/Neuropix-PXI-100.{subject_id}-AP/sample_numbers.npy\")\n",
    "        # lfp_sample_numbers_path = ( recording_path / f\"continuous/Neuropix-PXI-100.{subject_id}-LFP/sample_numbers.npy\")\n",
    "        # ap_events_sync_path = (recording_path / f\"events/Neuropix-PXI-100.{subject_id}-AP\" / \"TTL/sample_numbers.npy\")\n",
    "        # lfp_events_sync_path = (recording_path / f\"events/Neuropix-PXI-100.{subject_id}-LFP\" / \"TTL/sample_numbers.npy\")\n",
    "        \n",
    "        sync_times = np.load(ap_events_sync_path)[::2]  # pulse pulse start & end times, just take start\n",
    "        sample_numbers = np.load(ap_sample_numbers_path)\n",
    "        adjusted_sync_times = adjusted_syncs = np.array([np.argmax(sample_numbers > sync) for sync in sync_times]) #note vectorising uses to much memory\n",
    "\n",
    "        if not os.path.exists(sync_file):\n",
    "            print(f\"Warning: no sync_file found for {file_path}.  Using entire session as is.\")\n",
    "            first_sync_ms = 0.0\n",
    "        else:\n",
    "            sync_times = np.load(ap_events_sync_path)[::2]  # pulse pulse start & end times, just take start\n",
    "            sample_numbers = np.load(ap_sample_numbers_path)\n",
    "            adjusted_sync_times = adjusted_syncs = np.array([np.argmax(sample_numbers > sync) for sync in sync_times]) #note vectorising uses to much me\n",
    "            \n",
    "            if len(adjusted_sync_times) != 0:\n",
    "                first_sync_sample = adjusted_sync_times[0]\n",
    "                first_sync_ms = first_sync_sample / 30.0\n",
    "                print(f\"Session {i}: First sync pulse at {first_sync_sample} samples => {first_sync_ms:.2f} ms\")\n",
    "            else:\n",
    "                print(f\"Warning: empty sync_file for {file_path}. Using entire session as is.\")\n",
    "                first_sync_ms = 0.0\n",
    "        \n",
    "        new_session_start = session_abs_start + first_sync_ms\n",
    "        new_session_duration = session_abs_end - new_session_start\n",
    "        \n",
    "        # bin the spikes\n",
    "        spike_count_mat = bin_spikes(\n",
    "            spike_times_ms=spike_times_ms,\n",
    "            spike_clusters=spike_clusters,\n",
    "            good_clusters=good_clusters,\n",
    "            bin_size_ms=bin_size_ms,\n",
    "            session_offset=new_session_start,\n",
    "            session_duration_ms=new_session_duration\n",
    "        )\n",
    "        \n",
    "        session_spike_counts.append(spike_count_mat)\n",
    "        \n",
    "        print(f\"Session {i}: {file_path}\")\n",
    "        print(f\"    final time range: [{new_session_start:.2f}, {new_session_start + new_session_duration:.2f}) ms\")\n",
    "        print(f\"    binned shape: {spike_count_mat.shape}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) (Optional) Save raw binned results BEFORE FR filtering\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_dir = '/Users/AdamHarris/Desktop/test_output_mats_2425'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, (file_path, _, _) in enumerate(session_info):\n",
    "        out_path = os.path.join(output_dir, f\"binnedSpikes_session{i}_unfiltered.npy\")\n",
    "        np.save(out_path, session_spike_counts[i])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) Compute mean firing rates and identify any session with total zero mean\n",
    "    # -------------------------------------------------------------------------\n",
    "    # We'll build an array of shape [n_sessions, n_good_clusters]\n",
    "    # Then we can see if a cluster's FR < threshold in ANY session.\n",
    "    \n",
    "    n_sessions = len(session_spike_counts)\n",
    "    n_good     = len(good_clusters)\n",
    "    FR_means   = np.zeros((n_sessions, n_good), dtype=np.float32)\n",
    "    \n",
    "    for s in range(n_sessions):\n",
    "        spike_count_mat = session_spike_counts[s]  # shape: (n_good, n_time_bins)\n",
    "        total_spikes = np.sum(spike_count_mat, axis=1)  # sum across bins => shape: (n_good,)\n",
    "        \n",
    "        # each bin is 25 ms => 0.025 s. So total session time in seconds:\n",
    "        total_time_s = spike_count_mat.shape[1] * (bin_size_ms / 1000.0)\n",
    "        \n",
    "        # Firing rate (spikes/sec) for each cluster\n",
    "        # if total_time_s=0, we'd get a divide-by-zero; we can handle that\n",
    "        if total_time_s > 0:\n",
    "            cluster_fr = total_spikes / total_time_s\n",
    "        else:\n",
    "            cluster_fr = np.zeros(n_good, dtype=np.float32)\n",
    "        \n",
    "        FR_means[s, :] = cluster_fr\n",
    "    \n",
    "    # session_means: overall FR across all clusters for each session\n",
    "    # if session_means == 0 => that session is considered invalid\n",
    "    session_means = np.mean(FR_means, axis=1)  # shape: (n_sessions,)\n",
    "    session_mask  = (session_means != 0.0)     # ignore sessions w/ zero overall FR\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Filter out clusters whose FR dips below threshold in ANY valid session\n",
    "    # -------------------------------------------------------------------------\n",
    "    FR_THRESHOLD = 0.002  # e.g. 0.002 spikes/s\n",
    "    # For each cluster, we want to see if FR < 0.002 in any session where session_mask==True\n",
    "    # => cluster fails if it is < 0.002 in ANY valid session.\n",
    "    \n",
    "    # FR_means[session_mask, :] => shape: (#valid_sessions, n_good)\n",
    "    # (FR_means[session_mask, :] < FR_THRESHOLD) => bool array of same shape\n",
    "    # .any(axis=0) => True for any cluster that fails\n",
    "    clusters_below_threshold = (FR_means[session_mask, :] < FR_THRESHOLD).any(axis=0)\n",
    "    # The final mask is True for clusters that pass\n",
    "    FR_mask = ~clusters_below_threshold  # shape: (n_good,)\n",
    "    \n",
    "    # We'll create a new list of truly-good clusters\n",
    "    filtered_good_clusters = [c for c, keep in zip(good_clusters, FR_mask) if keep]\n",
    "    print(f\"Filtering: {np.sum(~FR_mask)} clusters fail FR< {FR_THRESHOLD} criterion.\") \n",
    "    print(f\"Remaining: {np.sum(FR_mask)} clusters.\")\n",
    "    \n",
    "    # Now, also filter the session_spike_counts arrays\n",
    "    filtered_spike_counts = []\n",
    "    for s in range(n_sessions):\n",
    "        mat = session_spike_counts[s]  # shape: (n_good, n_time_bins)\n",
    "        mat_filtered = mat[FR_mask, :] # keep only clusters that pass\n",
    "        filtered_spike_counts.append(mat_filtered)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save final, filtered results\n",
    "    # -------------------------------------------------------------------------\n",
    "    # We can either overwrite the old output or store in new files\n",
    "    filtered_output_dir = '/Users/AdamHarris/Desktop/test_output_mats_filtered_2123'\n",
    "    os.makedirs(filtered_output_dir, exist_ok=True)\n",
    "    \n",
    "    for s, (file_path, _, _) in enumerate(session_info):\n",
    "        out_path = os.path.join(filtered_output_dir, f\"binnedSpikes_session{s}.npy\")\n",
    "        np.save(out_path, filtered_spike_counts[s])\n",
    "    \n",
    "    # We might also want to save the new list of clusters\n",
    "    cluster_out_path = os.path.join(filtered_output_dir, \"good_clusters_filtered.npy\")\n",
    "    np.save(cluster_out_path, np.array(filtered_good_clusters))\n",
    "    \n",
    "    print(\"All sessions binned and firing-rate-filtered. Final # of clusters:\", len(filtered_good_clusters))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
