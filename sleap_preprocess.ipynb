{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tifffile\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_masks_from_directory(mask_dir):\n",
    "    \"\"\"\n",
    "    Load all TIFF binary mask images from a directory. \n",
    "    Assumes each mask file is named something like 'node_1.tiff', 'edge_1-2.tiff', etc.\n",
    "    Returns a dictionary: \n",
    "        key = subcomponent name (e.g. 'node_1'), \n",
    "        value = 2D numpy array for the binary mask.\n",
    "    \"\"\"\n",
    "    mask_dict = {}\n",
    "    # Adjust the pattern below if your TIFFs follow a different naming convention\n",
    "    tiff_files = glob.glob(os.path.join(mask_dir, \"*.tif*\"))\n",
    "\n",
    "    for tiff_path in tiff_files:\n",
    "        # Extract subcomponent name from filename (e.g., 'node_1' from 'node_1.tif')\n",
    "        filename = os.path.basename(tiff_path)\n",
    "        # Remove file extension to get subcomponent label\n",
    "        subcomponent_name, _ = os.path.splitext(filename)\n",
    "        \n",
    "        # Read the binary mask as a 2D numpy array\n",
    "        mask_array = tifffile.imread(tiff_path)\n",
    "        \n",
    "        # If the mask is not already boolean, ensure it is cast properly\n",
    "        # (assuming 255 for True, 0 for False, or 1 for True, 0 for False)\n",
    "        mask_array = mask_array.astype(bool)\n",
    "        \n",
    "        mask_dict[subcomponent_name] = mask_array\n",
    "    \n",
    "    return mask_dict\n",
    "\n",
    "\n",
    "def map_body_parts_to_subcomponent(sleap_csv_path, mask_dir, output_csv_path, \n",
    "                                   bodypart_names=None, x_suffix=\".x\", y_suffix=\".y\"):\n",
    "    \"\"\"\n",
    "    Loads a SLEAP CSV of body part coordinates, loads TIFF binary masks for maze subcomponents,\n",
    "    and outputs a new CSV where each column is a body part and each row is the subcomponent \n",
    "    where that body part resides at that timepoint.\n",
    "\n",
    "    Args:\n",
    "        sleap_csv_path (str): Path to the SLEAP CSV file.\n",
    "        mask_dir (str): Directory containing the binary mask TIFF files.\n",
    "        output_csv_path (str): Path for saving the output CSV.\n",
    "        bodypart_names (list): If None, the code will infer them from CSV column names\n",
    "                               (anything ending with x_suffix or y_suffix).\n",
    "        x_suffix (str): Suffix used in the CSV for the x-coordinate columns (default \"_x\").\n",
    "        y_suffix (str): Suffix used in the CSV for the y-coordinate columns (default \"_y\").\n",
    "    \"\"\"\n",
    "    # 1. Load the SLEAP CSV\n",
    "    sleap_df = pd.read_csv(sleap_csv_path)\n",
    "    \n",
    "    # If user hasn't specified the bodypart names, infer them from columns\n",
    "    if bodypart_names is None:\n",
    "        # If columns look like 'nose_x', 'nose_y', 'tail_x', 'tail_y', etc.\n",
    "        # then we identify them by removing the suffixes\n",
    "        x_cols = [col for col in sleap_df.columns if col.endswith(x_suffix)]\n",
    "        bodypart_names = [col.replace(x_suffix, \"\") for col in x_cols]\n",
    "\n",
    "    # 2. Load all the binary masks\n",
    "    masks = load_masks_from_directory(mask_dir)\n",
    "    plt.matshow(masks[\"node_1\"])\n",
    "    # masks is a dict: subcomponent_name -> boolean 2D array\n",
    "\n",
    "    # 3. Prepare output DataFrame: \n",
    "    # one row per row in SLEAP CSV, columns = bodypart names\n",
    "    # each cell will eventually be the subcomponent name (str)\n",
    "    output_df = pd.DataFrame(index=sleap_df.index, columns=bodypart_names, dtype=object)\n",
    "\n",
    "    # 4. For each row (timepoint) in SLEAP, for each body part:\n",
    "    #    find which subcomponent mask is True at that coordinate\n",
    "    for i, row_data in sleap_df.iterrows():\n",
    "        # For each body part:\n",
    "        for bp in bodypart_names:\n",
    "            x_col = f\"{bp}{x_suffix}\"\n",
    "            y_col = f\"{bp}{y_suffix}\"\n",
    "\n",
    "            if x_col not in sleap_df.columns or y_col not in sleap_df.columns:\n",
    "                # If for some reason these columns aren't present, skip\n",
    "                output_df.at[i, bp] = None\n",
    "                continue\n",
    "\n",
    "            x_coord = row_data[x_col]\n",
    "            y_coord = row_data[y_col]\n",
    "\n",
    "            # Round or convert the float coordinates to integer indices\n",
    "            # SLEAP outputs may be float, whereas the mask arrays use integer pixel indices\n",
    "            x_idx = int(round(x_coord))\n",
    "            y_idx = int(round(y_coord))\n",
    "            \n",
    "            # Handle edge cases: if out of image bounds, skip or label as None\n",
    "            # First, pick an approach. Let's label out-of-bounds as \"None\" or \"outside\".\n",
    "            # Adjust if you prefer some other convention.\n",
    "            in_bounds = True\n",
    "            mask_shape = None\n",
    "            for subcomponent_name, mask_array in masks.items():\n",
    "                mask_shape = mask_array.shape\n",
    "                break\n",
    "\n",
    "            if mask_shape is not None:\n",
    "                height, width = mask_shape\n",
    "                if x_idx < 0 or x_idx >= width or y_idx < 0 or y_idx >= height:\n",
    "                    # Out of bounds\n",
    "                    output_df.at[i, bp] = \"None\"\n",
    "                    continue\n",
    "            \n",
    "            # Now find which mask is True at (y_idx, x_idx).\n",
    "            # We assume only one subcomponent is True at a given point,\n",
    "            # or if multiple are True, we take the first. \n",
    "            # Adjust logic as needed.\n",
    "            found_subcomponent = False\n",
    "            for subcomponent_name, mask_array in masks.items():\n",
    "                # mask_array[y, x] = True if inside that subcomponent\n",
    "                if mask_array[y_idx, x_idx]:\n",
    "                    output_df.at[i, bp] = subcomponent_name\n",
    "                    found_subcomponent = True\n",
    "                    break\n",
    "            \n",
    "            if not found_subcomponent:\n",
    "                # Not found in any mask; label as None or \"background\"\n",
    "                output_df.at[i, bp] = \"None\"\n",
    "\n",
    "    # 5. Save out the new CSV\n",
    "    output_df.to_csv(output_csv_path, index=False)  \n",
    "    print(f\"Output saved to: {output_csv_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleap_csv = \"/Users/AdamHarris/Desktop/mEC_SLEAP/labels.v004.025_bp01_2024-03-31-163524.analysis.csv\"\n",
    "masks_directory = \"/Users/AdamHarris/Desktop/bp01_Masks/late/\"\n",
    "output_csv = \"/Users/AdamHarris/Desktop/output_ROI.csv\"\n",
    "\n",
    "map_body_parts_to_subcomponent(\n",
    "    sleap_csv_path=sleap_csv,\n",
    "    mask_dir=masks_directory,\n",
    "    output_csv_path=output_csv,\n",
    "    # Optional: explicitly list bodypart names if you know them; else it infers from columns\n",
    "    bodypart_names=None,\n",
    "    x_suffix=\".x\",\n",
    "    y_suffix=\".y\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleap_df = pd.read_csv(sleap_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [col for col in sleap_df.columns if col.endswith(\".x\")]\n",
    "bodypart_names = [col.replace(\".x\", \"\") for col in x_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodypart_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(math.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tifffile\n",
    "\n",
    "\n",
    "def load_masks_from_directory(mask_dir):\n",
    "    \"\"\"\n",
    "    Load all TIFF binary mask images from a directory. \n",
    "    Assumes each mask file is named like 'node_1.tif', 'edge_1-2.tif', etc.\n",
    "    Returns a dictionary: \n",
    "        key = subcomponent name (e.g. 'node_1'), \n",
    "        value = 2D numpy array for the binary mask.\n",
    "    \"\"\"\n",
    "    mask_dict = {}\n",
    "    tiff_files = glob.glob(os.path.join(mask_dir, \"*.tif*\"))\n",
    "\n",
    "    for tiff_path in tiff_files:\n",
    "        filename = os.path.basename(tiff_path)\n",
    "        # Remove file extension to get subcomponent label\n",
    "        subcomponent_name, _ = os.path.splitext(filename)\n",
    "        \n",
    "        # Read the binary mask as a 2D numpy array\n",
    "        mask_array = tifffile.imread(tiff_path)\n",
    "        \n",
    "        # Ensure boolean type (True/False)\n",
    "        mask_array = mask_array.astype(bool)\n",
    "        \n",
    "        mask_dict[subcomponent_name] = mask_array\n",
    "    \n",
    "    return mask_dict\n",
    "\n",
    "\n",
    "def detect_outliers_by_jump(df, bodypart_names, x_suffix, y_suffix,\n",
    "                            distance_threshold=None,\n",
    "                            robust=True, multiplier=3.0):\n",
    "    \"\"\"\n",
    "    Detect outliers based on large frame-to-frame jumps for each bodypart.\n",
    "    If the distance between consecutive frames exceeds a certain threshold,\n",
    "    mark those coordinates as NaN.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The data containing body part coordinates (e.g. from a SLEAP CSV).\n",
    "    bodypart_names : list of str\n",
    "        List of bodypart labels to process (e.g. [\"nose\", \"tail\"]).\n",
    "    x_suffix : str\n",
    "        Suffix for x-coordinates (e.g. \".x\" or \"_x\").\n",
    "    y_suffix : str\n",
    "        Suffix for y-coordinates.\n",
    "    distance_threshold : float or None\n",
    "        If a float, use this fixed threshold (in pixels).\n",
    "        If None, compute a robust threshold from the distribution of frame-to-frame distances\n",
    "        using median absolute deviation (MAD) or standard deviation, as specified by 'robust'.\n",
    "    robust : bool\n",
    "        If True and distance_threshold is None, use median absolute deviation to set a threshold.\n",
    "        If False and distance_threshold is None, use mean + multiplier * std.\n",
    "    multiplier : float\n",
    "        The multiplier for the robust or standard deviation approach. E.g., 3.0 for 3*MAD or 3*std.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A copy of df where outliers have been marked as NaN.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    for bp in bodypart_names:\n",
    "        x_col = f\"{bp}{x_suffix}\"\n",
    "        y_col = f\"{bp}{y_suffix}\"\n",
    "\n",
    "        if x_col not in df_out.columns or y_col not in df_out.columns:\n",
    "            continue\n",
    "        \n",
    "        x_vals = df_out[x_col].values\n",
    "        y_vals = df_out[y_col].values\n",
    "\n",
    "        # Calculate frame-to-frame distances\n",
    "        # distance[i] = distance between (x[i], y[i]) and (x[i-1], y[i-1])\n",
    "        dx = np.diff(x_vals)\n",
    "        dy = np.diff(y_vals)\n",
    "        dist = np.sqrt(dx**2 + dy**2)\n",
    "        \n",
    "        # If no threshold provided, compute from data\n",
    "        if distance_threshold is None:\n",
    "            # We'll skip NaNs in dist\n",
    "            dist_no_nan = dist[~np.isnan(dist)]\n",
    "            if len(dist_no_nan) == 0:\n",
    "                # no valid distances, skip\n",
    "                continue\n",
    "            if robust:\n",
    "                # Use median + multiplier * MAD\n",
    "                median_dist = np.median(dist_no_nan)\n",
    "                mad = np.median(np.abs(dist_no_nan - median_dist))\n",
    "                # A common robust scale factor for MAD is 1.4826,\n",
    "                # but we can keep it simple or adapt as needed.\n",
    "                if mad == 0:\n",
    "                    # fallback if all distances are the same\n",
    "                    distance_threshold = median_dist * multiplier\n",
    "                else:\n",
    "                    distance_threshold = median_dist + multiplier * mad\n",
    "            else:\n",
    "                # Use mean + multiplier * std\n",
    "                mean_dist = np.mean(dist_no_nan)\n",
    "                std_dist = np.std(dist_no_nan)\n",
    "                distance_threshold = mean_dist + multiplier * std_dist\n",
    "\n",
    "        # Now we have a distance_threshold, let's mark outliers\n",
    "        # dist[i] corresponds to jump from frame i to i+1 in x_vals\n",
    "        # We can mark frame i+1 as NaN if dist[i] is above threshold\n",
    "        outlier_indices = np.where(dist > distance_threshold)[0] + 1  # shift by 1 for the \"destination\" frame\n",
    "\n",
    "        # Mark outliers as NaN\n",
    "        x_vals[outlier_indices] = np.nan\n",
    "        y_vals[outlier_indices] = np.nan\n",
    "\n",
    "        # Save back\n",
    "        df_out[x_col] = x_vals\n",
    "        df_out[y_col] = y_vals\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "def interpolate_and_smooth_coordinates(df, bodypart_names, x_suffix, y_suffix,\n",
    "                                       interpolation_method='linear', \n",
    "                                       rolling_window=5):\n",
    "    \"\"\"\n",
    "    For each bodypart.x and bodypart.y column:\n",
    "      1. Perform NaN interpolation (default 'linear').\n",
    "      2. Smooth data with a rolling mean of specified window size.\n",
    "    \"\"\"\n",
    "    for bp in bodypart_names:\n",
    "        x_col = f\"{bp}{x_suffix}\"\n",
    "        y_col = f\"{bp}{y_suffix}\"\n",
    "\n",
    "        # Skip if columns don't exist\n",
    "        if x_col not in df.columns or y_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Interpolation (fills internal NaNs, can also do 'limit_direction' or 'order' if polynomial, etc.)\n",
    "        df[x_col] = df[x_col].interpolate(method=interpolation_method, limit_direction='both')\n",
    "        df[y_col] = df[y_col].interpolate(method=interpolation_method, limit_direction='both')\n",
    "\n",
    "        # Smoothing with rolling mean\n",
    "        df[x_col] = df[x_col].rolling(window=rolling_window, min_periods=1, center=True).mean()\n",
    "        df[y_col] = df[y_col].rolling(window=rolling_window, min_periods=1, center=True).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def map_body_parts_to_subcomponent(sleap_csv_path, \n",
    "                                   mask_dir, \n",
    "                                   output_csv_path, \n",
    "                                   bodypart_names=None, \n",
    "                                   x_suffix=\"_x\", \n",
    "                                   y_suffix=\"_y\",\n",
    "                                   # Outlier detection parameters\n",
    "                                   outlier_detection=True,\n",
    "                                   distance_threshold=None,\n",
    "                                   robust=True,\n",
    "                                   multiplier=3.0,\n",
    "                                   # Interpolation & smoothing parameters\n",
    "                                   interpolation_method='linear',\n",
    "                                   rolling_window=5):\n",
    "    \"\"\"\n",
    "    1) Loads SLEAP CSV of body part coordinates\n",
    "    2) Optionally detects & removes large jump outliers\n",
    "    3) Interpolates + smooths\n",
    "    4) Loads TIFF binary masks\n",
    "    5) Maps each body part's (x,y) to a subcomponent\n",
    "    6) Exports the final mapping in a CSV\n",
    "    \n",
    "    Args:\n",
    "        sleap_csv_path (str): Path to the SLEAP CSV file.\n",
    "        mask_dir (str): Directory containing the binary mask TIFF files.\n",
    "        output_csv_path (str): Path for saving the output CSV.\n",
    "        bodypart_names (list): If None, the code will infer from CSV col names that end with x_suffix/y_suffix.\n",
    "        x_suffix (str): Suffix for x coordinates (default \"_x\").\n",
    "        y_suffix (str): Suffix for y coordinates (default \"_y\").\n",
    "        \n",
    "        outlier_detection (bool): Whether to do outlier detection by large jumps.\n",
    "        distance_threshold (float or None): If float, fixed threshold in pixels. If None, auto-calc from data.\n",
    "        robust (bool): If True and threshold is None, use median+MAD approach; else use mean+std.\n",
    "        multiplier (float): Multiplier for outlier detection threshold.\n",
    "        \n",
    "        interpolation_method (str): Pandas interpolation method (e.g., 'linear', 'time', 'polynomial', etc.).\n",
    "        rolling_window (int): Window size for rolling average smoothing.\n",
    "    \"\"\"\n",
    "    # 1. Load SLEAP CSV\n",
    "    sleap_df = pd.read_csv(sleap_csv_path)\n",
    "    \n",
    "    # If user hasn't specified bodypart names, infer them from columns\n",
    "    if bodypart_names is None:\n",
    "        x_cols = [col for col in sleap_df.columns if col.endswith(x_suffix)]\n",
    "        bodypart_names = [col.replace(x_suffix, \"\") for col in x_cols]\n",
    "\n",
    "    # 2. Outlier detection & removal\n",
    "    if outlier_detection:\n",
    "        sleap_df = detect_outliers_by_jump(\n",
    "            sleap_df, \n",
    "            bodypart_names, \n",
    "            x_suffix, \n",
    "            y_suffix,\n",
    "            distance_threshold=distance_threshold,\n",
    "            robust=robust, \n",
    "            multiplier=multiplier\n",
    "        )\n",
    "\n",
    "    # 3. Interpolate & smooth\n",
    "    sleap_df = interpolate_and_smooth_coordinates(\n",
    "        sleap_df, \n",
    "        bodypart_names, \n",
    "        x_suffix, \n",
    "        y_suffix,\n",
    "        interpolation_method=interpolation_method, \n",
    "        rolling_window=rolling_window\n",
    "    )\n",
    "\n",
    "    # 4. Load binary masks\n",
    "    masks = load_masks_from_directory(mask_dir)\n",
    "    # We assume consistent shape\n",
    "    mask_shape = None\n",
    "    if len(masks) > 0:\n",
    "        first_key = next(iter(masks.keys()))\n",
    "        mask_shape = masks[first_key].shape\n",
    "\n",
    "    # 5. Map each body part to subcomponent\n",
    "    output_df = pd.DataFrame(index=sleap_df.index, columns=bodypart_names, dtype=object)\n",
    "\n",
    "    for i, row_data in sleap_df.iterrows():\n",
    "        for bp in bodypart_names:\n",
    "            x_col = f\"{bp}{x_suffix}\"\n",
    "            y_col = f\"{bp}{y_suffix}\"\n",
    "\n",
    "            if x_col not in sleap_df.columns or y_col not in sleap_df.columns:\n",
    "                output_df.at[i, bp] = \"None\"\n",
    "                continue\n",
    "\n",
    "            x_coord = row_data[x_col]\n",
    "            y_coord = row_data[y_col]\n",
    "\n",
    "            if pd.isna(x_coord) or pd.isna(y_coord):\n",
    "                output_df.at[i, bp] = \"None\"\n",
    "                continue\n",
    "            \n",
    "            x_idx = int(round(x_coord))\n",
    "            y_idx = int(round(y_coord))\n",
    "\n",
    "            # Check bounds\n",
    "            if mask_shape is not None:\n",
    "                h, w = mask_shape\n",
    "                if x_idx < 0 or x_idx >= w or y_idx < 0 or y_idx >= h:\n",
    "                    output_df.at[i, bp] = \"None\"\n",
    "                    continue\n",
    "\n",
    "            # Identify which mask is True\n",
    "            found_subcomponent = False\n",
    "            for subcomp_name, mask_arr in masks.items():\n",
    "                if mask_arr[y_idx, x_idx]:\n",
    "                    output_df.at[i, bp] = subcomp_name\n",
    "                    found_subcomponent = True\n",
    "                    break\n",
    "            if not found_subcomponent:\n",
    "                output_df.at[i, bp] = \"None\"\n",
    "\n",
    "    # 6. Save final output\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Output saved to: {output_csv_path}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Example usage (if needed):\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example file paths\n",
    "    sleap_csv = \"/Users/AdamHarris/Desktop/mEC_SLEAP/labels.v004.025_bp01_2024-03-31-163524.analysis.csv\"\n",
    "    masks_directory = \"/Users/AdamHarris/Desktop/bp01_Masks/late/\"\n",
    "    output_csv = \"/Users/AdamHarris/Desktop/output_ROI.csv\"\n",
    "\n",
    "    map_body_parts_to_subcomponent(\n",
    "        sleap_csv_path=sleap_csv,\n",
    "        mask_dir=masks_directory,\n",
    "        output_csv_path=output_csv,\n",
    "        bodypart_names=None,             # or e.g. [\"nose\", \"tail_base\", \"left_ear\", \"right_ear\", ...]\n",
    "        x_suffix=\".x\",                  # if your columns are like \"nose.x\", \"nose.y\"\n",
    "        y_suffix=\".y\",                  #\n",
    "        outlier_detection=True,\n",
    "        distance_threshold=None,        # auto-compute from data\n",
    "        robust=True,                    # use median+MAD approach\n",
    "        multiplier=3.0,                 # threshold = median_dist + 3.0*MAD\n",
    "        interpolation_method='linear',\n",
    "        rolling_window=5                # window for rolling average\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_masks_from_directory(mask_dir):\n",
    "    \"\"\"\n",
    "    Load all TIFF binary mask images from a directory.\n",
    "    Returns a dictionary: subcomponent_name -> 2D boolean numpy array.\n",
    "    \"\"\"\n",
    "    mask_dict = {}\n",
    "    tiff_files = glob.glob(os.path.join(mask_dir, \"*.tif*\"))\n",
    "\n",
    "    for tiff_path in tiff_files:\n",
    "        filename = os.path.basename(tiff_path)\n",
    "        # Remove file extension to get subcomponent label (e.g. \"node_1\" from \"node_1.tif\")\n",
    "        subcomponent_name, _ = os.path.splitext(filename)\n",
    "        \n",
    "        # Read the binary mask as a 2D numpy array\n",
    "        mask_array = tifffile.imread(tiff_path)\n",
    "        \n",
    "        # Ensure it's boolean\n",
    "        mask_array = mask_array.astype(bool)\n",
    "        \n",
    "        mask_dict[subcomponent_name] = mask_array\n",
    "    \n",
    "    return mask_dict\n",
    "\n",
    "\n",
    "def detect_outliers_by_jump(df, bodypart_names, x_suffix, y_suffix,\n",
    "                            distance_threshold=None,\n",
    "                            robust=True, multiplier=3.0):\n",
    "    \"\"\"\n",
    "    Mark body-part positions as NaN if the frame-to-frame jump\n",
    "    exceeds a distance threshold.\n",
    "    \n",
    "    If distance_threshold is None, it is computed from the distribution\n",
    "    of all distances using either:\n",
    "      - median + multiplier * MAD (if robust=True), or\n",
    "      - mean + multiplier * std  (if robust=False).\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    for bp in bodypart_names:\n",
    "        x_col = f\"{bp}{x_suffix}\"\n",
    "        y_col = f\"{bp}{y_suffix}\"\n",
    "\n",
    "        # Skip if columns don't exist\n",
    "        if x_col not in df_out.columns or y_col not in df_out.columns:\n",
    "            continue\n",
    "        \n",
    "        x_vals = df_out[x_col].values\n",
    "        y_vals = df_out[y_col].values\n",
    "\n",
    "        # Calculate frame-to-frame distances\n",
    "        dx = np.diff(x_vals)\n",
    "        dy = np.diff(y_vals)\n",
    "        dist = np.sqrt(dx**2 + dy**2)\n",
    "        \n",
    "        # If no threshold provided, auto-compute from the data\n",
    "        if distance_threshold is None:\n",
    "            dist_no_nan = dist[~np.isnan(dist)]\n",
    "            if len(dist_no_nan) == 0:\n",
    "                # No valid distances, skip\n",
    "                continue\n",
    "\n",
    "            if robust:\n",
    "                # Use median + multiplier * MAD\n",
    "                median_dist = np.median(dist_no_nan)\n",
    "                mad = np.median(np.abs(dist_no_nan - median_dist))\n",
    "                if mad == 0:\n",
    "                    distance_threshold = median_dist * multiplier\n",
    "                else:\n",
    "                    distance_threshold = median_dist + multiplier * mad\n",
    "            else:\n",
    "                # mean + multiplier * std\n",
    "                mean_dist = np.mean(dist_no_nan)\n",
    "                std_dist = np.std(dist_no_nan)\n",
    "                distance_threshold = mean_dist + multiplier * std_dist\n",
    "\n",
    "        # dist[i] is jump from frame i to i+1\n",
    "        outlier_indices = np.where(dist > distance_threshold)[0] + 1\n",
    "        # Mark outliers as NaN in the 'destination' frame\n",
    "        x_vals[outlier_indices] = np.nan\n",
    "        y_vals[outlier_indices] = np.nan\n",
    "\n",
    "        df_out[x_col] = x_vals\n",
    "        df_out[y_col] = y_vals\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "def interpolate_and_smooth_coordinates(df, bodypart_names, x_suffix, y_suffix,\n",
    "                                       interpolation_method='linear', \n",
    "                                       rolling_window=5):\n",
    "    \"\"\"\n",
    "    1) Interpolates NaN values (default 'linear').\n",
    "    2) Applies a rolling average smoothing over a specified window.\n",
    "    \"\"\"\n",
    "    for bp in bodypart_names:\n",
    "        x_col = f\"{bp}{x_suffix}\"\n",
    "        y_col = f\"{bp}{y_suffix}\"\n",
    "\n",
    "        # Skip if missing columns\n",
    "        if x_col not in df.columns or y_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Interpolate: fill internal NaNs from neighbors\n",
    "        df[x_col] = df[x_col].interpolate(method=interpolation_method, limit_direction='both')\n",
    "        df[y_col] = df[y_col].interpolate(method=interpolation_method, limit_direction='both')\n",
    "\n",
    "        # Rolling mean smoothing\n",
    "        df[x_col] = df[x_col].rolling(window=rolling_window, min_periods=1, center=True).mean()\n",
    "        df[y_col] = df[y_col].rolling(window=rolling_window, min_periods=1, center=True).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def estimate_affine_transform(pixel_points, physical_points):\n",
    "    \"\"\"\n",
    "    Estimate the 2x2 matrix A and 2x1 vector b that maps:\n",
    "        [x_phys, y_phys]^T = A [x_pix, y_pix]^T + b\n",
    "    using a set of corresponding pixel_points and physical_points.\n",
    "\n",
    "    pixel_points:   Nx2 array of (x_pix, y_pix)\n",
    "    physical_points: Nx2 array of (x_phys, y_phys)\n",
    "    Returns:\n",
    "        A (2x2 numpy array), b (2x1 numpy array)\n",
    "    \"\"\"\n",
    "    N = pixel_points.shape[0]\n",
    "\n",
    "    # Build the design matrix M (2N x 6) and vector v (2N x 1).\n",
    "    # The unknown parameters are [a11, a12, bx, a21, a22, by].\n",
    "    M = np.zeros((2*N, 6))\n",
    "    v = np.zeros((2*N,))\n",
    "\n",
    "    for i in range(N):\n",
    "        x_pix, y_pix = pixel_points[i]\n",
    "        x_phys, y_phys = physical_points[i]\n",
    "\n",
    "        # x_phys = a11*x_pix + a12*y_pix + bx\n",
    "        M[2*i, 0] = x_pix\n",
    "        M[2*i, 1] = y_pix\n",
    "        M[2*i, 2] = 1.0\n",
    "        v[2*i]     = x_phys\n",
    "\n",
    "        # y_phys = a21*x_pix + a22*y_pix + by\n",
    "        M[2*i+1, 3] = x_pix\n",
    "        M[2*i+1, 4] = y_pix\n",
    "        M[2*i+1, 5] = 1.0\n",
    "        v[2*i+1]    = y_phys\n",
    "\n",
    "    # Solve in a least squares sense\n",
    "    solution, residuals, rank, s = np.linalg.lstsq(M, v, rcond=None)\n",
    "    a11, a12, bx, a21, a22, by = solution\n",
    "\n",
    "    A = np.array([[a11, a12],\n",
    "                  [a21, a22]])\n",
    "    b = np.array([bx, by])\n",
    "\n",
    "    return A, b\n",
    "\n",
    "def transform_points(pixel_points, A, b):\n",
    "    \"\"\"\n",
    "    Transform an Nx2 array of pixel_points via the affine transform:\n",
    "      [x_phys, y_phys]^T = A [x_pix, y_pix]^T + b\n",
    "    \"\"\"\n",
    "    pixel_points = np.asarray(pixel_points)            # shape (N, 2)\n",
    "    transformed = (A @ pixel_points.T).T + b           # shape (N, 2)\n",
    "    return transformed\n",
    "\n",
    "\n",
    "def transform_sleap_dataframe_inplace(df, A, b):\n",
    "    \"\"\"\n",
    "    For each body-part column pair that looks like <part>.x, <part>.y\n",
    "    in DataFrame df, transform those pixel coordinates in-place to physical (cm).\n",
    "    i.e., we overwrite the .x, .y columns with the new values.\n",
    "    \n",
    "    df: Pandas DataFrame with columns like 'nose.x', 'nose.y', ...\n",
    "    A, b: affine transform parameters\n",
    "    \"\"\"\n",
    "    # Find all columns ending with \".x\".\n",
    "    x_cols = [col for col in df.columns if col.endswith('.x')]\n",
    "    \n",
    "    for x_col in x_cols:\n",
    "        # Replace '.x' with '.y' to get the matching y column\n",
    "        y_col = x_col[:-2] + '.y'\n",
    "        if y_col not in df.columns:\n",
    "            # If the matching .y doesn't exist, skip\n",
    "            continue\n",
    "        \n",
    "        # Extract Nx2 array of pixel coordinates\n",
    "        pixel_coords = df[[x_col, y_col]].values\n",
    "        \n",
    "        # Transform\n",
    "        phys_coords = transform_points(pixel_coords, A, b)\n",
    "        \n",
    "        # Overwrite the DataFrame columns in-place\n",
    "        df[x_col] = phys_coords[:, 0]\n",
    "        df[y_col] = phys_coords[:, 1]\n",
    "    \n",
    "    # Return df (not strictly necessary if we modify in-place)\n",
    "    return df\n",
    "\n",
    "def compute_head_directions(df, x_suffix=\".x\", y_suffix=\".y\"):\n",
    "    \"\"\"\n",
    "    Given a DataFrame that has at least these columns:\n",
    "        head_mid.x, head_mid.y,\n",
    "        head_back.x, head_back.y,\n",
    "        ear_L.x,    ear_L.y,\n",
    "        ear_R.x,    ear_R.y\n",
    "    compute two angles per frame:\n",
    "      - back2mid_deg:  angle from head_back -> head_mid\n",
    "      - earL2earR_deg: angle from ear_L -> ear_R\n",
    "    Angles are in degrees, relative to the x-axis, using arctan2 (i.e. [-180, 180] range).\n",
    "\n",
    "    Returns a new DataFrame of shape (n_frames, 2).\n",
    "    \"\"\"\n",
    "    angles_df = pd.DataFrame(index=df.index, columns=[\"back2mid_deg\", \"earL2earR_deg\"], dtype=float)\n",
    "\n",
    "    def angle_deg(x1, y1, x2, y2):\n",
    "        return np.degrees(np.arctan2(y2 - y1, x2 - x1))\n",
    "\n",
    "    for i in df.index:\n",
    "        # Extract the relevant coordinates\n",
    "        # We do a quick check for any NaNs\n",
    "        needed_cols = [\"head_mid\", \"head_back\", \"ear_L\", \"ear_R\"]\n",
    "        coords = {}\n",
    "        is_nan = False\n",
    "        for part in needed_cols:\n",
    "            xcol = f\"{part}{x_suffix}\"\n",
    "            ycol = f\"{part}{y_suffix}\"\n",
    "            if (xcol not in df.columns) or (ycol not in df.columns):\n",
    "                is_nan = True\n",
    "            else:\n",
    "                if pd.isna(df.loc[i, xcol]) or pd.isna(df.loc[i, ycol]):\n",
    "                    is_nan = True\n",
    "            coords[part] = (df.loc[i, xcol], df.loc[i, ycol])\n",
    "\n",
    "        if is_nan:\n",
    "            angles_df.loc[i, [\"back2mid_deg\", \"earL2earR_deg\"]] = np.nan\n",
    "        else:\n",
    "            (xb, yb) = coords[\"head_back\"]\n",
    "            (xm, ym) = coords[\"head_mid\"]\n",
    "            (xl, yl) = coords[\"ear_L\"]\n",
    "            (xr, yr) = coords[\"ear_R\"]\n",
    "\n",
    "            # Angle from back -> mid\n",
    "            back2mid = angle_deg(xb, yb, xm, ym)\n",
    "            # Angle from left ear -> right ear\n",
    "            earL2earR = angle_deg(xl, yl, xr, yr)\n",
    "\n",
    "            angles_df.loc[i, \"back2mid_deg\"] = back2mid\n",
    "            angles_df.loc[i, \"earL2earR_deg\"] = earL2earR\n",
    "\n",
    "    return angles_df\n",
    "\n",
    "\n",
    "def map_body_parts_to_subcomponent(\n",
    "    sleap_csv_path,\n",
    "    mask_dir,\n",
    "    output_csv_path,\n",
    "    # Additional outputs\n",
    "    cleaned_coords_csv_path=\"cleaned_coordinates.csv\",\n",
    "    head_direction_csv_path=\"head_direction.csv\",\n",
    "    # Bodypart parsing\n",
    "    bodypart_names=None,\n",
    "    x_suffix=\".x\",\n",
    "    y_suffix=\".y\",\n",
    "    # Outlier detection params\n",
    "    outlier_detection=True,\n",
    "    distance_threshold=None,\n",
    "    robust=True,\n",
    "    multiplier=3.0,\n",
    "    transform_to_physical=False,\n",
    "    pixel_pts=None,\n",
    "    physical_pts=None,\n",
    "    # Interpolation/smoothing params\n",
    "    interpolation_method='linear',\n",
    "    rolling_window=5\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Load SLEAP CSV\n",
    "    2) Optionally do outlier detection\n",
    "    3) Interpolate + smooth\n",
    "    4) Save cleaned coordinates to CSV\n",
    "    5) Compute head direction from (head_mid, head_back, ear_L, ear_R), save to CSV\n",
    "    6) Load masks\n",
    "    7) Map each body-part coordinate to subcomponent, save final mapping\n",
    "\n",
    "    Args:\n",
    "        sleap_csv_path: path to the original SLEAP CSV\n",
    "        mask_dir: directory containing the binary mask TIFF files\n",
    "        output_csv_path: path for final subcomponent membership CSV\n",
    "        cleaned_coords_csv_path: path for saving the post-processed (x,y) CSV\n",
    "        head_direction_csv_path: path for saving the head orientation CSV\n",
    "        bodypart_names: if None, infer from columns. Otherwise, list of bodypart strings\n",
    "        x_suffix, y_suffix: how the SLEAP CSV columns are named (default \".x\"/\".y\")\n",
    "        outlier_detection: whether to remove large jumps\n",
    "        distance_threshold: pixel threshold for outliers (or None to auto-calc)\n",
    "        robust: if True and threshold is None, use median+MAD; else mean+std\n",
    "        multiplier: factor for outlier detection\n",
    "        interpolation_method: method for pd.DataFrame.interpolate (e.g. 'linear')\n",
    "        rolling_window: window size for rolling-average smoothing\n",
    "    \"\"\"\n",
    "    # 1) Load SLEAP CSV\n",
    "    sleap_df = pd.read_csv(sleap_csv_path)\n",
    "    sleap_df_formasks = sleap_df.copy()\n",
    "    \n",
    "    # 2) Identify bodypart names\n",
    "    if bodypart_names is None:\n",
    "        x_cols = [col for col in sleap_df.columns if col.endswith(x_suffix)]\n",
    "        bodypart_names = [col.replace(x_suffix, \"\") for col in x_cols]\n",
    "\n",
    "    # 3) Outlier detection (optional)\n",
    "    if outlier_detection:\n",
    "        sleap_df = detect_outliers_by_jump(\n",
    "            sleap_df, \n",
    "            bodypart_names,\n",
    "            x_suffix,\n",
    "            y_suffix,\n",
    "            distance_threshold=distance_threshold,\n",
    "            robust=robust,\n",
    "            multiplier=multiplier\n",
    "        )\n",
    "\n",
    "    # 4) Interpolate and smooth\n",
    "    sleap_df = interpolate_and_smooth_coordinates(\n",
    "        sleap_df,\n",
    "        bodypart_names,\n",
    "        x_suffix,\n",
    "        y_suffix,\n",
    "        interpolation_method=interpolation_method,\n",
    "        rolling_window=rolling_window\n",
    "    )\n",
    "\n",
    "    # 5 compute affine transform to put into physical coordinates\n",
    "    if transform_to_physical:\n",
    "        A, b = estimate_affine_transform(pixel_pts, physical_pts)\n",
    "        print(\"Estimated affine matrix A:\\n\", A)\n",
    "        print(\"Estimated translation vector b:\\n\", b)\n",
    "        sleap_df = transform_sleap_dataframe_inplace(sleap_df, A, b)\n",
    "\n",
    "    # ---- (A) SAVE the cleaned coordinates to CSV\n",
    "    sleap_df.to_csv(cleaned_coords_csv_path, index=False)\n",
    "    print(f\"Cleaned coordinates saved to: {cleaned_coords_csv_path}\")\n",
    "\n",
    "    # 5) Compute head direction angles from head_mid, head_back, ear_L, ear_R\n",
    "    head_directions_df = compute_head_directions(sleap_df, x_suffix, y_suffix)\n",
    "    # Save to CSV\n",
    "    head_directions_df.to_csv(head_direction_csv_path, index=False)\n",
    "    print(f\"Head direction angles saved to: {head_direction_csv_path}\")\n",
    "\n",
    "    # 6) Load the binary mask TIFFs\n",
    "    masks = load_masks_from_directory(mask_dir)\n",
    "    # We assume all masks share the same shape\n",
    "    mask_shape = None\n",
    "    if len(masks) > 0:\n",
    "        key = next(iter(masks.keys()))\n",
    "        mask_shape = masks[key].shape\n",
    "\n",
    "    # 7) Map each coordinate to subcomponent\n",
    "    out_subcomp_df = pd.DataFrame(index=sleap_df_formasks.index, columns=bodypart_names, dtype=object)\n",
    "    for i, row_data in sleap_df_formasks.iterrows():\n",
    "        for bp in bodypart_names:\n",
    "            x_col = f\"{bp}{x_suffix}\"\n",
    "            y_col = f\"{bp}{y_suffix}\"\n",
    "\n",
    "            if x_col not in sleap_df_formasks.columns or y_col not in sleap_df_formasks.columns:\n",
    "                out_subcomp_df.at[i, bp] = \"None\"\n",
    "                continue\n",
    "\n",
    "            x_coord = row_data[x_col]\n",
    "            y_coord = row_data[y_col]\n",
    "\n",
    "            # If still NaN, label as \"None\"\n",
    "            if pd.isna(x_coord) or pd.isna(y_coord):\n",
    "                out_subcomp_df.at[i, bp] = \"None\"\n",
    "                continue\n",
    "            \n",
    "            x_idx = int(round(x_coord))\n",
    "            y_idx = int(round(y_coord))\n",
    "\n",
    "            # Check if out of bounds\n",
    "            if mask_shape is not None:\n",
    "                h, w = mask_shape\n",
    "                if x_idx < 0 or x_idx >= w or y_idx < 0 or y_idx >= h:\n",
    "                    out_subcomp_df.at[i, bp] = \"None\"\n",
    "                    continue\n",
    "\n",
    "            # Identify which mask is True\n",
    "            found_subcomponent = False\n",
    "            for subcomp_name, mask_arr in masks.items():\n",
    "                if mask_arr[y_idx, x_idx]:\n",
    "                    out_subcomp_df.at[i, bp] = subcomp_name\n",
    "                    found_subcomponent = True\n",
    "                    break\n",
    "            if not found_subcomponent:\n",
    "                out_subcomp_df.at[i, bp] = \"None\"\n",
    "\n",
    "    # 8) Save final subcomponent membership\n",
    "    out_subcomp_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Subcomponent mapping saved to: {output_csv_path}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    folder = r'/Users/AdamHarris/Desktop/cohort_10_exploration/maze2'\n",
    "\n",
    "    \"\"\"\n",
    "    below is for mEC cohort\n",
    "    \"\"\"\n",
    "    late_pixel_pts = np.array([[1109, 826],\n",
    "                              [908, -5], \n",
    "                              [72, 174], \n",
    "                              [254, 1028]])\n",
    "\n",
    "    early_pixel_pts = np.array([[1208, 840],\n",
    "                              [989, 25], \n",
    "                              [184, 213], \n",
    "                              [376, 1030]])\n",
    "\n",
    "    physical_pts = np.array([[0, 0],\n",
    "                            [47, 0],\n",
    "                            [47, 47],\n",
    "                            [0, 47]])\n",
    "\n",
    "    \"\"\"\n",
    "    below is for lEC cohort\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    maze_1_pixel_pts = np.array([[1022, 974],\n",
    "                                [981, 136],\n",
    "                                [158, 169],\n",
    "                                [172, 1010]])\n",
    "\n",
    "    maze_2_pixel_pts = np.array([[234, 945],\n",
    "                                [1036, 953],\n",
    "                                [1020, 158],\n",
    "                                [248, 169]]\n",
    "                               )\n",
    "\n",
    "\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder)):\n",
    "        \n",
    "        if not filename.endswith(\".csv\"):\n",
    "            continue\n",
    "        \n",
    "        sleap_csv = os.path.join(folder, filename)\n",
    "        print(f\"now working with {sleap_csv}\")\n",
    "        masks_directory = \"/Users/AdamHarris/Desktop/cohort10_masks/maze2\"\n",
    "        output_csv = f\"/Users/AdamHarris/Desktop/cohort_10_exploration/processed/{filename[16:-4]}.csv\"\n",
    "\n",
    "        map_body_parts_to_subcomponent(\n",
    "            sleap_csv_path=sleap_csv,\n",
    "            mask_dir=masks_directory,\n",
    "            output_csv_path=f\"/Users/AdamHarris/Desktop/cohort_10_exploration/processed/{filename[16:-4]}_ROIs.csv\",\n",
    "            cleaned_coords_csv_path=f\"/Users/AdamHarris/Desktop/cohort_10_exploration/processed/{filename[16:-4]}_cleaned_coordinates.csv\",\n",
    "            head_direction_csv_path=f\"/Users/AdamHarris/Desktop/cohort_10_exploration/processed/{filename[16:-4]}_head_direction.csv\",\n",
    "            bodypart_names=None, #[\"head_mid\", \"head_back\", \"ear_L\", \"ear_R\"],\n",
    "            x_suffix=\".x\",\n",
    "            y_suffix=\".y\",\n",
    "            outlier_detection=True,\n",
    "            distance_threshold=None,   # auto-compute\n",
    "            robust=True,              # median+MAD approach\n",
    "            multiplier=3.0,\n",
    "            transform_to_physical=True,\n",
    "            pixel_pts=maze_2_pixel_pts,\n",
    "            physical_pts=physical_pts,\n",
    "            interpolation_method='linear',\n",
    "            rolling_window=5\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "def map_body_parts_to_subcomponent(sleap_h5_path, \n",
    "                                   mask_dir, \n",
    "                                   output_csv_path, \n",
    "                                   bodypart_names=None, \n",
    "                                   x_suffix=\"_x\", \n",
    "                                   y_suffix=\"_y\",\n",
    "                                   # Outlier detection parameters\n",
    "                                   outlier_detection=True,\n",
    "                                   distance_threshold=None,\n",
    "                                   robust=True,\n",
    "                                   multiplier=3.0,\n",
    "                                   # Interpolation & smoothing parameters\n",
    "                                   interpolation_method='linear',\n",
    "                                   rolling_window=5):\n",
    "    \"\"\"\n",
    "    1) Loads SLEAP HDF5 of body part coordinates\n",
    "    2) Optionally detects & removes large jump outliers\n",
    "    3) Interpolates + smooths\n",
    "    4) Loads TIFF binary masks\n",
    "    5) Maps each body part's (x,y) to a subcomponent\n",
    "    6) Exports the final mapping in a CSV\n",
    "    \n",
    "    Args:\n",
    "        sleap_h5_path (str): Path to the SLEAP HDF5 file.\n",
    "        mask_dir (str): Directory containing the binary mask TIFF files.\n",
    "        output_csv_path (str): Path for saving the output CSV.\n",
    "        bodypart_names (list): If None, the code will infer from HDF5 dataset names.\n",
    "        x_suffix (str): Suffix for x coordinates (default \"_x\").\n",
    "        y_suffix (str): Suffix for y coordinates (default \"_y\").\n",
    "        \n",
    "        outlier_detection (bool): Whether to do outlier detection by large jumps.\n",
    "        distance_threshold (float or None): If float, fixed threshold in pixels. If None, auto-calc from data.\n",
    "        robust (bool): If True and threshold is None, use median+MAD approach; else use mean+std.\n",
    "        multiplier (float): Multiplier for outlier detection threshold.\n",
    "        \n",
    "        interpolation_method (str): Pandas interpolation method (e.g., 'linear', 'time', 'polynomial', etc.).\n",
    "        rolling_window (int): Window size for rolling average smoothing.\n",
    "    \"\"\"\n",
    "    # 1. Load SLEAP HDF5\n",
    "    with h5py.File(sleap_h5_path, 'r') as f:\n",
    "        tracks = f['tracks'][:]\n",
    "        bodypart_names = bodypart_names or list(f['node_names'][:])\n",
    "        bodypart_names = [name.decode('utf-8') for name in bodypart_names]\n",
    "        \n",
    "        data = {name: f['tracks'][:, :, i] for i, name in enumerate(bodypart_names)}\n",
    "        sleap_df = pd.DataFrame(data)\n",
    "    \n",
    "    # 2. Outlier detection & removal\n",
    "    if outlier_detection:\n",
    "        sleap_df = detect_outliers_by_jump(\n",
    "            sleap_df, \n",
    "            bodypart_names, \n",
    "            x_suffix, \n",
    "            y_suffix,\n",
    "            distance_threshold=distance_threshold,\n",
    "            robust=robust, \n",
    "            multiplier=multiplier\n",
    "        )\n",
    "\n",
    "    # 3. Interpolate & smooth\n",
    "    sleap_df = interpolate_and_smooth_coordinates(\n",
    "        sleap_df, \n",
    "        bodypart_names, \n",
    "        x_suffix, \n",
    "        y_suffix,\n",
    "        interpolation_method=interpolation_method, \n",
    "        rolling_window=rolling_window\n",
    "    )\n",
    "\n",
    "    # 4. Load binary masks\n",
    "    masks = load_masks_from_directory(mask_dir)\n",
    "    # We assume consistent shape\n",
    "    mask_shape = None\n",
    "    if len(masks) > 0:\n",
    "        first_key = next(iter(masks.keys()))\n",
    "        mask_shape = masks[first_key].shape\n",
    "\n",
    "    # 5. Map each body part to subcomponent\n",
    "    output_df = pd.DataFrame(index=sleap_df.index, columns=bodypart_names, dtype=object)\n",
    "\n",
    "    for i, row_data in sleap_df.iterrows():\n",
    "        for bp in bodypart_names:\n",
    "            x_col = f\"{bp}{x_suffix}\"\n",
    "            y_col = f\"{bp}{y_suffix}\"\n",
    "\n",
    "            if x_col not in sleap_df.columns or y_col not in sleap_df.columns:\n",
    "                output_df.at[i, bp] = \"None\"\n",
    "                continue\n",
    "\n",
    "            x_coord = row_data[x_col]\n",
    "            y_coord = row_data[y_col]\n",
    "\n",
    "            if pd.isna(x_coord) or pd.isna(y_coord):\n",
    "                output_df.at[i, bp] = \"None\"\n",
    "                continue\n",
    "            \n",
    "            x_idx = int(round(x_coord))\n",
    "            y_idx = int(round(y_coord))\n",
    "\n",
    "            # Check bounds\n",
    "            if mask_shape is not None:\n",
    "                h, w = mask_shape\n",
    "                if x_idx < 0 or x_idx >= w or y_idx < 0 or y_idx >= h:\n",
    "                    output_df.at[i, bp] = \"None\"\n",
    "                    continue\n",
    "\n",
    "            # Identify which mask is True\n",
    "            found_subcomponent = False\n",
    "            for subcomp_name, mask_arr in masks.items():\n",
    "                if mask_arr[y_idx, x_idx]:\n",
    "                    output_df.at[i, bp] = subcomp_name\n",
    "                    found_subcomponent = True\n",
    "                    break\n",
    "            if not found_subcomponent:\n",
    "                output_df.at[i, bp] = \"None\"\n",
    "\n",
    "    # 6. Save final output\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Output saved to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code below is for checking missing frames from sleap outputs. CSV vs hdf5 formats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder = \"/Users/AdamHarris/Desktop/cohort_10_exploration/maze1\"\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"{filename}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "vids_paths = '/Users/AdamHarris/Desktop/exploration_videos'\n",
    "folder = \"/Users/AdamHarris/Desktop/cohort_10_exploration/maze1\"\n",
    "folder2 = \"/Users/AdamHarris/Desktop/cohort_10_exploration/maze2\"\n",
    "def get_video_frame_count(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "    return frame_count\n",
    "\n",
    "def format_missing_frames(missing_frames):\n",
    "    sorted_frames = sorted(missing_frames)\n",
    "    ranges = []\n",
    "    start = end = sorted_frames[0]\n",
    "\n",
    "    for frame in sorted_frames[1:]:\n",
    "        if frame == end + 1:\n",
    "            end = frame\n",
    "        else:\n",
    "            if start == end:\n",
    "                ranges.append(f\"{start}\")\n",
    "            else:\n",
    "                ranges.append(f\"{start}-{end}\")\n",
    "            start = end = frame\n",
    "\n",
    "    if start == end:\n",
    "        ranges.append(f\"{start}\")\n",
    "    else:\n",
    "        ranges.append(f\"{start}-{end}\")\n",
    "\n",
    "    return \", \".join(ranges)\n",
    "\n",
    "for folder in [folder, folder2]:\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Extract mouse ID and datetime string from filename\n",
    "            parts = filename.split('_')\n",
    "            mouse_id = parts[1]\n",
    "            datetime_str = parts[2].split('.')[0]\n",
    "            \n",
    "            # Find the corresponding video file\n",
    "            video_filename = f\"{mouse_id}_{datetime_str}.mp4\"\n",
    "            video_path = os.path.join(vids_paths, video_filename)\n",
    "            \n",
    "            if os.path.exists(video_path):\n",
    "                total_frames = get_video_frame_count(video_path)\n",
    "                print(f\"{filename}: {len(df)} rows, {total_frames} frames in video | {total_frames - len(df)} frames missing.  {(total_frames - len(df))/60:.2f} seconds\")\n",
    "\n",
    "                # Check for missing frames\n",
    "                csv_frames = set(df['frame_idx'])\n",
    "                video_frames = set(range(total_frames))\n",
    "                missing_frames = video_frames - csv_frames\n",
    "                \n",
    "                if missing_frames:\n",
    "                    formatted_missing_frames = format_missing_frames(missing_frames)\n",
    "                    print(f\"Missing frames in {filename}: {formatted_missing_frames}\")\n",
    "                else:\n",
    "                    print(f\"No missing frames in {filename}\")\n",
    "            else:\n",
    "                print(f\"Video file not found for {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = h5py.File(h5_path, 'r')\n",
    "xx['tracks'][0,0,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "comparisons_dir = \"/Users/AdamHarris/Desktop/bp01_c7_h5_csv\"\n",
    "comparisons_dir = \"/Users/AdamHarris/Desktop/openfield_sleap\"\n",
    "comparisons_dir = \"/Users/AdamHarris/Desktop/objectvector_sleap\"\n",
    "\n",
    "for filename in os.listdir(comparisons_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_path = os.path.join(comparisons_dir, filename)\n",
    "        h5_path = os.path.join(comparisons_dir, filename.replace(\".csv\", \".h5\"))\n",
    "\n",
    "        if os.path.exists(h5_path):\n",
    "            # Load CSV\n",
    "            csv_df = pd.read_csv(csv_path)\n",
    "            csv_count = len(csv_df)\n",
    "\n",
    "            # Load H5\n",
    "            with h5py.File(h5_path, 'r') as h5_file:\n",
    "                h5_count = len(h5_file['tracks'][0,0,0,:])\n",
    "\n",
    "            # Print the counts and the difference\n",
    "            print(f\"{filename}: CSV entries = {csv_count}, H5 entries = {h5_count}, Difference = {(h5_count-csv_count)/60} seconds\")\n",
    "        else:\n",
    "            print(f\"H5 file not found for {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(h5_path, 'r') as h5_file:\n",
    "    h5_count = len(h5_file['tracks'][0,0,0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = h5py.File(h5_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_coords_df = pd.read_csv('/Users/AdamHarris/Desktop/cohort_10_exploration/processed/c1m3_2025-03-12-121432.analysis_cleaned_coordinates.csv')\n",
    "test_coords_x, test_coords_y = np.array(test_coords_df['head_back.x']), np.array(test_coords_df['head_back.y'])\n",
    "test_coords = np.column_stack((test_coords_x, test_coords_y))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_coords[:,0], test_coords[:,1], '-', linewidth=0.2, c='grey')\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "test_coords_df = pd.read_csv('/Users/AdamHarris/Desktop/cohort_10_exploration/processed/c1m4_2025-03-12-121432.analysis_cleaned_coordinates.csv')\n",
    "test_coords_x, test_coords_y = np.array(test_coords_df['head_back.x']), np.array(test_coords_df['head_back.y'])\n",
    "test_coords = np.column_stack((test_coords_x, test_coords_y))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_coords[:,0], test_coords[:,1], '-', linewidth=0.2, c='grey')\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_coords_df = pd.read_csv('/Users/AdamHarris/Desktop/mEC_SLEAP/outputs/bp01_2024-03-21-125921.analysis_cleaned_coordinates.csv')\n",
    "test_coords_x, test_coords_y = np.array(test_coords_df['head_back.x']), np.array(test_coords_df['head_back.y'])\n",
    "test_coords = np.column_stack((test_coords_x, test_coords_y))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_coords[:,0], test_coords[:,1], '-', linewidth=0.1, c='grey')\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_coords_df = pd.read_csv(\"/Users/AdamHarris/Desktop/analysis/labels.v004.000_bp01_2024-03-17-140534.analysis.csv\")\n",
    "test_coords_x, test_coords_y = np.array(test_coords_df['head_back.x']), np.array(test_coords_df['head_back.y'])\n",
    "test_coords = np.column_stack((test_coords_x, test_coords_y))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_coords[:,0], test_coords[:,1], '-', linewidth=0.1, c='grey')\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_coords_df = pd.read_csv('/Users/AdamHarris/Desktop/analysis 2/labels.v003.000_bp01_2024-03-15-144543.analysis.csv')\n",
    "test_coords_x, test_coords_y = np.array(test_coords_df['head_back.x']), np.array(test_coords_df['head_back.y'])\n",
    "test_coords = np.column_stack((test_coords_x, test_coords_y))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_coords[:,0], test_coords[:,1], '-', linewidth=0.1, c='grey')\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_coords[:,0], test_coords[:,1], '-', linewidth=0.1, c='grey')\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def calibrate_and_correct(\n",
    "    pixel_points: np.ndarray,\n",
    "    world_points: np.ndarray,\n",
    "    image_size: tuple,\n",
    "    flags: int = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calibrate camera for radial (and possibly tangential) distortion\n",
    "    based on 2D correspondences, then return camera matrix and distortion coefficients.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pixel_points : np.ndarray\n",
    "        Nx2 array of pixel coordinates (image points).\n",
    "    world_points : np.ndarray\n",
    "        Nx2 array of corresponding physical coordinates (in real-world space, 2D).\n",
    "    image_size : tuple\n",
    "        (width, height) of the image in pixels.\n",
    "    flags : int, optional\n",
    "        OpenCV flags to refine calibration method. \n",
    "        E.g. cv2.CALIB_FIX_ASPECT_RATIO, etc.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    camera_matrix : np.ndarray\n",
    "        The computed camera intrinsic matrix.\n",
    "    dist_coeffs : np.ndarray\n",
    "        Distortion coefficients (k1, k2, p1, p2, k3, ...).\n",
    "    rvecs : list\n",
    "        Rotation vectors from the calibration routine.\n",
    "    tvecs : list\n",
    "        Translation vectors from the calibration routine.\n",
    "    \"\"\"\n",
    "    # We need these in the format (N, 1, 2) for cv2.calibrateCamera\n",
    "    pixel_points_reshaped = pixel_points.reshape(-1, 1, 2).astype(np.float32)\n",
    "    world_points_reshaped = world_points.reshape(-1, 1, 2).astype(np.float32)\n",
    "    \n",
    "    # Because we're doing a planar approximation, we can treat the real-world points as if z=0\n",
    "    # However, OpenCV calibrateCamera expects 3D object points for the real-world coords.\n",
    "    # So let's just embed them in 3D as (x, y, 0).\n",
    "    object_points_3d = np.hstack([world_points, np.zeros((world_points.shape[0], 1))])\n",
    "    object_points_3d = object_points_3d.reshape(-1, 1, 3).astype(np.float32)\n",
    "    \n",
    "    # We must pass a list of arrays: each array is the correspondences for a single view.\n",
    "    # If we have only one set of correspondences, it's like we have 1 \"image\" of that plane.\n",
    "    object_points_list = [object_points_3d]\n",
    "    image_points_list = [pixel_points_reshaped]\n",
    "    \n",
    "    # Initialize a guess for the camera matrix\n",
    "    # fx ~ fy ~ some fraction of image size, and principal point near the center\n",
    "    focal_length = 0.8 * image_size[0]\n",
    "    cx, cy = image_size[0] / 2.0, image_size[1] / 2.0\n",
    "    camera_matrix_init = np.array([[focal_length, 0, cx],\n",
    "                                   [0, focal_length, cy],\n",
    "                                   [0,           0,  1]], dtype=np.float32)\n",
    "    \n",
    "    dist_coeffs_init = np.zeros(5)  # [k1, k2, p1, p2, k3] etc.\n",
    "    \n",
    "    if flags is None:\n",
    "        # Default to something that tries to solve k1, k2, p1, p2, k3\n",
    "        flags = 0\n",
    "\n",
    "    # calibrateCamera returns:\n",
    "    #   rms_error, camera_matrix, dist_coeffs, rvecs, tvecs\n",
    "    rms, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(\n",
    "        object_points_list,\n",
    "        image_points_list,\n",
    "        image_size,\n",
    "        camera_matrix_init,\n",
    "        dist_coeffs_init,\n",
    "        flags=flags\n",
    "    )\n",
    "    \n",
    "    print(f\"Calibration RMS re-projection error: {rms:.4f}\")\n",
    "    print(\"Camera Matrix:\\n\", camera_matrix)\n",
    "    print(\"Distortion Coeffs:\\n\", dist_coeffs.ravel())\n",
    "    \n",
    "    return camera_matrix, dist_coeffs, rvecs, tvecs\n",
    "\n",
    "def undistort_points(\n",
    "    pixel_points: np.ndarray,\n",
    "    camera_matrix: np.ndarray,\n",
    "    dist_coeffs: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Undistort pixel points given the camera matrix and distortion coefficients.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pixel_points : np.ndarray\n",
    "        Nx2 array of pixel coordinates.\n",
    "    camera_matrix : np.ndarray\n",
    "        3x3 intrinsic camera matrix.\n",
    "    dist_coeffs : np.ndarray\n",
    "        Distortion coefficients.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    undistorted_points : np.ndarray\n",
    "        Nx2 array of undistorted (corrected) pixel coordinates.\n",
    "    \"\"\"\n",
    "    # reshape for OpenCV\n",
    "    points_reshaped = pixel_points.reshape(-1, 1, 2).astype(np.float32)\n",
    "    \n",
    "    # Undistort\n",
    "    undistorted = cv2.undistortPoints(points_reshaped, camera_matrix, dist_coeffs, P=camera_matrix)\n",
    "    \n",
    "    # undistorted returns Nx1x2\n",
    "    undistorted = undistorted.reshape(-1, 2)\n",
    "    return undistorted\n",
    "\n",
    "def find_planar_homography(\n",
    "    pixel_points: np.ndarray,\n",
    "    world_points: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimate a 3x3 homography matrix mapping 2D pixel space to 2D physical world space.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pixel_points : np.ndarray\n",
    "        Nx2 array of (x, y) pixel coordinates.\n",
    "    world_points : np.ndarray\n",
    "        Nx2 array of (x, y) physical coordinates in the real world.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    H : np.ndarray\n",
    "        3x3 homography matrix that maps from pixel to physical coordinates.\n",
    "    \"\"\"\n",
    "    # Need float32 for OpenCV\n",
    "    pixel_points_32 = pixel_points.reshape(-1, 1, 2).astype(np.float32)\n",
    "    world_points_32 = world_points.reshape(-1, 1, 2).astype(np.float32)\n",
    "    \n",
    "    H, mask = cv2.findHomography(pixel_points_32, world_points_32, cv2.RANSAC, 5.0)\n",
    "    \n",
    "    return H\n",
    "\n",
    "def transform_points_homography(\n",
    "    pixel_points: np.ndarray,\n",
    "    H: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply a 3x3 homography transform to a set of 2D pixel points.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pixel_points : np.ndarray\n",
    "        Nx2 array of pixel coordinates.\n",
    "    H : np.ndarray\n",
    "        3x3 homography matrix.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    physical_points : np.ndarray\n",
    "        Nx2 array of physical coordinates.\n",
    "    \"\"\"\n",
    "    # Convert to homogeneous coordinates: (x, y, 1)\n",
    "    n = pixel_points.shape[0]\n",
    "    ones = np.ones((n, 1))\n",
    "    pixel_points_h = np.hstack([pixel_points, ones])\n",
    "    \n",
    "    # Transform\n",
    "    transformed = (H @ pixel_points_h.T).T  # shape Nx3\n",
    "    \n",
    "    # Convert back from homogeneous\n",
    "    transformed_xy = transformed[:, :2] / transformed[:, 2][:, np.newaxis]\n",
    "    return transformed_xy\n",
    "\n",
    "\n",
    "def transform_sleap_dataframe_inplace_radialdistortion(df,  camera_matrix, dist_coeffs, pixel_points, world_points):\n",
    "    \"\"\"\n",
    "    For each body-part column pair that looks like <part>.x, <part>.y\n",
    "    in DataFrame df, transform those pixel coordinates in-place to physical (cm).\n",
    "    i.e., we overwrite the .x, .y columns with the new values.\n",
    "    \n",
    "    df: Pandas DataFrame with columns like 'nose.x', 'nose.y', ...\n",
    "    camera_matrix, dist_coeffs: camera calibration parameters\n",
    "    \"\"\"\n",
    "    # Find all columns ending with \".x\".\n",
    "    x_cols = [col for col in df.columns if col.endswith('.x')]\n",
    "    \n",
    "    for x_col in x_cols:\n",
    "        # Replace '.x' with '.y' to get the matching y column\n",
    "        y_col = x_col[:-2] + '.y'\n",
    "        if y_col not in df.columns:\n",
    "            # If the matching .y doesn't exist, skip\n",
    "            continue\n",
    "        \n",
    "        # Extract Nx2 array of pixel coordinates\n",
    "        pixel_coords = df[[x_col, y_col]].values\n",
    "        \n",
    "        # Transform\n",
    "        undistorted_pixel_points = undistort_points(pixel_coords, camera_matrix, dist_coeffs)\n",
    "        H = find_planar_homography(pixel_points, world_points)\n",
    "        \n",
    "        # Now test the transformation on the same points:\n",
    "        predicted_world_points = transform_points_homography(undistorted_pixel_points, H)\n",
    "        \n",
    "        # Overwrite the DataFrame columns in-place\n",
    "        df[x_col] = predicted_world_points[:, 0]\n",
    "        df[y_col] = predicted_world_points[:, 1]\n",
    "    \n",
    "    # Return df (not strict\n",
    "    #ly necessary if we modify in-place)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_openfield(\n",
    "    sleap_csv_path,\n",
    "    # Additional outputs\n",
    "    cleaned_coords_csv_path=\"cleaned_coordinates.csv\",\n",
    "    head_direction_csv_path=\"head_direction.csv\",\n",
    "    # Bodypart parsing\n",
    "    bodypart_names=None,\n",
    "    x_suffix=\".x\",\n",
    "    y_suffix=\".y\",\n",
    "    # Outlier detection params\n",
    "    outlier_detection=True,\n",
    "    distance_threshold=None,\n",
    "    robust=True,\n",
    "    multiplier=3.0,\n",
    "    transform_to_physical=False,\n",
    "    pixel_pts=None,\n",
    "    physical_pts=None,\n",
    "    # Interpolation/smoothing params\n",
    "    interpolation_method='linear',\n",
    "    rolling_window=5\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Load SLEAP CSV\n",
    "    2) Optionally do outlier detection\n",
    "    3) Interpolate + smooth\n",
    "    4) Save cleaned coordinates to CSV\n",
    "    5) Compute head direction from (head_mid, head_back, ear_L, ear_R), save to CSV\n",
    "\n",
    "    Args:\n",
    "        sleap_csv_path: path to the original SLEAP CSV\n",
    "        cleaned_coords_csv_path: path for saving the post-processed (x,y) CSV\n",
    "        head_direction_csv_path: path for saving the head orientation CSV\n",
    "        bodypart_names: if None, infer from columns. Otherwise, list of bodypart strings\n",
    "        x_suffix, y_suffix: how the SLEAP CSV columns are named (default \".x\"/\".y\")\n",
    "        outlier_detection: whether to remove large jumps\n",
    "        distance_threshold: pixel threshold for outliers (or None to auto-calc)\n",
    "        robust: if True and threshold is None, use median+MAD; else mean+std\n",
    "        multiplier: factor for outlier detection\n",
    "        interpolation_method: method for pd.DataFrame.interpolate (e.g. 'linear')\n",
    "        rolling_window: window size for rolling-average smoothing\n",
    "    \"\"\"\n",
    "    # 1) Load SLEAP CSV\n",
    "    sleap_df = pd.read_csv(sleap_csv_path)\n",
    "\n",
    "    \n",
    "    # 2) Identify bodypart names\n",
    "    if bodypart_names is None:\n",
    "        x_cols = [col for col in sleap_df.columns if col.endswith(x_suffix)]\n",
    "        bodypart_names = [col.replace(x_suffix, \"\") for col in x_cols]\n",
    "\n",
    "    # 3) Outlier detection (optional)\n",
    "    if outlier_detection:\n",
    "        sleap_df = detect_outliers_by_jump(\n",
    "            sleap_df, \n",
    "            bodypart_names,\n",
    "            x_suffix,\n",
    "            y_suffix,\n",
    "            distance_threshold=distance_threshold,\n",
    "            robust=robust,\n",
    "            multiplier=multiplier\n",
    "        )\n",
    "\n",
    "    # 4) Interpolate and smooth\n",
    "    sleap_df = interpolate_and_smooth_coordinates(\n",
    "        sleap_df,\n",
    "        bodypart_names,\n",
    "        x_suffix,\n",
    "        y_suffix,\n",
    "        interpolation_method=interpolation_method,\n",
    "        rolling_window=rolling_window\n",
    "    )\n",
    "\n",
    "    # 5 compute affine transform to put into physical coordinates\n",
    "    if transform_to_physical:\n",
    "        \n",
    "        camera_matrix, dist_coeffs, rvecs, tvecs = calibrate_and_correct(\n",
    "        pixel_points=pixel_pts,\n",
    "        world_points=physical_pts,\n",
    "        image_size=image_size\n",
    "        )\n",
    "    \n",
    "        sleap_df = transform_sleap_dataframe_inplace_radialdistortion(sleap_df, camera_matrix, dist_coeffs, pixel_pts, physical_pts)\n",
    "            \n",
    "\n",
    "    # ---- (A) SAVE the cleaned coordinates to CSV\n",
    "    sleap_df.to_csv(cleaned_coords_csv_path, index=False)\n",
    "    print(f\"Cleaned coordinates saved to: {cleaned_coords_csv_path}\")\n",
    "\n",
    "    # 5) Compute head direction angles from head_mid, head_back, ear_L, ear_R\n",
    "    head_directions_df = compute_head_directions(sleap_df, x_suffix, y_suffix)\n",
    "    # Save to CSV\n",
    "    head_directions_df.to_csv(head_direction_csv_path, index=False)\n",
    "    print(f\"Head direction angles saved to: {head_direction_csv_path}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_body_parts_to_subcomponent_h5(sleap_h5_path, \n",
    "                                   mask_dir, \n",
    "                                   bodypart_names=['head_back'], \n",
    "                                   x_suffix=\"_x\", \n",
    "                                   y_suffix=\"_y\",\n",
    "                                   # Outlier detection parameters\n",
    "                                   outlier_detection=True,\n",
    "                                   distance_threshold=None,\n",
    "                                   robust=True,\n",
    "                                   multiplier=3.0,\n",
    "                                   # Interpolation & smoothing parameters\n",
    "                                   interpolation_method='linear',\n",
    "                                   rolling_window=5):\n",
    "    \"\"\"\n",
    "        1) Loads SLEAP HDF5 of body part coordinates\n",
    "        2) Optionally detects & removes large jump outliers\n",
    "        3) Interpolates + smooths\n",
    "        4) Loads TIFF binary masks\n",
    "        5) Maps each body part's (x,y) to a subcomponent\n",
    "        6) Exports the final mapping in a CSV\n",
    "        \n",
    "        Args:\n",
    "            sleap_h5_path (str): Path to the SLEAP HDF5 file.\n",
    "            mask_dir (str): Directory containing the binary mask TIFF files.\n",
    "            output_csv_path (str): Path for saving the output CSV.\n",
    "            bodypart_names (list): If None, the code will infer from HDF5 dataset names.\n",
    "            x_suffix (str): Suffix for x coordinates (default \"_x\").\n",
    "            y_suffix (str): Suffix for y coordinates (default \"_y\").\n",
    "            \n",
    "            outlier_detection (bool): Whether to do outlier detection by large jumps.\n",
    "            distance_threshold (float or None): If float, fixed threshold in pixels. If None, auto-calc from data.\n",
    "            robust (bool): If True and threshold is None, use median+MAD approach; else use mean+std.\n",
    "            multiplier (float): Multiplier for outlier detection threshold.\n",
    "            \n",
    "            interpolation_method (str): Pandas interpolation method (e.g., 'linear', 'time', 'polynomial', etc.).\n",
    "            rolling_window (int): Window size for rolling average smoothing.\n",
    "    \"\"\"\n",
    "        # 1. Load SLEAP HDF5\n",
    "    with h5py.File(sleap_h5_path, 'r') as f:\n",
    "        tracks = f['tracks'][:]\n",
    "        bodypart_names = bodypart_names or list(f['node_names'][:])\n",
    "\n",
    "\n",
    "        ##############\n",
    "        bodypart_names_h5 = [name if isinstance(name, str) else name.decode('utf-8') for name in list(f['node_names'][:])]\n",
    "        \n",
    "        if len(bodypart_names) == 1:\n",
    "            # Find the index in f['node_names'] where the bodypart name appears\n",
    "\n",
    "            bodypart_ind = np.where(np.array(bodypart_names_h5) == bodypart_names[0])[0][0]\n",
    "            \n",
    "      \n",
    "\n",
    "            data = {bodypart_names[0]: tracks[0, :, bodypart_ind, :]}\n",
    "            \n",
    "            data_dict = {}\n",
    "            data_dict[f\"{bodypart_names[0]}{x_suffix}\"] = tracks[0, 0, bodypart_ind, :]\n",
    "            data_dict[f\"{bodypart_names[0]}{y_suffix}\"] = tracks[0, 1, bodypart_ind, :]\n",
    "        else:\n",
    "            data = {name: tracks[0, :, i, :] for i, name in enumerate(bodypart_names)}\n",
    "            data_dict = {}\n",
    "            for i, bp in enumerate(bodypart_names):\n",
    "                data_dict[f\"{bp}{x_suffix}\"] = tracks[0, 0, i, :]\n",
    "                data_dict[f\"{bp}{y_suffix}\"] = tracks[0, 1, i, :]\n",
    " \n",
    "        sleap_df = pd.DataFrame(data_dict)\n",
    "\n",
    "    # 2. Outlier detection & removal\n",
    "    if outlier_detection:\n",
    "        sleap_df = detect_outliers_by_jump(\n",
    "            sleap_df, \n",
    "            bodypart_names, \n",
    "            x_suffix, \n",
    "            y_suffix,\n",
    "            distance_threshold=distance_threshold,\n",
    "            robust=robust, \n",
    "            multiplier=multiplier\n",
    "        )\n",
    "\n",
    "    # 3. Interpolate & smooth\n",
    "    sleap_df = interpolate_and_smooth_coordinates(\n",
    "        sleap_df, \n",
    "        bodypart_names, \n",
    "        x_suffix, \n",
    "        y_suffix,\n",
    "        interpolation_method=interpolation_method, \n",
    "        rolling_window=rolling_window\n",
    "    )\n",
    "\n",
    "    # 4. Load binary masks\n",
    "    masks = load_masks_from_directory(mask_dir)\n",
    "    # We assume consistent shape\n",
    "    mask_shape = None\n",
    "    if len(masks) > 0:\n",
    "        first_key = next(iter(masks.keys()))\n",
    "        mask_shape = masks[first_key].shape\n",
    "\n",
    "    # 5. Map each body part to subcomponent\n",
    "    output_df = pd.DataFrame(index=sleap_df.index, columns=bodypart_names, dtype=object)\n",
    "\n",
    "    for i, row_data in sleap_df.iterrows():\n",
    "        for bp in ['head_back']:\n",
    "            x_col = f\"{bp}{x_suffix}\"\n",
    "            y_col = f\"{bp}{y_suffix}\"\n",
    "\n",
    "            if x_col not in sleap_df.columns or y_col not in sleap_df.columns:\n",
    "                output_df.at[i, bp] = \"None\"\n",
    "                continue\n",
    "\n",
    "            x_coord = row_data[x_col]\n",
    "            y_coord = row_data[y_col]\n",
    "\n",
    "            if pd.isna(x_coord) or pd.isna(y_coord):\n",
    "                output_df.at[i, bp] = \"None\"\n",
    "                continue\n",
    "            \n",
    "            x_idx = int(round(x_coord))\n",
    "            y_idx = int(round(y_coord))\n",
    "\n",
    "            # Check bounds\n",
    "            if mask_shape is not None:\n",
    "                h, w = mask_shape\n",
    "                if x_idx < 0 or x_idx >= w or y_idx < 0 or y_idx >= h:\n",
    "                    output_df.at[i, bp] = \"None\"\n",
    "                    continue\n",
    "\n",
    "            # Identify which mask is True\n",
    "            found_subcomponent = False\n",
    "            for subcomp_name, mask_arr in masks.items():\n",
    "                if mask_arr[y_idx, x_idx]:\n",
    "                    output_df.at[i, bp] = subcomp_name\n",
    "                    found_subcomponent = True\n",
    "                    break\n",
    "            if not found_subcomponent:\n",
    "                output_df.at[i, bp] = \"None\"\n",
    "\n",
    "    # 6. Save final output\n",
    "    return output_df['head_back']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #------------------------------------------------------------------\n",
    "    # 1) Example: Suppose we have measured correspondences in (x,y)\n",
    "    #    in pixels, and we also have the corresponding real-world\n",
    "    #    coordinates in some length unit (e.g. mm).\n",
    "    #------------------------------------------------------------------\n",
    "    \n",
    "    #import ast\n",
    "    \n",
    "    openfield_registration = pd.read_csv(\"/Users/AdamHarris/Desktop/maze_registration.htsv\", sep='\\t')\n",
    "\n",
    "    pixel_coords_of = np.array([list(x) for x in openfield_registration[\"pixel_coords\"].apply(ast.literal_eval)])\n",
    "    physical_coords_of = np.array([list(x) for x in openfield_registration[\"physical_coords\"].apply(ast.literal_eval)])\n",
    "\n",
    "    # The size of the camera image in pixels (width, height).\n",
    "    image_size = (1280, 1024)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    data_folder = r'/Users/AdamHarris/Desktop/big_maze_csvs/CSVs'\n",
    "\n",
    "    for filename in tqdm(os.listdir(data_folder)):\n",
    "\n",
    "        sleap_csv = os.path.join(data_folder, filename)\n",
    "\n",
    "     \n",
    "\n",
    "        process_openfield(\n",
    "            sleap_csv_path=sleap_csv,\n",
    "            cleaned_coords_csv_path=f\"/Users/AdamHarris/Desktop/big_maze_csvs/outputs/{filename[16:-4]}_cleaned_coordinates.csv\",\n",
    "            head_direction_csv_path=f\"/Users/AdamHarris/Desktop/big_maze_csvs/outputs/{filename[16:-4]}_head_direction.csv\",\n",
    "            bodypart_names=None, #[\"head_mid\", \"head_back\", \"ear_L\", \"ear_R\"],\n",
    "            x_suffix=\".x\",\n",
    "            y_suffix=\".y\",\n",
    "            outlier_detection=True,\n",
    "            distance_threshold=None,   # auto-compute\n",
    "            robust=True,              # median+MAD approach\n",
    "            multiplier=4.0,\n",
    "            transform_to_physical=True,\n",
    "            pixel_pts=pixel_coords_of,\n",
    "            physical_pts=physical_coords_of,\n",
    "            interpolation_method='linear',\n",
    "            rolling_window=5\n",
    "        )\n",
    "    \n",
    "\n",
    "    #------------------------------------------------------------------\n",
    "    # 2) Calibrate camera: find camera_matrix + dist_coeffs using\n",
    "    #    planar correspondences. This helps remove radial distortion.\n",
    "    #------------------------------------------------------------------\n",
    "    \n",
    "    #------------------------------------------------------------------\n",
    "    # 5) Finally, for new SLEAP-labeled points (pixel coords),\n",
    "    #    you would do:\n",
    "    #       new_undistorted = undistort_points(new_pixel_points, ...)\n",
    "    #       new_physical    = transform_points_homography(new_undistorted, H)\n",
    "    #    to get them in physical (distortion-corrected) coordinates.\n",
    "    #------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = pd.read_csv('/Users/AdamHarris/Desktop/big_maze_csvs/outputs/bp01_2024-03-26-191116.analysis_cleaned_coordinates.csv')\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xxx['head_back.x'], xxx['head_back.y'], '-', linewidth=0.1, c='grey')\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = pd.read_csv('/Users/AdamHarris/Desktop/big_maze_csvs/CSVs/labels.v004.004_bp01_2024-03-26-191116.analysis.csv')\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xxx['head_back.x'], xxx['head_back.y'], '-', linewidth=0.1, c='grey')\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = pd.read_csv('/Users/AdamHarris/Desktop/big_maze_csvs/outputs/bp01_2024-03-21-181423.analysis_cleaned_coordinates.csv')\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xxx['head_back.x'], xxx['head_back.y'], '-', linewidth=0.1, c='grey')\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = pd.read_csv('/Users/AdamHarris/Desktop/big_maze_csvs/CSVs/labels.v003.001_bp01_2024-03-21-181423.analysis.csv')\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xxx['head_back.x'], xxx['head_back.y'], '-', linewidth=0.1, c='grey')\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
