{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import re\n",
    "from datetime import datetime, date\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r'/Users/AdamHarris/Documents/Behaviour/cohort10'\n",
    "\n",
    "mouse_to_maze_dict = {'ah08': 2, \n",
    "                        'ah09': 2, \n",
    "                        'ah10': 1, \n",
    "                        'ly05': 1, \n",
    "                        'ly06': 2,\n",
    "                        'ly07': 1}\n",
    "\n",
    "maze_1_pixel_pts = np.array([[1022, 974],\n",
    "                            [981, 136],\n",
    "                            [158, 169],\n",
    "                            [172, 1010]])\n",
    "\n",
    "maze_2_pixel_pts = np.array([[234, 945],\n",
    "                            [1036, 953],\n",
    "                            [1020, 158],\n",
    "                            [248, 169]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_masks_from_directory(mask_dir):\n",
    "    \"\"\"\n",
    "    Load all TIFF binary mask images from a directory. \n",
    "    Assumes each mask file is named like 'node_1.tif', 'edge_1-2.tif', etc.\n",
    "    Returns a dictionary: \n",
    "        key = subcomponent name (e.g. 'node_1'), \n",
    "        value = 2D numpy array for the binary mask.\n",
    "    \"\"\"\n",
    "    mask_dict = {}\n",
    "    tiff_files = glob.glob(os.path.join(mask_dir, \"*.tif*\"))\n",
    "\n",
    "    for tiff_path in tiff_files:\n",
    "        filename = os.path.basename(tiff_path)\n",
    "        # Remove file extension to get subcomponent label\n",
    "        subcomponent_name, _ = os.path.splitext(filename)\n",
    "        \n",
    "        # Read the binary mask as a 2D numpy array\n",
    "        mask_array = tifffile.imread(tiff_path)\n",
    "        \n",
    "        # Ensure boolean type (True/False)\n",
    "        mask_array = mask_array.astype(bool)\n",
    "        \n",
    "        mask_dict[subcomponent_name] = mask_array\n",
    "    \n",
    "    return mask_dict\n",
    "\n",
    "\n",
    "def detect_outliers_by_jump(df, bodypart_names, x_suffix, y_suffix,\n",
    "                            distance_threshold=None,\n",
    "                            robust=True, multiplier=3.0):\n",
    "    \"\"\"\n",
    "    Detect outliers based on large frame-to-frame jumps for each bodypart.\n",
    "    If the distance between consecutive frames exceeds a certain threshold,\n",
    "    mark those coordinates as NaN.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The data containing body part coordinates (e.g. from a SLEAP CSV).\n",
    "    bodypart_names : list of str\n",
    "        List of bodypart labels to process (e.g. [\"nose\", \"tail\"]).\n",
    "    x_suffix : str\n",
    "        Suffix for x-coordinates (e.g. \".x\" or \"_x\").\n",
    "    y_suffix : str\n",
    "        Suffix for y-coordinates.\n",
    "    distance_threshold : float or None\n",
    "        If a float, use this fixed threshold (in pixels).\n",
    "        If None, compute a robust threshold from the distribution of frame-to-frame distances\n",
    "        using median absolute deviation (MAD) or standard deviation, as specified by 'robust'.\n",
    "    robust : bool\n",
    "        If True and distance_threshold is None, use median absolute deviation to set a threshold.\n",
    "        If False and distance_threshold is None, use mean + multiplier * std.\n",
    "    multiplier : float\n",
    "        The multiplier for the robust or standard deviation approach. E.g., 3.0 for 3*MAD or 3*std.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A copy of df where outliers have been marked as NaN.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    for bp in bodypart_names:\n",
    "        x_col = f\"{bp}{x_suffix}\"\n",
    "        y_col = f\"{bp}{y_suffix}\"\n",
    "\n",
    "        if x_col not in df_out.columns or y_col not in df_out.columns:\n",
    "            continue\n",
    "        \n",
    "        x_vals = df_out[x_col].values\n",
    "        y_vals = df_out[y_col].values\n",
    "\n",
    "        # Calculate frame-to-frame distances\n",
    "        # distance[i] = distance between (x[i], y[i]) and (x[i-1], y[i-1])\n",
    "        dx = np.diff(x_vals)\n",
    "        dy = np.diff(y_vals)\n",
    "        dist = np.sqrt(dx**2 + dy**2)\n",
    "        \n",
    "        # If no threshold provided, compute from data\n",
    "        if distance_threshold is None:\n",
    "            # We'll skip NaNs in dist\n",
    "            dist_no_nan = dist[~np.isnan(dist)]\n",
    "            if len(dist_no_nan) == 0:\n",
    "                # no valid distances, skip\n",
    "                continue\n",
    "            if robust:\n",
    "                # Use median + multiplier * MAD\n",
    "                median_dist = np.median(dist_no_nan)\n",
    "                mad = np.median(np.abs(dist_no_nan - median_dist))\n",
    "                # A common robust scale factor for MAD is 1.4826,\n",
    "                # but we can keep it simple or adapt as needed.\n",
    "                if mad == 0:\n",
    "                    # fallback if all distances are the same\n",
    "                    distance_threshold = median_dist * multiplier\n",
    "                else:\n",
    "                    distance_threshold = median_dist + multiplier * mad\n",
    "            else:\n",
    "                # Use mean + multiplier * std\n",
    "                mean_dist = np.mean(dist_no_nan)\n",
    "                std_dist = np.std(dist_no_nan)\n",
    "                distance_threshold = mean_dist + multiplier * std_dist\n",
    "\n",
    "        # Now we have a distance_threshold, let's mark outliers\n",
    "        # dist[i] corresponds to jump from frame i to i+1 in x_vals\n",
    "        # We can mark frame i+1 as NaN if dist[i] is above threshold\n",
    "        outlier_indices = np.where(dist > distance_threshold)[0] + 1  # shift by 1 for the \"destination\" frame\n",
    "\n",
    "        # Mark outliers as NaN\n",
    "        x_vals[outlier_indices] = np.nan\n",
    "        y_vals[outlier_indices] = np.nan\n",
    "\n",
    "        # Save back\n",
    "        df_out[x_col] = x_vals\n",
    "        df_out[y_col] = y_vals\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "def interpolate_and_smooth_coordinates(df, bodypart_names, x_suffix, y_suffix,\n",
    "                                       interpolation_method='linear', \n",
    "                                       rolling_window=5):\n",
    "    \"\"\"\n",
    "    For each bodypart.x and bodypart.y column:\n",
    "      1. Perform NaN interpolation (default 'linear').\n",
    "      2. Smooth data with a rolling mean of specified window size.\n",
    "    \"\"\"\n",
    "    for bp in bodypart_names:\n",
    "        x_col = f\"{bp}{x_suffix}\"\n",
    "        y_col = f\"{bp}{y_suffix}\"\n",
    "\n",
    "        # Skip if columns don't exist\n",
    "        if x_col not in df.columns or y_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Interpolation (fills internal NaNs, can also do 'limit_direction' or 'order' if polynomial, etc.)\n",
    "        df[x_col] = df[x_col].interpolate(method=interpolation_method, limit_direction='both')\n",
    "        df[y_col] = df[y_col].interpolate(method=interpolation_method, limit_direction='both')\n",
    "\n",
    "        # Smoothing with rolling mean\n",
    "        df[x_col] = df[x_col].rolling(window=rolling_window, min_periods=1, center=True).mean()\n",
    "        df[y_col] = df[y_col].rolling(window=rolling_window, min_periods=1, center=True).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "def map_body_parts_to_subcomponent(sleap_h5_path, \n",
    "                                   mask_dir, \n",
    "                                   bodypart_names=['head_back'], \n",
    "                                   x_suffix=\"_x\", \n",
    "                                   y_suffix=\"_y\",\n",
    "                                   # Outlier detection parameters\n",
    "                                   outlier_detection=True,\n",
    "                                   distance_threshold=None,\n",
    "                                   robust=True,\n",
    "                                   multiplier=3.0,\n",
    "                                   # Interpolation & smoothing parameters\n",
    "                                   interpolation_method='linear',\n",
    "                                   rolling_window=5):\n",
    "    \"\"\"\n",
    "        1) Loads SLEAP HDF5 of body part coordinates\n",
    "        2) Optionally detects & removes large jump outliers\n",
    "        3) Interpolates + smooths\n",
    "        4) Loads TIFF binary masks\n",
    "        5) Maps each body part's (x,y) to a subcomponent\n",
    "        6) Exports the final mapping in a CSV\n",
    "        \n",
    "        Args:\n",
    "            sleap_h5_path (str): Path to the SLEAP HDF5 file.\n",
    "            mask_dir (str): Directory containing the binary mask TIFF files.\n",
    "            output_csv_path (str): Path for saving the output CSV.\n",
    "            bodypart_names (list): If None, the code will infer from HDF5 dataset names.\n",
    "            x_suffix (str): Suffix for x coordinates (default \"_x\").\n",
    "            y_suffix (str): Suffix for y coordinates (default \"_y\").\n",
    "            \n",
    "            outlier_detection (bool): Whether to do outlier detection by large jumps.\n",
    "            distance_threshold (float or None): If float, fixed threshold in pixels. If None, auto-calc from data.\n",
    "            robust (bool): If True and threshold is None, use median+MAD approach; else use mean+std.\n",
    "            multiplier (float): Multiplier for outlier detection threshold.\n",
    "            \n",
    "            interpolation_method (str): Pandas interpolation method (e.g., 'linear', 'time', 'polynomial', etc.).\n",
    "            rolling_window (int): Window size for rolling average smoothing.\n",
    "    \"\"\"\n",
    "        # 1. Load SLEAP HDF5\n",
    "    with h5py.File(sleap_h5_path, 'r') as f:\n",
    "        tracks = f['tracks'][:]\n",
    "        bodypart_names = bodypart_names or list(f['node_names'][:])\n",
    "\n",
    "\n",
    "        ##############\n",
    "        bodypart_names_h5 = [name if isinstance(name, str) else name.decode('utf-8') for name in list(f['node_names'][:])]\n",
    "        \n",
    "        if len(bodypart_names) == 1:\n",
    "            # Find the index in f['node_names'] where the bodypart name appears\n",
    "\n",
    "            bodypart_ind = np.where(np.array(bodypart_names_h5) == bodypart_names[0])[0][0]\n",
    "            \n",
    "      \n",
    "\n",
    "            data = {bodypart_names[0]: tracks[0, :, bodypart_ind, :]}\n",
    "            \n",
    "            data_dict = {}\n",
    "            data_dict[f\"{bodypart_names[0]}{x_suffix}\"] = tracks[0, 0, bodypart_ind, :]\n",
    "            data_dict[f\"{bodypart_names[0]}{y_suffix}\"] = tracks[0, 1, bodypart_ind, :]\n",
    "        else:\n",
    "            data = {name: tracks[0, :, i, :] for i, name in enumerate(bodypart_names)}\n",
    "            data_dict = {}\n",
    "            for i, bp in enumerate(bodypart_names):\n",
    "                data_dict[f\"{bp}{x_suffix}\"] = tracks[0, 0, i, :]\n",
    "                data_dict[f\"{bp}{y_suffix}\"] = tracks[0, 1, i, :]\n",
    " \n",
    "        sleap_df = pd.DataFrame(data_dict)\n",
    "\n",
    "    # 2. Outlier detection & removal\n",
    "    if outlier_detection:\n",
    "        sleap_df = detect_outliers_by_jump(\n",
    "            sleap_df, \n",
    "            bodypart_names, \n",
    "            x_suffix, \n",
    "            y_suffix,\n",
    "            distance_threshold=distance_threshold,\n",
    "            robust=robust, \n",
    "            multiplier=multiplier\n",
    "        )\n",
    "\n",
    "    # 3. Interpolate & smooth\n",
    "    sleap_df = interpolate_and_smooth_coordinates(\n",
    "        sleap_df, \n",
    "        bodypart_names, \n",
    "        x_suffix, \n",
    "        y_suffix,\n",
    "        interpolation_method=interpolation_method, \n",
    "        rolling_window=rolling_window\n",
    "    )\n",
    "\n",
    "    # 4. Load binary masks\n",
    "    masks = load_masks_from_directory(mask_dir)\n",
    "    # We assume consistent shape\n",
    "    mask_shape = None\n",
    "    if len(masks) > 0:\n",
    "        first_key = next(iter(masks.keys()))\n",
    "        mask_shape = masks[first_key].shape\n",
    "\n",
    "    # 5. Map each body part to subcomponent\n",
    "    output_df = pd.DataFrame(index=sleap_df.index, columns=bodypart_names, dtype=object)\n",
    "\n",
    "    for i, row_data in sleap_df.iterrows():\n",
    "        for bp in ['head_back']:\n",
    "            x_col = f\"{bp}{x_suffix}\"\n",
    "            y_col = f\"{bp}{y_suffix}\"\n",
    "\n",
    "            if x_col not in sleap_df.columns or y_col not in sleap_df.columns:\n",
    "                output_df.at[i, bp] = \"None\"\n",
    "                continue\n",
    "\n",
    "            x_coord = row_data[x_col]\n",
    "            y_coord = row_data[y_col]\n",
    "\n",
    "            if pd.isna(x_coord) or pd.isna(y_coord):\n",
    "                output_df.at[i, bp] = \"None\"\n",
    "                continue\n",
    "            \n",
    "            x_idx = int(round(x_coord))\n",
    "            y_idx = int(round(y_coord))\n",
    "\n",
    "            # Check bounds\n",
    "            if mask_shape is not None:\n",
    "                h, w = mask_shape\n",
    "                if x_idx < 0 or x_idx >= w or y_idx < 0 or y_idx >= h:\n",
    "                    output_df.at[i, bp] = \"None\"\n",
    "                    continue\n",
    "\n",
    "            # Identify which mask is True\n",
    "            found_subcomponent = False\n",
    "            for subcomp_name, mask_arr in masks.items():\n",
    "                if mask_arr[y_idx, x_idx]:\n",
    "                    output_df.at[i, bp] = subcomp_name\n",
    "                    found_subcomponent = True\n",
    "                    break\n",
    "            if not found_subcomponent:\n",
    "                output_df.at[i, bp] = \"None\"\n",
    "\n",
    "    # 6. Save final output\n",
    "    return output_df['head_back']\n",
    "\n",
    "\n",
    "def get_behaviour_txt(Behaviourfile_path, Structure_abstract=\"ABCD\"):\n",
    "    \"\"\"\n",
    "    Retrieves the pycontrol output file for awake sessions and produces a dictionary \n",
    "    containing the raw pycontrol data.\n",
    "    Parameters:\n",
    "    data_path (str): The base directory path where the data is stored.\n",
    "    mouse (str): The identifier for the mouse.\n",
    "    cohort (str): The cohort number or identifier.\n",
    "    date (str): The date of the session in 'YYYYMMDD' format.\n",
    "    Behaviour_timestamp (str): The timestamp of the behaviour session.\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are event names and values are numpy arrays of event times.\n",
    "    The function performs the following steps:\n",
    "    1. Constructs the file path for the behaviour data file.\n",
    "    2. Reads the file and extracts relevant information.\n",
    "    3. Parses session information including experiment name, task name, subject ID, and start date.\n",
    "    4. Extracts state and event IDs, and session data.\n",
    "    5. Converts subject ID to integer if `int_subject_IDs` is True.\n",
    "    6. Returns a dictionary with event names as keys and numpy arrays of event times as values.\n",
    "    Note:\n",
    "    - The function assumes that the file format and structure are consistent with the expected format.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    # print('Importing data file: '+os.path.split(Behaviourfile_path)[1])\n",
    "\n",
    "    with open(Behaviourfile_path, 'r') as f:\n",
    "        all_lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "\n",
    "    # Extract and store session information.\n",
    "    file_name = os.path.split(Behaviourfile_path)[1]\n",
    "    Event = namedtuple('Event', ['time','name'])\n",
    "\n",
    "    info_lines = [line[2:] for line in all_lines if line[0]=='I']\n",
    "\n",
    "    experiment_name = next(line for line in info_lines if 'Experiment name' in line).split(' : ')[1]\n",
    "    task_name       = next(line for line in info_lines if 'Task name'       in line).split(' : ')[1]\n",
    "    subject_ID_string    = next(line for line in info_lines if 'Subject ID'      in line).split(' : ')[1]\n",
    "    datetime_string      = next(line for line in info_lines if 'Start date'      in line).split(' : ')[1]\n",
    "\n",
    "\n",
    "\n",
    "    datetime_ = datetime.strptime(datetime_string, '%Y/%m/%d %H:%M:%S')\n",
    "    datetime_string = datetime_.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Extract and store session data.\n",
    "\n",
    "    state_IDs = eval(next(line for line in all_lines if line[0]=='S')[2:])\n",
    "    event_IDs = eval(next(line for line in all_lines if line[0]=='E')[2:])\n",
    "    variable_lines = [line[2:] for line in all_lines if line[0]=='V']\n",
    "\n",
    "    if Structure_abstract not in ['ABCD','AB','ABCDA2','ABCDE','ABCAD']:\n",
    "        pass\n",
    "    else:\n",
    "        structurexx = next(line for line in variable_lines if 'active_poke' in line).split(' active_poke ')[1]\n",
    "        if 'ot' in structurexx:\n",
    "            structurex=structurexx[:8]+']'\n",
    "        else:\n",
    "            structurex=structurexx\n",
    "\n",
    "        if Structure_abstract in ['ABCD','AB','ABCDE']:\n",
    "            structure=np.asarray((structurex[1:-1]).split(',')).astype(int)\n",
    "        else:\n",
    "            structure=structurex\n",
    "\n",
    "        ID2name = {v: k for k, v in {**state_IDs, **event_IDs}.items()}\n",
    "        data_lines = [line[2:].split(' ') for line in all_lines if line[0]=='D']\n",
    "        events = [Event(int(dl[0]), ID2name[int(dl[1])]) for dl in data_lines]\n",
    "        times = {event_name: np.array([ev.time for ev in events if ev.name == event_name])  \n",
    "                    for event_name in ID2name.values()}\n",
    "  \n",
    "    return times, variable_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_txt_timestamp(h5_path, mouse):\n",
    "    # Extract the timestamp from the h5 filename\n",
    "    h5_filename = os.path.basename(h5_path)\n",
    "    m = re.search(r'(\\d{4}-\\d{2}-\\d{2}-\\d{6})', h5_filename)\n",
    "    if not m:\n",
    "        raise ValueError(\"No valid timestamp found in the h5 filename.\")\n",
    "    h5_timestamp = datetime.strptime(m.group(1), \"%Y-%m-%d-%H%M%S\")\n",
    "    \n",
    "    # Get the directory and list all txt files that include the mouse identifier\n",
    "    directory = os.path.dirname(h5_path)\n",
    "    txt_files = [f for f in os.listdir(directory) if f.lower().endswith('.txt') and mouse in f]\n",
    "    \n",
    "    nearest_filename = None\n",
    "    min_timedelta = None\n",
    "    \n",
    "    # Try to find a txt file with a timestamp later than or equal to h5 timestamp\n",
    "    for txt_file in txt_files:\n",
    "        m_txt = re.search(r'(\\d{4}-\\d{2}-\\d{2}-\\d{6})', txt_file)\n",
    "        if not m_txt:\n",
    "            continue\n",
    "        txt_timestamp = datetime.strptime(m_txt.group(1), \"%Y-%m-%d-%H%M%S\")\n",
    "        diff = txt_timestamp - h5_timestamp\n",
    "        if diff.total_seconds() >= 0:\n",
    "            if min_timedelta is None or diff < min_timedelta:\n",
    "                min_timedelta = diff\n",
    "                nearest_filename = txt_file\n",
    "\n",
    "    # Fallback: if no txt file is later than the h5 file, choose the one with the smallest absolute difference\n",
    "    if nearest_filename is None:\n",
    "        for txt_file in txt_files:\n",
    "            m_txt = re.search(r'(\\d{4}-\\d{2}-\\d{2}-\\d{6})', txt_file)\n",
    "            if not m_txt:\n",
    "                continue\n",
    "            txt_timestamp = datetime.strptime(m_txt.group(1), \"%Y-%m-%d-%H%M%S\")\n",
    "            diff = abs(txt_timestamp - h5_timestamp)\n",
    "            if min_timedelta is None or diff < min_timedelta:\n",
    "                min_timedelta = diff\n",
    "                nearest_filename = txt_file\n",
    "\n",
    "    if nearest_filename is not None:\n",
    "        return os.path.join(directory, nearest_filename)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pinstate(h5_path, mouse):\n",
    "    # Extract the timestamp from the h5 filename.\n",
    "    h5_filename = os.path.basename(h5_path)\n",
    "    m = re.search(r'(\\d{4}-\\d{2}-\\d{2}-\\d{6})', h5_filename)\n",
    "    if not m:\n",
    "        raise ValueError(\"No valid timestamp found in the h5 filename.\")\n",
    "    h5_timestamp = datetime.strptime(m.group(1), \"%Y-%m-%d-%H%M%S\")\n",
    "    \n",
    "    # Search for CSV files containing 'pinstate' in the filename in the same directory as the h5 file\n",
    "    directory = os.path.dirname(h5_path)\n",
    "    csv_files = [f for f in os.listdir(directory) if f.lower().endswith('.csv') and 'pinstate' in f.lower() and mouse in f]\n",
    "    \n",
    "    nearest_filename = None\n",
    "    min_timedelta = None\n",
    "    \n",
    "    # First, try to find a csv file with a timestamp later than or equal to h5 timestamp\n",
    "    for csv_file in csv_files:\n",
    "        m_csv = re.search(r'(\\d{4}-\\d{2}-\\d{2}-\\d{6})', csv_file)\n",
    "        if not m_csv:\n",
    "            continue\n",
    "        csv_timestamp = datetime.strptime(m_csv.group(1), \"%Y-%m-%d-%H%M%S\")\n",
    "        diff = csv_timestamp - h5_timestamp\n",
    "        if diff.total_seconds() >= 0:\n",
    "            if min_timedelta is None or diff < min_timedelta:\n",
    "                min_timedelta = diff\n",
    "                nearest_filename = csv_file\n",
    "\n",
    "    # Fallback: if no csv is later than the h5 file, choose the one with the smallest absolute difference\n",
    "    if nearest_filename is None:\n",
    "        for csv_file in csv_files:\n",
    "            m_csv = re.search(r'(\\d{4}-\\d{2}-\\d{2}-\\d{6})', csv_file)\n",
    "            if not m_csv:\n",
    "                continue\n",
    "            csv_timestamp = datetime.strptime(m_csv.group(1), \"%Y-%m-%d-%H%M%S\")\n",
    "            diff = abs(csv_timestamp - h5_timestamp)\n",
    "            if min_timedelta is None or diff < min_timedelta:\n",
    "                min_timedelta = diff\n",
    "                nearest_filename = csv_file\n",
    "\n",
    "    if nearest_filename is not None:\n",
    "        return os.path.join(directory, nearest_filename)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_pinstate_indices(pinstate_series, min_length=3):\n",
    "\n",
    "    # Convert to a numpy array if necessary.\n",
    "    arr = pinstate_series.values if hasattr(pinstate_series, 'values') else pinstate_series\n",
    "    # Create a Boolean mask for high values.\n",
    "    high_mask = (arr == max(arr))\n",
    "    indices = []\n",
    "    for i in range(len(arr)):\n",
    "        # Check if this is a transition: current is high and either it's the very first element\n",
    "        # or the previous state was low.\n",
    "        if high_mask[i] and (i == 0 or not high_mask[i-1]):\n",
    "            # Ensure that there are enough data points and that the next (min_length-1) values are high.\n",
    "            if i + min_length - 1 < len(arr) and np.all(high_mask[i:i+min_length]):\n",
    "                indices.append(i)\n",
    "    return np.array(indices)\n",
    "\n",
    "def extract_task(variables, index=0):\n",
    "    bracket_part = re.search(r'\\[.*\\]', variables[index]).group(0)\n",
    "    return ast.literal_eval(bracket_part)\n",
    "\n",
    "def get_sorted_h5_files(mouse, folder):\n",
    "    return sorted(\n",
    "        [i for i in os.listdir(folder) if mouse in i and '.h5' in i],\n",
    "        key=lambda fname: datetime.strptime(\n",
    "            re.search(r'(\\d{4}-\\d{2}-\\d{2}-\\d{6})', fname).group(1),\n",
    "            \"%Y-%m-%d-%H%M%S\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mapping_dict():\n",
    "    \"\"\"\n",
    "    Builds and returns a dictionary that maps node labels (node_1 through node_9)\n",
    "    to integers 1-9, and edge labels (edge_1-2, edge_1-4, etc.) to integers 10-21.\n",
    "    \n",
    "    Feel free to modify or expand the edges list if needed.\n",
    "    \"\"\"\n",
    "    # 1) Node labels: \"node_1\" -> 1, \"node_2\" -> 2, ..., \"node_9\" -> 9\n",
    "    mapping_dict = {}\n",
    "    for i in range(1, 10):\n",
    "        mapping_dict[f\"node_{i}\"] = i\n",
    "\n",
    "    # 2) Define the list of edges you have in your maze\n",
    "    #    For example, let's pretend we have 12 distinct edges total\n",
    "    #    so that they map from 10 to 21 (12 edges). Adjust as needed.\n",
    "\n",
    "\n",
    "    edges = [\n",
    "        \"1-2\",  # corresponds to \"edge_1-2\"\n",
    "        \"2-3\",\n",
    "        \"1-4\",\n",
    "        \"2-5\",\n",
    "        \"3-6\",\n",
    "        \"4-5\",\n",
    "        \"5-6\",\n",
    "        \"4-7\",\n",
    "        \"5-8\",\n",
    "        \"6-9\",\n",
    "        \"7-8\",\n",
    "        \"8-9\",\n",
    "    ]\n",
    "\n",
    "    # 3) Starting from integer 10 for edges. We create keys of the form \"edge_1-2\"\n",
    "    start_value = 10\n",
    "    for idx, edge in enumerate(edges, start=start_value):\n",
    "        mapping_dict[f\"edge_{edge}\"] = idx\n",
    "\n",
    "    return mapping_dict\n",
    "\n",
    "\n",
    "def convert_label_to_int(label, mapping):\n",
    "    \"\"\"\n",
    "    Given a label (e.g., 'node_4' or 'edge_1-2') and a mapping dictionary,\n",
    "    returns the corresponding integer code.\n",
    "    \"\"\"\n",
    "    if pd.isna(label) or label == 'nan':\n",
    "        return np.nan\n",
    "    # Otherwise, look up in the mapping dictionary\n",
    "    if label not in mapping:\n",
    "        raise ValueError(f\"Label '{label}' not found in mapping dictionary.\")\n",
    "    return mapping[label]\n",
    "\n",
    "def get_mindistance_mat():\n",
    "    \n",
    "    x=(0,1,2)\n",
    "    Task_grid=np.asarray(list(product(x, x)))\n",
    "\n",
    "    mapping_pyth={2:2,5:3,8:4}\n",
    "\n",
    "    distance_mat_raw=distance_matrix(Task_grid, Task_grid)\n",
    "    len_matrix=len(distance_mat_raw)\n",
    "    distance_mat=np.zeros((len_matrix,len_matrix))\n",
    "    for ii in range(len_matrix):\n",
    "        for jj in range(len_matrix):\n",
    "            if (distance_mat_raw[ii,jj]).is_integer()==False:\n",
    "                hyp=int((distance_mat_raw[ii,jj])**2)\n",
    "                distance_mat[ii,jj]=mapping_pyth[hyp]\n",
    "            else:\n",
    "                distance_mat[ii,jj]=distance_mat_raw[ii,jj]\n",
    "    mindistance_mat=distance_mat.astype(int)\n",
    "    \n",
    "    return mindistance_mat\n",
    "\n",
    "def make_trial_times_array(times_dic, target_binning=1000/60, Structure_abstract='ABCD'):\n",
    "\n",
    "\n",
    "\n",
    "    if Structure_abstract == 'ABCD':\n",
    "        states = ['A_on', 'B_on', 'C_on', 'D_on']\n",
    "    elif Structure_abstract == 'ABCDE':\n",
    "        states = ['A_on', 'B_on', 'C_on', 'D_on', 'E_on']\n",
    "    elif Structure_abstract == 'AB':\n",
    "        states = ['A_on', 'B_on']\n",
    "    elif Structure_abstract == 'ABCAD':\n",
    "        states = ['A_on', 'B_on', 'C_on', 'A2_on', 'D_on']\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown Structure_abstract: {Structure_abstract}\")\n",
    "\n",
    "\n",
    "    for state in states:\n",
    "        if state not in times_dic:\n",
    "            raise ValueError(f\"State {state} not found in times_dic\")\n",
    "\n",
    "    num_trials = min([len(times_dic[state]) for state in states])\n",
    "    \n",
    "    trial_times = np.zeros((num_trials - 1, len(states) + 1), dtype=int)\n",
    "    for i in range(num_trials - 1):\n",
    "        # current trial: all state times in order\n",
    "        trial_times[i, :-1] = [times_dic[state][i] for state in states]\n",
    "        # periodic: the next trial's first state's time becomes the last element\n",
    "        trial_times[i, -1] = times_dic[states[0]][i + 1]\n",
    "\n",
    "    conversion_factor = 1000/60\n",
    "    return (trial_times/conversion_factor).astype(int)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial_times_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mouse_locations\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m mouse_locations_array \u001b[38;5;241m=\u001b[39m get_mouse_locations(\u001b[43mtrial_times_array\u001b[49m, ROIs_int)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(mouse_locations_array)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(task)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trial_times_array' is not defined"
     ]
    }
   ],
   "source": [
    "def get_mouse_locations(trial_times_array, ROIs_int):\n",
    "    \"\"\"\n",
    "    Given a trial times array and a list of ROIs_int, returns an array of the same structure\n",
    "    but where the entries are the mouse's location at each of the time bins.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trial_times_array : np.ndarray\n",
    "        Array of trial times.\n",
    "    ROIs_int : list\n",
    "        List of mouse locations (ROIs) as integers.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Array of the same structure as trial_times_array but with mouse locations.\n",
    "    \"\"\"\n",
    "    n_trials, n_states = trial_times_array.shape\n",
    "    mouse_locations = np.zeros_like(trial_times_array, dtype=int)\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        for j in range(n_states):\n",
    "            time_bin = int(trial_times_array[i, j])\n",
    "            if time_bin < len(ROIs_int):\n",
    "                if np.isnan(ROIs_int[time_bin]):\n",
    "                    mouse_locations[i, j] = 999\n",
    "                else:\n",
    "                    mouse_locations[i, j] = ROIs_int[time_bin]\n",
    "            else:\n",
    "                mouse_locations[i, j] = np.nan  # Handle out-of-bounds case\n",
    "    \n",
    "    return mouse_locations\n",
    "\n",
    "# Example usage\n",
    "mouse_locations_array = get_mouse_locations(trial_times_array, ROIs_int)\n",
    "print(mouse_locations_array)\n",
    "print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_trials(trial_times, ROIs_int, task, mindistance_mat):\n",
    "    \"\"\"\n",
    "    takes trial times array structured as follows:\n",
    "      [[a1, b1, c1, d1, a2], \n",
    "       [a2, b2, c2, d2, a3],\n",
    "       ...\n",
    "       [an-1, bn-1, cn-1, dn-1, dn]]\n",
    "    a list called task that contains the locations in int format of each of the goals,\n",
    "    and mindistance_mat that specifies the shortest number of steps between two locations.\n",
    "    \n",
    "    Note: Only node locations (1-9) count; consecutive duplicates are removed.\n",
    "    Returns:\n",
    "      shortest_paths_array - a 2D array (n x 4) where each entry is 1 if the mouse took the shortest path,\n",
    "      session_performance - the fraction of entries in shortest_paths_array that are 1.\n",
    "    \"\"\"\n",
    "    n_trials = trial_times.shape[0]\n",
    "    shortest_paths_array = np.zeros((n_trials, 4))\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        for j in range(4):\n",
    "            start_time = trial_times[i, j]\n",
    "            end_time = trial_times[i, j + 1]\n",
    "            start_location = task[j]\n",
    "            end_location = task[(j + 1) % 4]\n",
    "\n",
    "            # Get the ROI indices for the current trial segment (as a list)\n",
    "            trial_segment = ROIs_int[int(start_time):int(end_time)]\n",
    "            \n",
    "            # Convert to numpy array to filter node locations (1-9)\n",
    "            trial_segment_arr = np.array(trial_segment)\n",
    "            node_mask = trial_segment_arr <= 9\n",
    "            filtered_nodes = trial_segment_arr[node_mask]\n",
    "            \n",
    "            # Remove consecutive duplicates by converting to a pandas Series\n",
    "            node_series = pd.Series(filtered_nodes)\n",
    "            node_unique = node_series[node_series.shift() != node_series].reset_index(drop=True)\n",
    "\n",
    "            if len(node_unique) > 1:\n",
    "                # Calculate the number of steps taken based on consecutive pairs\n",
    "                steps_taken = sum(mindistance_mat[int(node_unique.iloc[k]) - 1, int(node_unique.iloc[k + 1]) - 1] \n",
    "                                  for k in range(len(node_unique) - 1))\n",
    "            else:\n",
    "                steps_taken = np.inf\n",
    "\n",
    "            # Get the shortest path distance from start to end\n",
    "            shortest_path_distance = mindistance_mat[int(start_location) - 1, int(end_location) - 1]\n",
    "\n",
    "            # Mouse took the shortest path if steps match shortest_path_distance\n",
    "            shortest_paths_array[i, j] = 1 if steps_taken == shortest_path_distance else 0\n",
    "\n",
    "    session_performance = np.mean(shortest_paths_array)\n",
    "    return shortest_paths_array, session_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76493\n",
      "76494\n",
      "244 244\n",
      "labels.v003.000_ah08_2025-03-18-132718.analysis.h5\n",
      "ah08-2025-03-18-132820.txt\n",
      "ah08_pinstate_2025-03-18-132718.csv\n",
      "[2, 4, 3, 5]\n",
      "[[1. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "[[2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]]\n",
      "ah08 performance = 0.3333333333333333\n",
      "_______________________\n",
      "75619\n",
      "75620\n",
      "244 244\n",
      "labels.v003.001_ah08_2025-03-18-142533.analysis.h5\n",
      "ah08-2025-03-18-142619.txt\n",
      "ah08_pinstate_2025-03-18-142533.csv\n",
      "[2, 4, 3, 5]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[2 4 3 5 2]\n",
      " [2 4 3 5 2]]\n",
      "ah08 performance = 0.0\n",
      "_______________________\n",
      "75288\n",
      "75289\n",
      "233 233\n",
      "labels.v003.002_ah08_2025-03-18-161148.analysis.h5\n",
      "ah08-2025-03-18-161158.txt\n",
      "ah08_pinstate_2025-03-18-161148.csv\n",
      "[2, 4, 3, 5]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 0. 0. 1.]]\n",
      "[[2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]]\n",
      "ah08 performance = 0.3\n",
      "_______________________\n",
      "75265\n",
      "75266\n",
      "245 245\n",
      "labels.v003.003_ah08_2025-03-18-170234.analysis.h5\n",
      "ah08-2025-03-18-170256.txt\n",
      "ah08_pinstate_2025-03-18-170234.csv\n",
      "[2, 4, 3, 5]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]]\n",
      "ah08 performance = 0.16666666666666666\n",
      "_______________________\n",
      "73100\n",
      "73101\n",
      "261 261\n",
      "labels.v003.004_ah08_2025-03-19-114159.analysis.h5\n",
      "ah08-2025-03-19-114207.txt\n",
      "ah08_pinstate_2025-03-19-114159.csv\n",
      "[2, 4, 3, 5]\n",
      "[[0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 1.]]\n",
      "[[2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]]\n",
      "ah08 performance = 0.3125\n",
      "_______________________\n",
      "72968\n",
      "72969\n",
      "232 232\n",
      "labels.v003.005_ah08_2025-03-19-130221.analysis.h5\n",
      "ah08-2025-03-19-130226.txt\n",
      "ah08_pinstate_2025-03-19-130221.csv\n",
      "[2, 4, 3, 5]\n",
      "[[1. 1. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 1. 0. 1.]\n",
      " [1. 1. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]]\n",
      "ah08 performance = 0.55\n",
      "_______________________\n",
      "72602\n",
      "72603\n",
      "236 236\n",
      "labels.v003.006_ah08_2025-03-19-145941.analysis.h5\n",
      "ah08-2025-03-19-145952.txt\n",
      "ah08_pinstate_2025-03-19-145941.csv\n",
      "[2, 4, 3, 5]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]]\n",
      "ah08 performance = 0.2857142857142857\n",
      "_______________________\n",
      "74854\n",
      "74855\n",
      "252 252\n",
      "labels.v003.007_ah08_2025-03-19-155001.analysis.h5\n",
      "ah08-2025-03-19-155017.txt\n",
      "ah08_pinstate_2025-03-19-155001.csv\n",
      "[2, 4, 3, 5]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 1. 1.]\n",
      " [1. 0. 1. 1.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "[[2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]]\n",
      "ah08 performance = 0.4166666666666667\n",
      "_______________________\n",
      "72282\n",
      "72283\n",
      "241 241\n",
      "labels.v003.000_ah08_2025-03-20-104812.analysis.h5\n",
      "ah08-2025-03-20-104817.txt\n",
      "ah08_pinstate_2025-03-20-104812.csv\n",
      "[2, 4, 3, 5]\n",
      "[[0. 1. 0. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 1.]]\n",
      "[[2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]]\n",
      "ah08 performance = 0.375\n",
      "_______________________\n",
      "72545\n",
      "72546\n",
      "230 230\n",
      "labels.v003.001_ah08_2025-03-20-121018.analysis.h5\n",
      "ah08-2025-03-20-121023.txt\n",
      "ah08_pinstate_2025-03-20-121018.csv\n",
      "[2, 4, 3, 5]\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]\n",
      " [2 4 3 5 2]]\n",
      "ah08 performance = 0.35714285714285715\n",
      "_______________________\n",
      "76294\n",
      "76295\n",
      "246 246\n",
      "skipping, no trials\n",
      "31\n",
      "74875\n",
      "0 246\n",
      "ruh roh, pinstate and rsync dont match. Skipping\n",
      "73805\n",
      "73806\n",
      "231 231\n",
      "labels.v003.010_ah09_2025-03-18-153449.analysis.h5\n",
      "ah09-2025-03-18-153515.txt\n",
      "ah09_pinstate_2025-03-18-153449.csv\n",
      "[8, 9, 5, 3]\n",
      "[[0. 0. 0. 0.]\n",
      " [1. 1. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]]\n",
      "ah09 performance = 0.1875\n",
      "_______________________\n",
      "73439\n",
      "73440\n",
      "226 226\n",
      "labels.v003.011_ah09_2025-03-18-163812.analysis.h5\n",
      "ah09-2025-03-18-163832.txt\n",
      "ah09_pinstate_2025-03-18-163812.csv\n",
      "[8, 9, 5, 3]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]]\n",
      "ah09 performance = 0.3125\n",
      "_______________________\n",
      "72770\n",
      "72771\n",
      "243 243\n",
      "labels.v003.012_ah09_2025-03-19-120803.analysis.h5\n",
      "ah09-2025-03-19-120811.txt\n",
      "ah09_pinstate_2025-03-19-120803.csv\n",
      "[8, 9, 5, 3]\n",
      "[[1. 1. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [0. 1. 1. 1.]\n",
      " [1. 1. 0. 0.]\n",
      " [0. 0. 1. 1.]]\n",
      "[[8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]]\n",
      "ah09 performance = 0.5\n",
      "_______________________\n",
      "75368\n",
      "75369\n",
      "239 239\n",
      "labels.v003.013_ah09_2025-03-19-132601.analysis.h5\n",
      "ah09-2025-03-19-132651.txt\n",
      "ah09_pinstate_2025-03-19-132601.csv\n",
      "[8, 9, 5, 3]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 1. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]]\n",
      "ah09 performance = 0.3333333333333333\n",
      "_______________________\n",
      "73157\n",
      "73158\n",
      "242 242\n",
      "labels.v003.014_ah09_2025-03-19-143328.analysis.h5\n",
      "ah09-2025-03-19-143335.txt\n",
      "ah09_pinstate_2025-03-19-143328.csv\n",
      "[8, 9, 5, 3]\n",
      "[[1. 0. 0. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n",
      "[[8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]]\n",
      "ah09 performance = 0.35\n",
      "_______________________\n",
      "72354\n",
      "72355\n",
      "237 237\n",
      "labels.v003.015_ah09_2025-03-19-152530.analysis.h5\n",
      "ah09-2025-03-19-152537.txt\n",
      "ah09_pinstate_2025-03-19-152530.csv\n",
      "[8, 9, 5, 3]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 1. 0. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 1. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 1. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 1. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]]\n",
      "ah09 performance = 0.4\n",
      "_______________________\n",
      "73973\n",
      "73974\n",
      "235 235\n",
      "labels.v003.002_ah09_2025-03-20-111207.analysis.h5\n",
      "ah09-2025-03-20-111240.txt\n",
      "ah09_pinstate_2025-03-20-111207.csv\n",
      "[8, 9, 5, 3]\n",
      "[[0. 1. 0. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 8 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]]\n",
      "ah09 performance = 0.2\n",
      "_______________________\n",
      "77612\n",
      "77613\n",
      "259 259\n",
      "labels.v003.003_ah09_2025-03-20-124311.analysis.h5\n",
      "ah09-2025-03-20-124404.txt\n",
      "ah09_pinstate_2025-03-20-124311.csv\n",
      "[8, 9, 5, 3]\n",
      "[[0. 1. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 1. 1. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 1. 0. 1.]]\n",
      "[[8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]\n",
      " [8 9 5 3 8]]\n",
      "ah09 performance = 0.4166666666666667\n",
      "_______________________\n",
      "72823\n",
      "72824\n",
      "235 235\n",
      "labels.v003.016_ah10_2025-03-19-120803.analysis.h5\n",
      "ah10-2025-03-19-120811.txt\n",
      "ah10_pinstate_2025-03-19-120803.csv\n",
      "[5, 7, 9, 3]\n",
      "[[0. 0. 0. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[5 7 9 3 5]\n",
      " [5 7 9 3 5]\n",
      " [5 7 9 3 5]\n",
      " [5 7 9 3 5]]\n",
      "ah10 performance = 0.25\n",
      "_______________________\n",
      "75426\n",
      "75427\n",
      "249 249\n",
      "labels.v003.017_ah10_2025-03-19-132601.analysis.h5\n",
      "ah10-2025-03-19-132651.txt\n",
      "ah10_pinstate_2025-03-19-132601.csv\n",
      "[5, 7, 9, 3]\n",
      "[[0. 0. 0. 1.]\n",
      " [1. 1. 0. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 1. 1.]]\n",
      "[[5 7 9 3 5]\n",
      " [5 7 9 3 5]\n",
      " [5 7 9 3 5]\n",
      " [5 7 9 3 5]]\n",
      "ah10 performance = 0.5\n",
      "_______________________\n",
      "73709\n",
      "73710\n",
      "240 240\n",
      "labels.v003.004_ah10_2025-03-20-102226.analysis.h5\n",
      "ah10-2025-03-20-102242.txt\n",
      "ah10_pinstate_2025-03-20-102226.csv\n",
      "[5, 7, 9, 3]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 0. 1. 1.]]\n",
      "[[5 7 9 3 5]\n",
      " [5 7 9 3 5]\n",
      " [5 7 9 3 5]\n",
      " [5 7 9 3 5]\n",
      " [5 7 9 3 5]]\n",
      "ah10 performance = 0.25\n",
      "_______________________\n",
      "74262\n",
      "74263\n",
      "242 242\n",
      "labels.v003.009_ah10_2025-03-20-114339.analysis.h5\n",
      "ah10-2025-03-20-114348.txt\n",
      "ah10_pinstate_2025-03-20-114339.csv\n",
      "[5, 7, 9, 3]\n",
      "[[0. 0. 0. 0.]\n",
      " [1. 1. 1. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n",
      "[[5 7 9 3 5]\n",
      " [5 7 9 3 5]\n",
      " [5 7 9 3 5]\n",
      " [5 7 9 3 5]\n",
      " [5 7 9 3 5]]\n",
      "ah10 performance = 0.4\n",
      "_______________________\n",
      "76355\n",
      "76355\n",
      "246 246\n",
      "labels.v003.018_ly05_2025-03-18-124436.analysis.h5\n",
      "ly05-2025-03-18-124512.txt\n",
      "ly05_pinstate_2025-03-18-124436.csv\n",
      "[9, 4, 5, 1]\n",
      "[[0. 1. 0. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]]\n",
      "ly05 performance = 0.3125\n",
      "_______________________\n",
      "31\n",
      "74932\n",
      "0 231\n",
      "ruh roh, pinstate and rsync dont match. Skipping\n",
      "73873\n",
      "73874\n",
      "243 243\n",
      "labels.v003.020_ly05_2025-03-18-153449.analysis.h5\n",
      "ly05-2025-03-18-153514.txt\n",
      "ly05_pinstate_2025-03-18-153449.csv\n",
      "[9, 4, 5, 1]\n",
      "[[1. 0. 1. 1.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[9 4 5 1 9]\n",
      " [9 4 5 1 9]]\n",
      "ly05 performance = 0.5\n",
      "_______________________\n",
      "73497\n",
      "73498\n",
      "239 239\n",
      "labels.v003.021_ly05_2025-03-18-163812.analysis.h5\n",
      "ly05-2025-03-18-163832.txt\n",
      "ly05_pinstate_2025-03-18-163812.csv\n",
      "[9, 4, 5, 1]\n",
      "[[1. 0. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 1. 0. 0.]\n",
      " [0. 0. 1. 1.]]\n",
      "[[9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]]\n",
      "ly05 performance = 0.5\n",
      "_______________________\n",
      "74813\n",
      "74814\n",
      "224 224\n",
      "labels.v003.022_ly05_2025-03-19-111547.analysis.h5\n",
      "ly05-2025-03-19-111630.txt\n",
      "ly05_pinstate_2025-03-19-111547.csv\n",
      "[9, 4, 5, 1]\n",
      "[[1. 0. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]]\n",
      "ly05 performance = 0.3333333333333333\n",
      "_______________________\n",
      "74292\n",
      "74293\n",
      "241 241\n",
      "labels.v003.023_ly05_2025-03-19-123456.analysis.h5\n",
      "ly05-2025-03-19-123512.txt\n",
      "ly05_pinstate_2025-03-19-123456.csv\n",
      "[9, 4, 5, 1]\n",
      "[[0. 0. 0. 1.]\n",
      " [1. 1. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 1. 0. 1.]]\n",
      "[[9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]]\n",
      "ly05 performance = 0.38636363636363635\n",
      "_______________________\n",
      "73213\n",
      "73214\n",
      "216 216\n",
      "labels.v003.024_ly05_2025-03-19-143328.analysis.h5\n",
      "ly05-2025-03-19-143335.txt\n",
      "ly05_pinstate_2025-03-19-143328.csv\n",
      "[9, 4, 5, 1]\n",
      "[[1. 1. 0. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 0. 1. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 1. 1.]\n",
      " [1. 0. 1. 0.]]\n",
      "[[9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]]\n",
      "ly05 performance = 0.5\n",
      "_______________________\n",
      "72406\n",
      "72407\n",
      "252 252\n",
      "labels.v003.025_ly05_2025-03-19-152530.analysis.h5\n",
      "ly05-2025-03-19-152537.txt\n",
      "ly05_pinstate_2025-03-19-152530.csv\n",
      "[9, 4, 5, 1]\n",
      "[[0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 1. 1. 1.]\n",
      " [1. 0. 1. 1.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 0. 0. 1.]]\n",
      "[[9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]]\n",
      "ly05 performance = 0.4375\n",
      "_______________________\n",
      "74028\n",
      "74029\n",
      "233 233\n",
      "labels.v003.006_ly05_2025-03-20-111207.analysis.h5\n",
      "ly05-2025-03-20-111240.txt\n",
      "ly05_pinstate_2025-03-20-111207.csv\n",
      "[9, 4, 5, 1]\n",
      "[[0. 1. 0. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [1. 0. 1. 1.]]\n",
      "[[9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]]\n",
      "ly05 performance = 0.53125\n",
      "_______________________\n",
      "77676\n",
      "77677\n",
      "240 240\n",
      "labels.v003.007_ly05_2025-03-20-124311.analysis.h5\n",
      "ly05-2025-03-20-124404.txt\n",
      "ly05_pinstate_2025-03-20-124311.csv\n",
      "[9, 4, 5, 1]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 1. 0.]]\n",
      "[[9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]\n",
      " [9 4 5 1 9]]\n",
      "ly05 performance = 0.35714285714285715\n",
      "_______________________\n",
      "74756\n",
      "74757\n",
      "247 247\n",
      "labels.v003.026_ly06_2025-03-19-111547.analysis.h5\n",
      "ly06-2025-03-19-111630.txt\n",
      "ly06_pinstate_2025-03-19-111547.csv\n",
      "[9, 8, 5, 7]\n",
      "[[1. 0. 1. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[9 8 5 7 9]\n",
      " [9 8 5 7 9]]\n",
      "ly06 performance = 0.5\n",
      "_______________________\n",
      "74234\n",
      "74235\n",
      "256 256\n",
      "labels.v003.027_ly06_2025-03-19-123456.analysis.h5\n",
      "ly06-2025-03-19-123512.txt\n",
      "ly06_pinstate_2025-03-19-123456.csv\n",
      "[9, 8, 5, 7]\n",
      "[[0. 1. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "[[9 8 5 7 9]\n",
      " [9 8 5 7 9]\n",
      " [9 8 5 7 9]\n",
      " [9 8 5 7 9]]\n",
      "ly06 performance = 0.1875\n",
      "_______________________\n",
      "73653\n",
      "73654\n",
      "260 260\n",
      "labels.v003.008_ly06_2025-03-20-102226.analysis.h5\n",
      "ly06-2025-03-20-102242.txt\n",
      "ly06_pinstate_2025-03-20-102226.csv\n",
      "[9, 8, 5, 7]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 1. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "[[ 9  8  5 17  9]\n",
      " [ 9  8  5  7  9]\n",
      " [ 9  8  5  7  9]\n",
      " [ 9  8  5  7  9]\n",
      " [ 9  8  5 17  9]\n",
      " [ 9  8  5  7  9]\n",
      " [ 9 20  5  7  9]]\n",
      "ly06 performance = 0.25\n",
      "_______________________\n",
      "74206\n",
      "74207\n",
      "252 252\n",
      "labels.v003.005_ly06_2025-03-20-114339.analysis.h5\n",
      "ly06-2025-03-20-114348.txt\n",
      "ly06_pinstate_2025-03-20-114339.csv\n",
      "[9, 8, 5, 7]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 1.]]\n",
      "[[ 9  8  5  7  9]\n",
      " [ 9  8  5  7 21]\n",
      " [21  8  5  7  9]\n",
      " [ 9 20  5  7  9]\n",
      " [ 9  8  5  7  9]]\n",
      "ly06 performance = 0.25\n",
      "_______________________\n",
      "76552\n",
      "76553\n",
      "235 235\n",
      "labels.v003.028_ly07_2025-03-18-132718.analysis.h5\n",
      "ly07-2025-03-18-132820.txt\n",
      "ly07_pinstate_2025-03-18-132718.csv\n",
      "[8, 2, 5, 1]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]]\n",
      "ly07 performance = 0.1875\n",
      "_______________________\n",
      "75704\n",
      "75705\n",
      "233 233\n",
      "labels.v003.029_ly07_2025-03-18-142533.analysis.h5\n",
      "ly07-2025-03-18-142619.txt\n",
      "ly07_pinstate_2025-03-18-142533.csv\n",
      "[8, 2, 5, 1]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]]\n",
      "[[8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]]\n",
      "ly07 performance = 0.3333333333333333\n",
      "_______________________\n",
      "75343\n",
      "75344\n",
      "249 249\n",
      "labels.v003.030_ly07_2025-03-18-161148.analysis.h5\n",
      "ly07-2025-03-18-161158.txt\n",
      "ly07_pinstate_2025-03-18-161148.csv\n",
      "[8, 2, 5, 1]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 1.]]\n",
      "[[8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]]\n",
      "ly07 performance = 0.3333333333333333\n",
      "_______________________\n",
      "75315\n",
      "75316\n",
      "240 240\n",
      "labels.v003.031_ly07_2025-03-18-170234.analysis.h5\n",
      "ly07-2025-03-18-170256.txt\n",
      "ly07_pinstate_2025-03-18-170234.csv\n",
      "[8, 2, 5, 1]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 1.]]\n",
      "[[8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]]\n",
      "ly07 performance = 0.2916666666666667\n",
      "_______________________\n",
      "73158\n",
      "73159\n",
      "245 245\n",
      "labels.v003.032_ly07_2025-03-19-114159.analysis.h5\n",
      "ly07-2025-03-19-114207.txt\n",
      "ly07_pinstate_2025-03-19-114159.csv\n",
      "[8, 2, 5, 1]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 1 5 1 8]]\n",
      "ly07 performance = 0.25\n",
      "_______________________\n",
      "73027\n",
      "73027\n",
      "266 266\n",
      "labels.v003.033_ly07_2025-03-19-130221.analysis.h5\n",
      "ly07-2025-03-19-130226.txt\n",
      "ly07_pinstate_2025-03-19-130221.csv\n",
      "[8, 2, 5, 1]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]]\n",
      "ly07 performance = 0.25\n",
      "_______________________\n",
      "72657\n",
      "72658\n",
      "250 250\n",
      "labels.v003.034_ly07_2025-03-19-145941.analysis.h5\n",
      "ly07-2025-03-19-145951.txt\n",
      "ly07_pinstate_2025-03-19-145941.csv\n",
      "[8, 2, 5, 1]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 1. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]]\n",
      "ly07 performance = 0.25\n",
      "_______________________\n",
      "74911\n",
      "74912\n",
      "247 247\n",
      "labels.v003.035_ly07_2025-03-19-155001.analysis.h5\n",
      "ly07-2025-03-19-155017.txt\n",
      "ly07_pinstate_2025-03-19-155001.csv\n",
      "[8, 2, 5, 1]\n",
      "[[0. 1. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]]\n",
      "ly07 performance = 0.25\n",
      "_______________________\n",
      "72348\n",
      "72349\n",
      "217 217\n",
      "labels.v003.010_ly07_2025-03-20-104812.analysis.h5\n",
      "ly07-2025-03-20-104817.txt\n",
      "ly07_pinstate_2025-03-20-104812.csv\n",
      "[8, 2, 5, 1]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]]\n",
      "ly07 performance = 0.1875\n",
      "_______________________\n",
      "72601\n",
      "72602\n",
      "229 229\n",
      "labels.v003.011_ly07_2025-03-20-121018.analysis.h5\n",
      "ly07-2025-03-20-121023.txt\n",
      "ly07_pinstate_2025-03-20-121018.csv\n",
      "[8, 2, 5, 1]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]\n",
      " [8 2 5 1 8]]\n",
      "ly07 performance = 0.20833333333333334\n",
      "_______________________\n"
     ]
    }
   ],
   "source": [
    "sampling_rate_conversion = 1000/60\n",
    "\n",
    "mindistance_mat = get_mindistance_mat()\n",
    "mapping_dict = build_mapping_dict()\n",
    "\n",
    "scores_dic = {}\n",
    "\n",
    "for mouse in mouse_to_maze_dict.keys():\n",
    "    maze_id = mouse_to_maze_dict[mouse]\n",
    "    masks_directory = f\"/Users/AdamHarris/Desktop/cohort10_masks/maze{maze_id}\"\n",
    "    sleap_h5_paths = get_sorted_h5_files(mouse, folder)\n",
    "    sleap_h5_paths = [os.path.join(folder, i) for i in sleap_h5_paths]\n",
    "    scores_dic[mouse] = {}\n",
    "\n",
    "    for i in sleap_h5_paths:\n",
    "\n",
    "        \n",
    "\n",
    "        ROIs = map_body_parts_to_subcomponent(i, masks_directory)\n",
    "\n",
    "\n",
    "        # with h5py.File(i, 'r') as f:\n",
    "        #     # Convert node names from bytes if needed\n",
    "        #     bodypart_names = [name.decode('utf8') if isinstance(name, bytes) else name \n",
    "        #                       for name in f['node_names'][:]]\n",
    "        #     tracks = f['tracks'][:]  # shape: (1, 2, num_bodyparts, num_frames)\n",
    "        # bp = 'head_back'\n",
    "        # idx = bodypart_names.index(bp)\n",
    "        # x = tracks[0, 0, idx, :]\n",
    "        # y = tracks[0, 1, idx, :]\n",
    "\n",
    "        # # Plot points colored by ROI\n",
    "        # plt.figure(figsize=(8,6))\n",
    "        # roi_labels, roi_uniques = pd.factorize(ROIs)\n",
    "        # colors = plt.cm.jet(np.linspace(0, 1, len(roi_uniques)))\n",
    "        # for j, roi in enumerate(roi_uniques):\n",
    "        #     plt.scatter(x[roi_labels==j], y[roi_labels==j], s=1, color=colors[j], label=roi)\n",
    "        # plt.xlabel(\"X\")\n",
    "        # plt.ylabel(\"Y\")\n",
    "        # plt.legend(fontsize=6)\n",
    "        # plt.title(\"SLEAP head_back: x and y colored by ROI\")\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "        behaviour_txt = get_nearest_txt_timestamp(i, mouse)\n",
    "        behaviour_timestamps, variables = get_behaviour_txt(behaviour_txt)\n",
    "        task = extract_task(variables)\n",
    "        \n",
    "\n",
    "        \n",
    "        pinstate_csv = get_pinstate(i, mouse)\n",
    "        pinstate = pd.read_csv(pinstate_csv)\n",
    "        print(len(pinstate))\n",
    "        print(len(ROIs))\n",
    "        pinstate = pinstate.squeeze()\n",
    "        sync_indices = np.where(pinstate > np.median(pinstate))[0]\n",
    "        sync_indices = sync_indices[np.diff(np.concatenate(([0], sync_indices))) > 1]        \n",
    "        print(len(sync_indices), len(behaviour_timestamps['rsync']))\n",
    "        \n",
    "        if len(sync_indices) == len(behaviour_timestamps['rsync']):\n",
    "\n",
    "            \n",
    "            first_pinstate = sync_indices[0]\n",
    "            trial_times_array = make_trial_times_array(behaviour_timestamps,\n",
    "                                                        target_binning=1000/60, \n",
    "                                                        Structure_abstract=\"ABCD\")\n",
    "            if len(trial_times_array)<1:\n",
    "                print('skipping, no trials')\n",
    "                continue\n",
    "\n",
    "            ROIs_single_bp = ROIs[first_pinstate:]\n",
    "            ROIs_int = [mapping_dict[i] if i in mapping_dict else np.nan for i in ROIs_single_bp]\n",
    "            # Replace np.nan values in ROIs_int with the last valid integer\n",
    "            \n",
    "            first_valid_int = next((val for val in ROIs_int if not np.isnan(val)), None)\n",
    "            for idx in range(len(ROIs_int)):\n",
    "                if np.isnan(ROIs_int[idx]):\n",
    "                    ROIs_int[idx] = ROIs_int[idx - 1] if idx > 0 else first_valid_int\n",
    "            \n",
    "            trial_locs = get_mouse_locations(trial_times_array, ROIs_int)\n",
    "            shortest_paths_array, session_performance = score_trials(trial_times_array,\n",
    "                                                                    ROIs_int,\n",
    "                                                                    task,\n",
    "                                                                    mindistance_mat) \n",
    "\n",
    "            # scores_dic[mouse][task] = {}\n",
    "            # scores_dic[mouse][task][]['scores'] = shortest_paths_array\n",
    "            # scores_dic[mouse][task]['session_performance']session_performance\n",
    "            print(os.path.split(i)[1])\n",
    "            print(os.path.split(behaviour_txt)[1])\n",
    "            print(os.path.split(pinstate_csv)[1])\n",
    "            print(task)\n",
    "            print(shortest_paths_array)\n",
    "            print(trial_locs)\n",
    "            print(f\"{mouse} performance = {session_performance}\")\n",
    "            print(\"_______________________\")\n",
    "        else:\n",
    "        \n",
    "            print(\"ruh roh, pinstate and rsync dont match. Skipping\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do \n",
    "\n",
    "- Checks that the nearest files isnt crazy far away in time (would be an issue for matching across sessions if files are missing)\n",
    "\n",
    "- integrate usual preprocessing code for coordinates so discrete locations don't have to be recomputed\n",
    "\n",
    "- save out scores arrays in dataframe or dict for each session so that we can plot performance over trials for each task rather than for individual sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
